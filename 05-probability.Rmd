```{r setup5, echo=FALSE}
library(stringr)
library(infer)
library(nycflights13)
library(broom)
library(dplyr)
library(ggplot2)
library(stringr)
library(rvest)
library(mosaicData) 
library(ggmosaic)
library(NHANES)
library(mosaic)
knitr::opts_chunk$set(echo = FALSE)
```

# Randomness and Probability

Recall the model we built to predict home price as a function of square footage and fireplaces. 

```{r echo=TRUE}
homes <- read.delim("http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt")
homes <- homes %>%
  mutate(AnyFireplace = Fireplaces > 0)
homes %>%
    ggplot(aes(x = Living.Area, y = Price, color = AnyFireplace)) + 
    geom_point(alpha = 0.25) +
    geom_smooth(method = "lm") +
    theme_minimal()
```

To allow for different slopes among homes with and without a fireplace, we used an interaction term between Living.Area and AnyFireplace. Based on our sample, we observed a difference in the slopes of \$26.85 per square foot. But is this true for the larger population of homes? Is each square foot worth exactly $26.85 more, on average, in homes with a fireplace than in homes without a fireplace?

```{r echo=TRUE}
lm.home3 <- lm(Price ~ AnyFireplace*Living.Area, data = homes)
tidy(lm.home3)
```

Probably not. In fact, our sample is just one random sample from the larger population of homes in upstate New York. If we had gotten a slightly different sample of homes, then we would have different estimates for each of the regression coefficients. We explored this sampling variation in Chapter 4 using bootstrapping.

Let's connect our goals to the terms: **statistic** and **parameter**. The interaction coefficient of \$26.85 is a **statistic**. It (as well as the other coefficient estimates) is a numerical summary of our data that was estimated from a **sample**. If we had a census, a full set of data on all homes in upstate New York, we could fit the same linear regression model. The coefficient estimates given to us by R would represent population **parameters** because they are computed from the whole population, rather than a sample. By understanding **how much** statistics vary from sample to sample, we can start to **quantify the amount of uncertainty** in our estimates. Because the process of obtaining a sample is a type of random process, we will spend some time discussing formal probability.

This chapter briefly discusses the theory of formal probability so that we have terminology and basic concepts to understand and discuss random events. This framework provides a way of thinking about **uncertainty**, **random variability**, and **average behavior in the long run.**

A **random process** or **random event** is any process/event whose outcome is governed by chance. It is any process/event that cannot be known with certainty. Examples range from the outcome of flipping a coin to the estimated model coefficients from randomly selected samples.

We've used the term "chances" up until now. We are now going to use "probability" as an equivalent word for "chance".

For a much more in-depth discussion of probability (calculus-based), take MATH 354 (Probability). 

## Three Types of Probability

```{block type="reflect"}
What is the probability of getting a 1 from a six-sided die? How do you know this? How can you justify that number?
```

There are three types of probability.

1. **Empirical Probability:** If you could repeat a random process over and over again, we'd get a sense of the possible outcomes and their associated probabilities by calculating their relative frequency in the long run. If you repeatedly tossed a balanced die, then the relative frequency of 1's after tossing the die MANY times would be the empirical probability. If you repeatedly got a sample of 100 people, the relative frequency of estimated odds ratios below 1 would be the empirical probability of getting an odds ratio below 1. 

2. **Theoretical Probability:** If you don't have time to toss a die a million times, you could calculate probabilities based on mathematical theory and assumptions. When tossing a die, you would assume that each side is equally likely to land face-up. Thus the chance of rolling a 1, is 1/6 for a six-sided die.

```{block type="reflect"}
What is the probability you'll get an A in this class? What does that number represent? How can you justify that number?
```

3. **Subjective Probability:** If you use a number between 0 and 1 (100%) to reflect your uncertainty in an outcome (rather than based on empirical evidence or mathematical theory), then you are using subjective probability. 

In this class, we'll focus on theoretical and empirical probability. In particular, we will use computational tools to estimate empirical probabilities using simulations (such as bootstrapping and randomization tests) and mathematical tools to estimate theoretical probabilities. 

## Probability Rules

In theoretical probability, we need to define a few terms and set some rules for working with probabilities (known as axioms).

The **sample space**, $S$, is the set of all possible outcomes of a random process.

- Example: If you flip two coins (one side Heads and one side Tails), then the sample space contains four possible outcomes: Heads and Heads (HH), Heads and Tails (HT), Tails and Heads (TH), and Tails and Tails (TT), $S = \{HH,HT,TH,TT\}$.

A subset of outcomes is called an **event**, $A$. 

- Example: If you flip two coins, an event $A$ could be that exactly one of the coins land Heads, $A = \{HT,TH\}$.

For events $A$ and $B$ and sample space $S$, the probability of an event $A$, notated as $P(A)$, follows the rules below:

- Rule 1: $0\leq P(A)\leq 1$ (probability has to be between 0 and 1)
- Rule 2: $P(S) = 1$ (one of the possible outcomes has to happen)
- Rule 3: $P(\text{not }A) = 1 - P(A)$ (if we know the chance of something happening, we also know that chance it doesn't happen)
- Rule 4: $P(A\text{ or }B) = P(A) + P(B)$ if $A$ and $B$ are disjoint events.
  - $A$ and $B$ are **disjoint** if $A$ occuring prevents $B$ from occurring (they both can't happen at the same time).
- Rule 4*: $P(A\text{ or }B) = P(A) + P(B) - P(A\text{ and } B)$ in general
- Rule 5: $ P(A\text{ and }B) = P(A)\times P(B)$ if $A$ and $B$ are independent.
  - $A$ and $B$ are **independent** if $B$ occurring doesn't change the probability of $A$ occurring.
- Rule 5*: $P(A\text{ and }B) = P(A \mid B)P(B) = P(B \mid A)P(A)$.
  - The **conditional probability** of A **given** that event B occurs, $P(A \mid B)$, is equal to the probability of the joint event (A and B) divided by the probability of B.
$$ P(A \mid B) = \frac{P(A \text{ and } B)}{P(B)} $$
  - Intuition: Given that $B$ happened, we focus on the subset of outcomes in $S$ in which $B$ occurs and then figure out what the chance of $A$ happening within that subset. 

For more details on theoretical probability, please see Appendix A. This material is optional but available for those of you who want to understand the mathematical reasoning behind the rest of the chapter.

### Diagnotic Testing and Probability

Let's start by taking a moment to consider a recent [Washington Post article](https://www.washingtonpost.com/news/posteverything/wp/2018/10/05/feature/doctors-are-surprisingly-bad-at-reading-lab-results-its-putting-us-all-at-risk/?utm_term=.73d08eefca3c) that discusses the role of probability in medical diagnostics. Before you read the whole article, consider a question.

```{block, type="reflect"}
Say that Disease X has a prevalence of 1 in 1,000 (meaning that 1 out of every 1,000 people will have it). The test to detect Disease X has a false-positive rate of 5 percent (meaning that out of every 100 subjects who do not have Disease X, 5 will falsely test positive for it). If a patient's test result comes back positive, what is the probability that this patient actually has the disease?
```

If you said the probability is 95%, then you are wrong, but almost half of the doctors surveyed in 2014 thought exactly the same thing.

We can use the rules of probability to get a sense of what the desired probability actually is. We want to know the probability that they have the disease GIVEN that they got a positive test result, $P(D \mid +)$ where $D$ stands for disease and $+$ stands for positive test result.

Based on the definition of conditional probability, we must consider only those that got a positive test result back and look at the proportion of them that have the disease. In mathematical notation, that is equal to

$$P(D \mid +) = \frac{P(D \text{ and } +)}{P(+)}$$

What information were we given again?

- The prevalence of the disease is 1 in 1,000, so $P(D) = 1/1000$. Using Rule 3, the probability of no disease is $P(\text{ no }D) = 999/1000$. In 1000 people, 1 will actually have the disease and 999 won't have the disease.

- The false-positive rate is 5 percent, so given you don't have the disease, the probability of getting a false positive is $P(+ \mid\text{ no } D) = 0.05$. So of the 999 that don't have the disease, about $0.05\times 999 = 49.95$ (about 50) of them will get a false positive test result.

- While it is not stated, most medical tests have a fairly high accuracy in catching the disease so $P(+ \mid D) = 0.99$. Therefore, the 1 person who actually has the disease will most likely get a positive test result back ($0.99*1 = 0.99$).

Remember that our interest is in $P(D \mid +)$. By the definition of conditional probability, we consider only those with positive test results (about 50 who are disease free and 1 who has the disease). So the probability of actually having the disease GIVEN a positive test result is about 1/51 = 0.019.

In mathematical notation, that looks like this

$$
\begin{align*}
P(D \mid +) &= \frac{P(D \text{ and } +)}{P(+)} \\
&= \frac{P(D \text{ and } +)}{P(+ \text{ and } D) + P(+ \text{ and no } D)} \\
&= \frac{P(+ \mid D) P(D)}{P( + \mid D) P(D) + P( + \mid \text{ no }D) P(\text{no }D)} \\
&= \frac{0.99*1/1000}{0.99*1/1000 + 0.05*999/1000} \\
&= \frac{0.99*1}{0.99*1 + 0.05*999}
\end{align*}
$$

The first expression above ($P(D \mid +) &= \frac{P(+ \mid D) P(D)}{P( + \mid D) P(D) + P( + \mid \text{ no }D) P(\text{no }D)}$) is often called **Bayes' Rule**. The important idea to take from this is that what we condition on can make a big difference in the resulting probability.

Now, take some time to read the full [Washington Post article](https://www.washingtonpost.com/news/posteverything/wp/2018/10/05/feature/doctors-are-surprisingly-bad-at-reading-lab-results-its-putting-us-all-at-risk/?utm_term=.73d08eefca3c).

### Court Arguments and Probability

The concept of conditional probability also plays an important role in the judicial system in the U.S. The foundation of the judicial system is the idea of "innocent until proven guilty". Assuming that the defendant is innocent, what are the changes of having this evidence? That is, evidence is presented to jurors as the conditional probability: $P(\text{ evidence } \mid \text{ innocent })$.

Unfortunately, many prosecutors try to make the wrong argument by flipping the conditional probability, whether maliciously or due to a lack of statistical knowledge. They sometimes mistakenly try to argue that it is unlikely that a person is innocent given the evidence that is available, $P(\text{ innocent } \mid \text{ evidence })$. This can be dangerous if a prosecutor argues that since $P(\text{ evidence } \mid \text{ innocent }) = 1/\text{1 million}$ to argue that $P(\text{ innocent } \mid \text{ evidence }) = 1/\text{1 million}$. We know this isn't true based on the disease testing example above.

This is known as the prosecutor's fallacy. You can read more about it [here](https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy).



## Random Variable

With a basic understanding of theoretical probability rules, we can introduce the most important concept from probability for our uses in this class.

A **random variable** ($X$) is variable whose outcome (the value it takes) is governed by chance. It is a variable (something capable of taking different values) whose value is random. Examples include:

- $X =$ age of the next person to walk into the building
- $X =$ the number of dots on the side that lands face up on a balanced 6-sided die

When considering data analysis and modeling, the random variables we will be thinking about will be estimated regression coefficients, estimated odds ratios, etc. Why are these random variables? Because their values depend on the random samples that we draw. To establish our understanding, let's start with a simple example.

You are going to flip a fair coin 3 times (the coin has 2-sides, we'll call one side Heads and the other Tails).

- Assume there are only 2 possible outcomes and $P(\text{Heads}) = P(\text{Tails}) = 0.5$. (The coin can't land on its side).
- Below are three possible random variables based on the same random process (flipping a 2-sided coin 3 times):

- Example 1: $X =$ the number of heads in 3 coin flips
- What are the possible values of $X$? 0, 1, 2, or 3.

- Example 2: Say I give you 3 dollars for each Head
- $Y =$ the amount of money won from 3 coin flips, $Y = 3*X$
- The possible values of $Y$ are 0, 3, 6, or 9.

- Example 3: $Z =$ the number of heads on the last of the 3 coin flips
- The possible values are 0 or 1.

What might you want to know about these random variables? In general, we'd like to know the probability model (what values it takes and the associated chances), expected value (long-run average), and variance (a measure of how much the values vary). Let's talk about each of these next.

## Probability Models

A **probability model** for a random variable $X$ gives the possible values of $X$ and the associated probabilities. 

- What is the probability model for $X$: the number of heads in 3 coin flips?

$P(X = 0) = P(\text{three tails}) =  0.5^3$ (using Rule 5)
$P(X = 1) = P(\text{HTT or THT or TTH }) = 3*0.5^3$ (using Rule 4 & 5)
$P(X = 2) = P(\text{HHT or HTH or THH }) = 3*0.5^3$ (using Rule 4 & 5)
$P(X = 3) = P(\text{three heads}) = 0.5^3$ (using Rule 5)

- What is the probability model for $Y = 3*X$? (The total number of dollars earned when $3 is paid for each head.)

$P(Y = 0) = P(\text{three tails}) =  0.5^3$ 
$P(Y = 3) = P(\text{HTT or THT or TTH }) = 3*0.5^3$ 
$P(Y = 6) = P(\text{HHT or HTH or THH }) = 3*0.5^3$ 
$P(Y = 9) = P(\text{three heads}) = 0.5^3$

- What about $Z$? (The number of heads on the last of the 3 coin flips)

$P(Z = 0) = P(\text{HHT or TTT or HTT or THT}) =  4*0.5^3 = 0.5$ (using Rule 4 & 5)
$P(Z = 1) = P(\text{HHH or TTH or HTH or THH}) = 4*0.5^3 = 0.5$ (using Rule 4 & 5)

For most situations in our class, we won't use the probability rules to calculate chances by hand. Rather, we will use a **named probability model** such that the we can calculate probabilities for particular values of the random variable using either:

- a **probability mass function (pmf)** (finite number of possible values) or 
- a **probability density function (pdf)** (infinite number of possible values)

### Using probability mass functions

Let's say that we are working with a random variable $X$ that represents the result of spinning the arrow on a spinner that has 3 regions labeled 1, 2, and 3. $X$ can only takes the values 1, 2, or 3. The probability mass function (pmf) for X gives the probabilities that $X=1$, $X=2$, and $X=3$ which are determined by the relative areas of the regions on the spinner. The pmf for X is frequently denoted as $p(x)$, which is shorthand for $P(X = x)$. Based on the construction of our spinner, the pmf can be denoted as below:

|    $x$ |  1  |  2  |  3  |
|-------:|:---:|:---:|:---:|
| $p(x)$ | 0.4 | 0.5 | 0.1 |


The first row indicates the values that $X$ can take. The second row indicates the associated probabilities, the values of the probability mass function. Note that this row adds up to 1 because one of these 3 outcomes must happen.

We can use the pmf and the probability rules introduced earlier to calculate probabilities of different events.

Example 1: $P(X = 1 \text{ or } 2)$

Because the events $X = 1$ and $X = 2$ are disjoint (mutually exclusive/can't happen simultaneously), this probability is equal to $P(X = 1) + P(X = 2) = 0.4 + 0.5 = 0.9$. (Rule 4)

Example 2: $P(X \neq 1)$

By Rule 3, $P(X \neq 1) = 1 - P(X = 1) = 1 - 0.4 = 0.6$. Another way to arrive at this would be to see that $P(X \neq 1) = P(X = 2 \text{ or } 3) = P(X = 2) + P(X = 3) = 0.5 + 0.1$.

### Using probability density functions

When a random variable can take infinitely many values (i.e. a quantitative variable), using a probability mass function will not work because we would have to specify infinitely many probabilities. A probability density function (pdf) serves an analogous role to probability mass functions but works for quantitative variables that can take infinitely many values.

We have looked at density plots previously when we learned about data visualization. These were smooth curves that showed us the distribution of quantitative variable. In this class, the probability density functions that we will look at will be smooth curves. An example of a pdf for a famous named probability model (called the Normal distribution) is shown below:

```{r}
x <- seq(-3,3,0.01)
y <- dnorm(x)
plot(x, y, xlab = "x", ylab = "Density", main = "Example of a probability density function", type='l')
```

There are two main ideas that you should be comfortable with when working with quantitative random variables:

1. With a quantitative random variable $X$, we are interested in the probability that $X$ falls in a certain range. Examples include $P(X > 3), P(X < -5), P(-1 < X < 1)$.
2. The calculation of such probabilities is achieved by looking at areas underneath the probability density function within that range.

The pictures below illustrate how different probability statements correspond to different areas beneath the pdf curve.

```{r}
## Plots area between x1 and x2 for the standard normal
## To plot tails, let x1 = -4 or x2 = 4
plot_area_std_normal <- function(x1, x2, title = "") {
    x <- seq(-4, 4, 0.01)
    y <- dnorm(x)
    plot(x, y, type = "l", main = title, xlab = "x", ylab = "Density")
    area <- pnorm(x2) - pnorm(x1)
    bool <- x >= x1 & x <= x2
    x_shaded <- x[bool]
    y_shaded <- y[bool]
    if (x1 == -4) {
        area <- pnorm(x2)
        x1 <- NULL
    }
    if (x2 == 4) {
        area <- 1-pnorm(x1)
        x2 <- NULL
    }
    abline(v = c(x1, x2), col = "red", lty = "dashed", lwd = 2)
    polygon(x = c(x_shaded, tail(x_shaded, 1), head(x_shaded, 1)), y = c(y_shaded, 0, 0), col = "darkorchid")
    text(x = 3, y = 0.3, paste("Area =", round(area, 3)))
}
plot_area_std_normal(-1, 1, title = "P(-1 < X < 1)")
plot_area_std_normal(-2, 2, title = "P(-2 < X < 2)")
plot_area_std_normal(-4, -1, title = "P(X < -1)")
plot_area_std_normal(2, 4, title = "P(X > 2)")
```


### Expected value and variance

Two important properties of a random variable $X$ are its **expected value** and **variance**.

The expected value of a random variable is a real number and gives a measure of the typical value that the random variable takes or the long-run average if you could repeat the random process many times. For a mathematical definition, you can look at the probability appendix in these notes. Generally speaking, an expected value is a weighted average of the values that the random variable can take. If, for example, $X$ has a very high probability to take the value 9, then 9 will be weighed more heavily in the average.

**Why do we care about expected value?** Remember that the random variables that we will be concerned with are quantities such as estimated slopes from linear regression. This estimated slope is a (quantitative) random variable because its value depends on the random sample that we drew. Because it is a random variable, it has an associated probability density function which tells us how this estimated slope would vary across different samples of the same size. This distribution is called a **sampling distribution**, and expected value tells us the mean (center) of this distribution. When we take simple random samples from the population, the expected value is usually equal to the true target population value that we are trying to estimate from our sample. This is a powerful idea because on average, the estimate we compute from modeling is on average correct.

The variance of a random variable is very much related to the variance of a set of numbers that was introduced as a measure of spread/variation/variability. Recall that variance as a measure of spread is approximately the average of the squared distances of each data point to the mean. The variance of a random variable follows in very much the same spirit. It gives the average squared distance of the random variable from its expected value. The variance of a random variable, like the variance of a set of numbers, measures how spread out the values of the random variable could be. The standard deviation of a random variable is the square root of the variance. 

**Why do we care about the variance of a random variable?** The spread of the sampling distribution is measured by the variance of the associated random variable. If the sampling distribution is very spread out, then our estimates could be all over the place from sample to sample. This means that there is a lot of uncertainty in our estimate. We may have estimated a slope of 3 from our linear regression model, but that slope could easily have been 7 or -2 depending on our particular sample. The variance of an estimated regression slope quantifies how much our estimate could vary from sample to sample which will allow us to give reasonable margins of error on our estimates.

Wait! These ideas are fine in theory...but how do we actually use them? It turns out that a lot of the random variables that we've worked with so far come from **named probability distributions** that allow for theoretical calculations of expected value and variance. This theory means that we don't have to go out and collect sample after sample to get a sense of sampling variability. Statistical theory gives us a way to understand/approximate how our estimates vary from sample to sample just from mathematics. This is based on **theoretical probability** rather than our bootstrapping which was giving us the same type of information bu through **empirical probability**.


## Bernoulli/Binomial Model

There are situations in which your response of interest only has two possible outcomes: success or failure, heart disease or no heart disease, o-ring failure or no o-ring failure, etc. In the past, we used logistic regression to create a model to predict a binary response variable based on explanatory variables. For a moment, let's just consider the response itself as a random process. 

If a random process satisfied the following three conditions, then we can use the **Bernoulli Model** to understand its long-run behavior:

1. Two possible outcomes (success or failure)
2. Independent "trials" (randomly sampling one person does not impact probability of sampling anyone else)
3. P(success) = $p$ is constant (the relative frequency of success in the population that we are drawing from is constant)

The probability mass function is $P(X = 1) = p$ and $P(X = 0) = 1-p$ where $X$ is the number of successes in on trial. In the long run, the expected number of successes will be $p$ and the variance will be $p(1-p)$.

Let's think back to the disease testing example. Say we have a very large population where 1 in every 1000 has a particular disease ($p = 1/1000$). We could model the disease outcome from randomly drawing an individual from the population using a Bernoulli Model where $P(X = 1) = 0.001$. If we just randomly drew 1 person, we'd expect 0.001 of a person to have the disease and a measure of the variability in outcomes is $0.001*0.999 = 0.000999$. These values don't make a lot of sense because we often sample more than one person.

Imagine we had a sample of $n$ individuals from the population. Then we are considering $n$ independent Bernoulli "trials". If we let the random variable be the count the number of successes in a sample of size $n$, then we can use the **Binomial Model.**

The probability mass function is $P(X = x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$ where $x$ is the number of successes in $n$ trials. In the long run, the expected number of successes will be $np$ and the variance will be $np(1-p)$. In the long run, the expected relative frequency of successes in $n$ trials will be $p$ and the variance will be $\frac{p(1-p)}{n}$. 

If we randomly draw 5000 people, we'd expect 0.001\*5000 = 5 people to have the disease and a measure of the variability in count is $5000*0.001*0.999 = 4.995$. Let's adjust this to relative frequencies. We'd expect 0.1% of the people to have the disease and a measure of the variability in relative frequency is $0.001*0.999/5000 = 1.998e-7$.


Let's look at the probability mass function for the Binomial Model for $n = 10, p = 0.01$.


```{r}
n = 10
p = 0.01
barplot(dbinom(0:n,size = n, p = p),names.arg = 0:n,ylab='Probability',main='n = 10, p = 0.01')
```

If we increase the sample size to n = 500, then we see this.

```{r}
n = 500
p = 0.01
barplot(dbinom(0:n,size = n, p = p),names.arg = 0:n,ylab='Probability',main='n = 500, p = 0.01')
```

Let's zoom in on the left hand side of this plot.

```{r}
n = 500
p = 0.01
barplot(dbinom(0:25,size = n, p = p),names.arg = 0:25,ylab='Probability',main='n = 500, p = 0.01')
```

What does this look like?



## Normal Model

We've been introduced to the Normal model already as a smooth version of a unimodal, symmetric histogram. For a quantitative random variable $X$ (that can take any real number), if the expected value is $\mu$ and the variance is $\sigma^2$, a Normal random variable has a probability density function of
$$f(x) =  \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

For every, potential value of $\mu$ and $\sigma$, there is a different curve.

```{r}
x = seq(-10,10,.01)
f = dnorm(x,mean = 0, sd = 1)
plot(x,f,type='l',ylab='f(x)',bty='n',ylim=c(0,.9))
f2 = dnorm(x,mean = 5, sd = 1)
lines(x,f2,col='red')
f3 = dnorm(x,mean = 5, sd = 0.5)
lines(x,f3,col='blue')
f4 = dnorm(x,mean = 0, sd = 3)
lines(x,f4,col='purple')
```

- In general, the center of the distribution is $\mu$ and the standard devation, square root of the variance, determines the spread of the distribution.

```{r}
x = seq(-5,5,.01)
f = dnorm(x,mean = 0, sd = 1)
plot(x,f,type='l',xaxt='n',ylab='f(x)',bty='n')
axis(1,at=c(-3,-2,-1,0,1,2,3),labels=expression(mu - 3*sigma,mu - 2*sigma,mu - sigma,mu,mu + sigma,mu + 2*sigma,mu +3*sigma))
```

- If we consider $\mu=0$ and $\sigma=1$, we know that $P(-1\leq X \leq 1) = 0.68$, calculated as the area under the curve between -1 and 1.

```{r}
plot_area_std_normal(-1, 1, title = "P(-1 < X < 1)")
```

```{r echo=TRUE}
pnorm(1) - pnorm(-1) 
#pnorm(1) gives the area under the curve to the left of 1
#pnorm(-1) gives the area under the curve to the left of -1
```

- We know that $P(-2\leq X \leq 2) = 0.95$, calculated as the area under the curve between -2 and 2.

```{r}
plot_area_std_normal(-2, 2, title = "P(-2 < X < 2)")
```

```{r echo=TRUE}
pnorm(2) - pnorm(-2)
```

- - We know that $P(-3\leq X \leq 3) = 0.997$, calculated as the area under the curve between -3 and 3.

```{r}
plot_area_std_normal(-3, 3, title = "P(-3 < X < 3)")
```

```{r echo=TRUE}
pnorm(3) - pnorm(-3)
```

No matter what the long-run average and variability are, if we standardize our data by subtracting $\mu$ and dividing by $\sigma$, then we can focus solely on the areas based on z-score values rather that in original values. 


Important: If a random variable can be modeled with a Normal model, then we know that about 95% of the time, the values will be within 2 standard deviations of the expected value. 


## Sampling Distribution and CLT


In construction!

- Define the Central Limite Theorem
- Connecting CLT and bootstrapping
- SE (estimate of RV SD), z-scores ( value - E(X) / SE)
- Brief Intro to Student T




