```{r setup4, include=FALSE}
library(broom)
library(dplyr)
library(ggplot2)
library(stringr)
library(rvest)
library(mosaicData) 
library(ggmosaic)
library(NHANES)
library(mosaic)
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
```

# Logistic Regression Models {#logistic}

If you want to predict a binary categorical variable (only 2 possible outcomes), the standard linear regression models don't apply. If you let the two possible outcome values be $Y=0$ and $Y=1$, you'll never get a straight line relationship with any $X$ variable. 

Throughout this section, we will refer to one outcome as "success" (denoted $Y=1$) and "failure" (denoted $Y=0$). Depending on the context of the data, the success could be a negative thing such as "heart attack" or "20 year mortality" or it could be a positive thing such as "passing a course".

We will denote $p$ to be the chance of success and $1-p$ be the chance of failure. You can think of $p$ as the expected outcome, $E[Y]$. For example, if we consider flipping a coin with 2 equally likely sides, we'd expected a one side (say Heads) 50% of the time, which is the chance of getting that side (Heads). Yes, it is not possible for $Y$ to be equal to 0.5 because in this context, $Y=0$ or $Y=1$, but the $E[Y]$ gives the relative frequency of successes in the long run. 

We want to build a model to explain why the chance of one outcome (success) may be higher for one group of people in comparison to another. 

## Logistic and Logit

Since we can't use a straight line model, we'll need to consider other mathematical functions to model the relationship between $X$ and the chance of success, $p=E[Y]$. 

The **logistic function** is an S shaped curve (sigmoid curve). For our purposes, the function will take the form

$$f(x) = \frac{1}{1 + e^{\beta_0 +\beta_1x}}$$

```{r echo=FALSE}
x <- seq(-10,10,by=.1)
beta0 <- 0
beta1 <- -0.5
plot(x, 1/(1 + exp(beta0 + beta1*x)), type = 'l', ylim = c(0,1), ylab = 'f(x)')
```

For any real value $x$, this function, $f(x)$, will be a value between 0 and 1. This is perfect for us since probabilities or chances should also be between 0 and 1. 

In fact, we'll let the chance of failure outcome (when $Y=0$), $1-p$, be modeled by this S function.

$$1-p = \frac{1}{1 + e^{\beta_0 +\beta_1X}}$$
```{block, type='mathbox'}
With a bit of algebra and rearranging terms, we can write this equation in terms of $p$, the chance of success. 

$$1-p = \frac{1}{1 + e^{\beta_0 +\beta_1X}}$$
$$p = 1-\frac{1}{1 + e^{\beta_0 +\beta_1X}}$$

$$p = 
\frac{1 + e^{\beta_0 +\beta_1X}}{1 + e^{\beta_0 +\beta_1X}}-\frac{1}{1 + e^{\beta_0 +\beta_1X}}$$

$$p = \frac{e^{\beta_0 +\beta_1X}}{1 + e^{\beta_0 +\beta_1X}}$$

```

Let's define one more term. The **odds** of a success is the ratio of the chance of success to the chance of failure, odds $=p/(1-p)$.

```{block, type='mathbox'}
With a bit more algebra and rearranging terms, we can write the above model as a linear regression model.

$$p/(1-p) = \frac{e^{\beta_0 +\beta_1X}}{1 + e^{\beta_0 +\beta_1X}}/\frac{1}{1 + e^{\beta_0 +\beta_1X}}$$
$$p/(1-p) = e^{\beta_0 +\beta_1X}$$
$$\log(p/(1-p)) = \beta_0 +\beta_1X$$
```

This is a **simple logistic regression model**. On the left hand side, we have the natural log of the odds of success for a given value of $X$. The log odds is called the **logit** function. Think of this as a transformed version of our expected outcome for a given value of $X$. On the right hand side, we have a familiar linear model equation. 

$$\log\left(\frac{p}{1-p}\right) =\log\left(\hbox{Odds of Success given } X\right) = \log\left(\frac{E[Y|X]}{1-E[Y|X]}\right) = \beta_0 +\beta_1X$$

Like a linear regression model, we can extend this model to a **multiple logistic regression model** by adding additional $X$ variables,

$$\log\left(\frac{p}{1-p}\right) =\log\left(\hbox{Odds of Success given } X_1,...,X_k\right)= \log\left(\frac{E[Y|X_1,...,X_k]}{1-E[Y|X_1,...,X_k]}\right) \\ = \beta_0 +\beta_1X_1+\beta_2X_2+\beta_3X_3+\cdots +\beta_kX_k$$

## Fitting the Model

Based on observed data that includes an indicator variable for the responses ($Y=1$ for success, $Y=0$ for failure) and predictor variables, we need to find the slope coefficients, $\beta_0$,...,$\beta_k$ that best fits the data. The way we do this is through a technique called **maximum likelihood estimation**. We will not discuss the details in this class; we'll save this for an upper level statistics class such as Mathematical Statistics.

In R, we do this with the **g**eneralized **l**inear **m**odel function, `glm()`.

For a data set, let's go back in history to January 28, 1986. On this day, the U.S. space shuttle called Challenger took off and tragically exploded about one minute after the launch. After the incident, scientists ruled that the disaster was due to an o-ring seal failure. Let's look at experimental data on the o-rings prior to the fateful day.

```{r echo=TRUE}
require(vcd)
data(SpaceShuttle)

SpaceShuttle %>%
  filter(!is.na(Fail)) %>%
  ggplot(aes(x = factor(Fail), y = Temperature) )+ 
  geom_boxplot() +
  theme_minimal()

SpaceShuttle %>%
  filter(!is.na(Fail)) %>%
  group_by(Temperature) %>%
  summarise(PercentFail = mean(Fail == 'yes')) %>%
  ggplot(aes(x = Temperature, y = PercentFail)) + 
  geom_point() +
  theme_minimal()
```

```{block, type="reflect"}
What are the plots above telling us about the relationship between chance of o-ring failure and temperature?
```


Let's fit a simple logistic regression model with one explanatory variable to predict the chance of o-ring failure (which is our "success" here -- we know it sounds morbid) based on the temperature using the experimental data. 

```{r echo=TRUE}
model.glm <- glm(Fail ~ Temperature, data = SpaceShuttle, family = binomial)
summary(model.glm)
```

Based on a logistic regression model, the predicted log odds of an o-ring failure is given by

$$\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = 15.0429 -0.2322\cdot Temperature$$


## Coefficient Interpretation

Let's take a look at the estimates from the model. What do they mean?

```{r echo=TRUE}
summary(model.glm)
```

The slope coefficient (-0.232) tells you how much the predicted log odds of o-ring failure increase with an increase of 1 degree  in temperature. But what does a 1 unit increase in log odds mean? We need to do a bit of algebra to get something more interpretable. 

```{block, type='mathbox'}
Our model is

$$\log\left(\frac{p}{1-p}\right) = \log\left(\frac{E[Y|X]}{1-E[Y|X]}\right) = \beta_0 +\beta_1X$$

Imagine considering a particular value of $X=x$ (such as temperature = $x$), 

$$\log\left(\frac{E[Y|X=x]}{1-E[Y|X=x]}\right) = \beta_0 +\beta_1x$$

If we increase $X$ by 1, then we expected a change in our outcome or chance of success, $E[Y|X = x+1]$,

$$\log\left(\frac{E[Y|X= x+1]}{1-E[Y|X= x+1]}\right) = \beta_0 +\beta_1(x+1)$$

Let's find the difference between these two equations,

$$\log\left(\frac{E[Y|X=x+1]}{1-E[Y|X=x+1]}\right)- \log\left(\frac{E[Y|X=x]}{1-E[Y|X=x]}\right)=  (\beta_0 +\beta_1(x+1)) - ( \beta_0 +\beta_1x)$$
and simplify the right hand side (we love it when things cancel!),

$$\log\left(\frac{E[Y|X=x+1]}{1-E[Y|X=x+1]}\right)- \log\left(\frac{E[Y|X=x]}{1-E[Y|X=x]}\right)=  \beta_1$$
and then simplify the left hand side (using our rules of logarithms),

$$\log\left( \frac{E[Y|X=x+1]/(1-E[Y|X=x+1])}{E[Y|X=x]/(1-E[Y|X=x])}\right) = \beta_1$$

Let's exponentiate both sides,

$$\left( \frac{E[Y|X=x+1]/(1-E[Y|X=x+1])}{E[Y|X=x]/(1-E[Y|X=x])}\right)= e^{\beta_1}$$

$$\left( \frac{\hbox{Odds of Success when }X=x+1}{\hbox{Odds of Success when }X=x}\right)= e^{\beta_1}$$
```

After that algebra, we find that $e^{\beta_1}$ is equal to the **odds ratio**, the ratio of the odds of success between two groups of values (those with $X=x+1$ and those with $X=x$, 1 unit apart)


When we fit the model to the o-ring experimental data, we estimate the coefficients to be $\hat{\beta}_0 = 15.04$ and $\hat{\beta}_1 = -0.232$. So the estimated odds ratio is $e^{\hat{\beta}_1} = e^{-0.232} = 0.793$ for an increase in one degree Fahrenheit in temperature (Temperature=T+1 v. Temperature=T). 

- If a ratio is greater than 1, that means that the denominator is less than the numerator or equivalently the numerator is greater than denominator (odds of success are greater with T+1 as compared to T --> positive relationship between $X$ variable and odds of success). 

- If a ratio is less than 1, that means that the denominator is greater than the numerator or equivalently the numerator is less than denominator (odds of success are less with T+1 as compared to T --> negative relationship between $X$ variable and odds of success). 

- If the ratio is equal to one, the numerator equals the denominator (odds of success are equal for the two groups --> no relationship).


In this case, we have an odds ratio < 1 which means that the estimated odds of o-ring failure is lower for increased temperatures (in particular by increasing by 1 degree Fahrenheit). This makes sense since we saw in experimental data that the proportion of o-ring failures was lower in warmer temperatures. 

## Logistic Model Prediction

On January 28, 1986, the temperature was 26 degrees F. Let's predict the chance of "success," which is a failure of o-rings in our data context, at that temperature.  

```{r echo=TRUE}
predict(model.glm, newdata = data.frame(Temperature = 26), type = 'response') #type = 'response' gives predicted chance, rather than predicted odds
```

They didn't have any experimental data testing o-rings at this low of temperatures, but even based on the data collected, we predict the chance of failure to be nearly 1 (near certainty). 

### Hard Predictions/Classifications

These predicted chances of "success" are useful to give us a sense of uncertainty in our prediction. If the chance of o-ring failure were 0.8, we would be fairly certain that a failure were likely but not absolutely. If the chance of o-ring failure were 0.01, we would be pretty certain that a failure was not likely to occur. 

But if we had to decide whether or not we should let the shuttle launch go, how high should the predicted chance be to delay the launch (even if it would cost a lot of money to delay)?

It depends. 

If we used a threshold of 0.8, then we'd say that for any experiment with a predicted chance of o-ring failure 0.8 or greater, we'll predict that there will be o-ring failure. As with any predictions, we may make an error. With this threshold, what is our **accuracy** (# of correctly predicted/# of data points)? 

In the table below, we see that there were 3 data points in which we correctly predicted o-ring failure (using a threshold of 0.8). There were 4 data points in which we erroneously predicted that it wouldn't fail when it actually did, and we correctly predicted no failure for 16 data points. So in total, our accuracy is (16+3)/(16+4+3) =  0.82 or 82%. 

```{r}
augment(model.glm, type.predict ='response') %>%
  mutate(predictFail = .fitted >= 0.8) %>%
  count(Fail, predictFail)
```

The only errors we made were **false negatives** (predict no "success" because $\hat{p}$ is less than threshold when outcome was true "success", $Y=1$); we predicted that the o-ring wouldn't fail  but the o-rings did fail in the experiment ($Y=1$). In this data context, false negatives have real consequences on human lives because a shuttle would launch and potentially explode because we had predicted there would be no o-ring failure. 

The **false negative rate** is the number of false negatives divided by the false negatives + true positives (denominator should be total number of experiments with actual o-ring failures). With a threshold of 0.80, our false negative rate is 4/(3+4) = 0.57 = 57%. We failed to predict 57% of the o-ring failures. This is fairly high when there are lives on the line.

The **sensitivity** of a prediction model is the true positives divided by the false negatives + true positives (denominator should be total number of experiments with actual o-ring failures). It is 1-false negative rate, so the model for o-ring failures has a 1-0.57 = 0.43 or 43% sensitivity. 

A **false positive** (predict "success" because $\hat{p}$ is greater than threshold when outcome was not a "success", $Y=0$). A false positive would happen when we predicted o-ring failure when it doesn't actually happen. This would delay launch (cost \$) but have minimal impact on human lives. 

The **false positive rate** is the number of false positives divided by the false positives + true negatives (denominator should be total number of experiments with no o-ring failures). With a threshold of 0.80, our false positive rate is 0/16 = 0. We always accurately predicted the o-rings would not fail.

The **specificity** of a prediction model is the true negatives divided by the false positives + true negatives (denominator should be total number of experiments with no o-ring failures). It is 1-false positive rate, so the model for o-ring failures has a 1-0 = 1 or 100% specificity. 

```{block, type='mathbox'}
To recap,

- The **accuracy** is the overall percentage of correctly predicted outcomes out of the total number of outcome values

- The **false negative rate (FNR)** is the percentage of incorrectly predicted outcomes out of the $Y=1$ "success" outcomes (conditional on "success")

- The **sensitivity** is the percentage of correctly predicted outcomes out of the $Y=1$ "success" outcomes (conditional on "success"); 1 - FNR

- The **false positive rate (FPR)** is the percentage of incorrectly predicted outcomes out of the $Y=0$ "failure" outcomes (conditional on "failure" or "no success")

- The **specificity** is the percentage of correctly predicted outcomes out of the $Y=0$ "failure" outcomes (conditional on "failure" or "no success"); 1 - FPR
```


What if we used a lower threshold to reduce the number of false negatives (those with very real human consequences)? Let's lower it to 0.25 so that we predict o-ring failure more easily. Let's find our accuracy: (10+4)/(10+3+6+4) = 0.61. Worse than before, but let's check false negative rate: 3/(3+4) = 0.43. That's lower. But now we have a non-zero false positive rate: 6/(6+10) = 0.375. So of the experiments with no o-ring failure, we predicted incorrectly 37.5% of the time. 

```{r}
augment(model.glm, type.predict ='response') %>%
  mutate(predictOFail = .fitted >= 0.25) %>%
  count(Fail, predictOFail)
```

Where the threshold goes depends on the real consequences of making those two types of errors and needs to be made in the social and ethical context of the data. 

## Logistic Model Evaluation

In deciding whether a logistic regression model is a good and useful model, we need to consider the accuracy, sensitivity, specificity, the false positive rate,  and the false negative rate. Depending on the context, we may focus on maximizing the overall accuracy or we may focus on minimizing the false negative rate (maximizing sensitivity) or minimizing the false positive rate  (maximizing specificity).  

There are other measures of model fit for a logistic regression such as AIC and BIC (lower is better), but those are beyond the scope of this course. You'll learn about them in future statistics courses. 

## Alternative Classification Models

Logistic regression is a very useful model to predict a binary outcome. However, it has its limitations. We are assuming a linear relationship between explanatory variables and the log odds of success. This is hard to check because we don't have a variable for odds that we could quickly plot. 

Other methods out there are more flexible but also more complex. Here is a list of some of the most popular classification methods. 

- Classification Trees can predict a binary outcome and choose the variables that are most important by recursively partitioning the data into groups that are more similar in terms of the outcome as well as in chosen predictor variables.

- Random Forests are an ensemble of classification tress that together are more stable than any one classification tree.

- Boosted Trees are classification trees that are sequentially created to target the errors from the last tree.

- Neural Networks are a type of classification algorithm that creates new features based on the original data that are the best predictors of the outcome. 

Take Statistical Machine Learning to learn more about these methods. But keep in mind that sometimes for a task, complex is not necessary: see https://www.huffingtonpost.com/2014/02/10/klemens-torggler-evolution-door_n_4762261.html.

