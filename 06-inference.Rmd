---
title: "Math 155 - Statistical Inference"
author: "Prof. Heggeseth"
date: "September 6, 2018"
output: 
  html_document: default
---

```{r setup6, echo=FALSE}
library(stringr)
library(infer)
library(nycflights13)
library(broom)
library(dplyr)
library(ggplot2)
library(stringr)
library(rvest)
library(mosaicData) 
library(ggmosaic)
library(NHANES)
library(mosaic)
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical Inference

Let's remember our goal of "turning data into information." Based on a sample data set, we want to be able to say something about the larger population of interest. This endeavor is called **statistical inference**. In statistical inference, we care about using sample data to make statements about "truths" in the larger population.

- To make causal inferences in the sample, we need to account for all possible confounding variables, or we need to randomize the "treatment" and assure there are no other possible reasons for an observed effect.
- To generalize to a larger population, we need the sample to be representative of the larger population. Ideally, that sample would be randomly drawn from the population. If we actually have a census in that we have data on country, state, or county-level, then we can consider the observed data as a "snapshot in time". There are random processes that govern how things behave over time, and we have just observed one period in time.

Let's do some statistical inference based on a simple random sample (SRS) of 100 flights leaving NYC in 2013. 

```{block type="reflect", echo=TRUE}
What is our population of interest? What population could we generalize to?
```

```{r}
set.seed(2018)
## This creates a dataset called flights_samp
## that contains the SRS of size 100
flights_samp <- flights %>%
    sample_n(size = 100)
```

We've already been thinking about random variation and how that plays a role in the conclusions we can draw. In this chapter, we will formalize two techniques that we use to do perform statistican inference: confidence intervals and hypothesis tests. 


## Confidence Intervals

A **confidence interval** (also known as an interval estimate) is an interval of plausible values of the unknown population parameter of interest based on randomly sampled data. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the population, the confidence interval obtained from the data is also random.

The **confidence level** represents the proportion of possible random samples and thus confidence intervals that contain the true value of the unknown population parameter. Typically, the confidence level is represented by $(1-\alpha)$ such that if $\alpha = 0.05$, then the confidence level is 95% or 0.95. What is $\alpha$? We will define $\alpha$ when we get to hypothesis testing, but for now, we will describe $\alpha$ as an error probability. Because we want an error probability to be low, it makes sense that the confidence level is $(1-\alpha)$.


**Valid Interpretation:** Assuming the sampling distribution model is accurate, I am 95% confident that my confidence interval of (lower, upper) contains the true population parameter (*put in context*), which means that we'd expect 95% of samples to lead to intervals that contain the true population parameter value. We just don't know if our particular interval from our study contains that true population parameter value or not.

### Via Classical Theory

If we can use theoretical probability to approximate the sampling distribution, then we can create a confidence interval by taking taking our estimate and adding and subtracting a margin of error:

$$\text{Estimate }\pm \text{ Margin of Error}$$

The margin of error is typically constructed using z-scores from the sampling distribution (such as $z^* = 2$ that corresponds to a 95% confidence interval) and an estimate of the standard deviation of the estimate, called a **standard error**. 

Once we have an estimate of the standard deviation (through a formula or R output) and an approximate sampling distribution, we can create the interval estimate:

$$\text{Estimate }\pm z^* *SE(\text{Estimate})$$

The fact that confidence intervals can be created as above is rooted in the Central Limit Theorem (CLT). If you would like to see how the form above is derived, see the Math Box below.


```{block, type="math"}
(Optional) Deriving confidence intervals from the CLT

The CLT originally expresses that 

$$ \frac{\text{sample mean} - \text{true mean}}{\text{true std. error of sample mean}} \sim \text{Normal}(0,1) $$

It turns out that the CLT also applies to regression coefficients:

$$ \frac{\hat{\beta} - \beta}{\text{ESTIMATED std. error of }\hat{\beta}} \sim \text{Normal}(0,1)$$

From there we can write a probability statement using the 68-95-99.7 rule of the normal distribution and rearrange the expression using algebra:

$$P\left(-2 < \frac{\hat{\beta} - \beta}{\text{ESTIMATED std. error of }\hat{\beta}} < 2 \right) = 0.95$$
$$P\left(-2 SE < \hat{\beta} - \beta < 2 SE \right) = 0.95$$
$$P\left(-2 SE -\hat{\beta} <  -\beta < 2 SE - \hat{\beta} \right) = 0.95$$
$$P\left(2 SE + \hat{\beta} > \beta > -2 SE + \hat{\beta} \right) = 0.95$$

You've seen the Student t distribution introduced in the previous chapter. We used the Normal distribution in this derivation, but it turns out that the Student t distribution is more accurate for linear regression coefficients (especially if sample size is small). The normal distribution is appropriate for logistic regression coefficients.
```

### Via Bootstrapping


In order to gauge the sampling variability, we can treat our sample as our "fake population" and generate repeated samples from this "population" using the technique of bootstrapping.

Once we have a distribution of sample statistics based on the generated data sets, we'll create a confidence interval by finding the $\alpha/2$th percentile and the $(1-\alpha/2)$th percentile for our lower and upper bounds. For example, for a 99% bootstrap confidence interval, $\alpha = 0.01$ and you would find the values that are the 0.5th and 99.5th percentiles.

## Confidence Interval Examples

### Proportion Outcome

Let's return to the flight data and estimate the proportion of afternoon flights based on a sample of 100 flights from NYC. 

First, the classical 95% confidence interval can be constructed using the theory of the Binomial Model (Do the 3 conditions hold? Is n large enough for it to look Normal?)

```{r}
flights_samp %>% 
  summarize(prop = count(day_hour)/100) %>%
  mutate(SE = sqrt(prop*(1-prop)/100)) %>%
  mutate(lb = prop - 2*SE, ub = prop + 2*SE)
```

Or we could bootstrap and get our confidence interval that way. 

```{r}
alpha <- 0.05

boot_data <- mosaic::do(1000)*( 
    flights_samp %>% # Start with the SAMPLE (not the FULL POPULATION)
      sample_frac(replace = TRUE) %>% # Generate by resampling with replacement
      summarize(prop = count(day_hour)/100) # Calculate statistics
)

boot_data %>%
  summarize(lower = quantile(prop, alpha/2),
    upper = quantile(prop, 1-alpha/2))
```

Our confidence interval gives a sense of the true proportion of flights departed NYC in the afternoon, keeping in mind that this sample could be one of the unlucky samples (the 5%) that have intervals that don't contain the true value.

### Mean and then Median

Perhaps you really care about the arrival delay time because you have somewhere important you need to be when you take flights out of NYC. Let's estimate the mean arrival delay based on a sample of 100 flights from NYC. 

First off, let's create a classical confidence interval. Since our sample size is relatively large, we can use the Normal model (instead of William Gosset's work). We use the sample standard deviation and plug into the SE formula. 

```{r}
flights_samp %>% 
  summarize(mean = mean(arr_delay), s = sd(arr_delay)) %>%
  mutate(SE = s/sqrt(100)) %>%
  mutate(lb = mean - 2*SE, ub = mean + 2*SE)
```


```{r}
alpha <- 0.05

boot_data <- mosaic::do(1000)*( 
    flights_samp %>% # Start with the SAMPLE (not the FULL POPULATION)
      sample_frac(replace = TRUE) %>% # Generate by resampling with replacement
      summarize(means = mean(arr_delay),medians = median(arr_delay)) # Calculate statistics
)

boot_data %>%
  summarize(lower = quantile(means, alpha/2),
    upper = quantile(means, 1-alpha/2))
```

Our confidence interval gives potential values for of the true mean arrival delay for flights that departed NYC, keeping in mind that this sample could be one of the unlucky samples (the 5%) that have intervals that don't contain the true value. Also remember that the mean is sensitive to outliers...Let's consider the median.

We get a slightly different story if we are interested in the middle number versus the average. But notice, we aren't using a classical CI here because the sampling distribution of a median is not necessarily Normal. 


```{r}
boot_data %>%
  summarize(lower = quantile(medians, alpha/2),
    upper = quantile(medians, 1-alpha/2))
```

### Logistic Regression Model

Are the same relative numbers of morning flights in the winter and the summer? Let's fit a logistic regression model and see what our sample says.

```{r}
flights_samp$afternoon = flights_samp$day_hour == 'afternoon'

glm.afternoon <- glm(afternoon ~ season, data = flights_samp, family = 'binomial')
summary(glm.afternoon)
```

The output for the model gives standard errors for the slopes, so we can create the classical confidence intervals directly from the output, 

```{r}
confint(glm.afternoon)
```

Or with bootstrapping,

```{r}
boot_data <- mosaic::do(1000)*( 
    flights_samp %>% # Start with the SAMPLE (not the FULL POPULATION)
      sample_frac(replace = TRUE) %>% # Generate by resampling with replacement
      glm(afternoon ~ season, data = ., family = 'binomial') # Calculate statistics
)

boot_data %>%
  summarize(lower = quantile(seasonwinter, alpha/2),
    upper = quantile(seasonwinter, 1-alpha/2))
```

Knowing that the sample is random, the interval estimate for the logistic regression slope is given by the confidence interval. But for logistic regression, we exponentiate the slopes to get an more interpretable value, the odds ratio. Here, we are comparing the odds of having a flight in the afternoon between winter months (numerator) and summer months (denominator). Is 1 in the interval? If so, what does that tell you?

```{r}
exp(confint(glm.afternoon))

boot_data %>%
  summarize(lower = exp(quantile(seasonwinter, alpha/2)),
    upper = exp(quantile(seasonwinter, 1-alpha/2)))
```



### Linear Regression Model Slope (Categorical Variable)

Everything says that there are longer delays in winter. Is that actually true? Let's fit a linear regression model to test it

```{r}
lm.delay <- lm(arr_delay ~ season, data = flights_samp)
summary(lm.delay)
```

The classical CI for the slope is given with by

```{r}
confint(lm.delay)
```

or with bootstrapping,

```{r}
boot_data <- mosaic::do(1000)*( 
    flights_samp %>% # Start with the SAMPLE (not the FULL POPULATION)
      sample_frac(replace = TRUE) %>% # Generate by resampling with replacement
      lm(arr_delay ~ season, data = .) # Calculate statistics
)

boot_data %>%
  summarize(lower = quantile(seasonwinter, alpha/2),
    upper = quantile(seasonwinter, 1-alpha/2))
```

The 95% confidence interval gives a sense of the difference of mean arrival delays of flights between winter and summer is given by the confidence interval. Is zero in the interval? If so, what does that tell you?


### Linear Regression Model Slope (Quantitative Var)

How well can the departure delay predict the arrival delay? What is the effect of departing 1 more minute later? Does that correspond to 1 minute later in arrival on average? Let's look at the estimated slope between departure and arrival delays for the sample of 100 flights from NYC.

```{r}
lm.delay2 <- lm(arr_delay ~ dep_delay, data = flights_samp)
summary(lm.delay2)
```

The classical CI for the slope is given with by

```{r}
confint(lm.delay2)
```

or with bootstrapping,

```{r}
boot_data <- mosaic::do(1000)*( 
    flights_samp %>% # Start with the SAMPLE (not the FULL POPULATION)
      sample_frac(replace = TRUE) %>% # Generate by resampling with replacement
      lm(arr_delay ~ dep_delay, data = .) # Calculate statistics
)

boot_data %>%
  summarize(lower = quantile(dep_delay, alpha/2),
    upper = quantile(dep_delay, 1-alpha/2))
```


If the flight leaves an additional minute later, then we'd expect the arrival delay to be increased by a value in the interval above, on average. 

### Confidence Intervals for Prediction

Imagine we are on a plane, we left 15 minutes late, how late will arrive? Since we only have a sample of 100 flights, we are a bit unsure of our prediction. 

A classical CI can give us an interval estimate of what the prediction should be (if we had data on all flights).

```{r}
predict(lm.delay2, newdata = data.frame(dep_delay = 15), interval = 'confidence')
```

This is taking into account how uncertain we are about our model prediction because our model is based on sample data rather than population data. 


### Prediction Intervals

We also know that every flight is different (different length, different weather conditions, etc), so the true arrival delay won't be exactly what we predict. 

So to get a better prediction for our arrival delay, we can account for the size of errors or residuals by creating a **prediction interval**. This interval will be much wider than the confidence interval because it takes into account how far the true values are from the prediction line. 

```{r}
predict(lm.delay2, newdata = data.frame(dep_delay = 15), interval = 'prediction')
```


### Probability Theory vs. Bootstrapping

In the modern age, computing power allows us to perform boostrapping easily to create confidence intervals. Before computing was as powerful as it is today, scientists needed mathematical theory to provide simple formulas for confidence intervals.

If certain assumptions hold, the mathematical theory proves to be just as accurate and less computationally-intensive than bootstrapping. Many scientists using statistics right now learned the theory because when they learned statistics, computers were not powerful enough to handle techniques such as bootstrapping.

Why do we teach both the mathematical theory and bootstrapping? You will encounter both types of techniques in your fields, and you'll need to have an understanding of what these techniques are to bridge the gap until statistical inference uses modern computational techniques more widely.

## Hypothesis Testing

Hypothesis testing is another tool that can be used for statistical inference. Let's warm up to the ideas of hypothesis testing by considering two broad types of scientific questions: (1) *Is there* a relationship? (2) *What* is the relationship?

Suppose that we are thinking about the relationship between housing prices and square footage. Accounting for sampling variation...

- ...**is there** a relationship between price and living area?
- ...**what** is the relationship between price and living area?

Whether by the Central Limit Theorem (mathematical theory) or bootstrapping, confidence intervals provide a *range of plausible values* for the true population parameter and allow us to answer both types of questions:

- **Is there** a relationship between price and living area?
    - Is the no difference/relationship value in the interval?
- **What** is the relationship between price and living area?
    - Look at the estimate and the values in the interval


**Hypothesis testing** is a general framework for answering questions of the first type. It is a general framework for making decisions between two "theories".

- **Example 1**    
    Decide between: true support for a law = 50% vs. true support $\neq$ 50%

- **Example 2**    
    In the model $\text{Price} = \beta_0 + \beta_1\text{Area}$, decide between $\beta_1 = 0$ and $\beta_1 \neq 0$.

In a hypothesis test, we use data to decide between two "hypotheses" labeled as follows:

1. **Null hypothesis** ($H_0$ = "H naught")    
Hypothesis that is assumed to be true by default.    
A status quo hypothesis: hypothesis of no effect/relationship/difference.

2. **Alternative hypothesis** ($H_A$ or $H_1$)    
A non-status quo hypothesis.    
Claim being made about the population.

### Test statistics

Let's consider the question: Is there a relationship between house price and living area? We can try to answer that with the linear regression model below:

$$\text{Price} = \beta_0 + \beta_1\text{Area}$$

We would phrase our null and alternative hypotheses as follows:

$$H_0: \beta_1 = 0 \qquad \text{vs.} \qquad H_A: \beta_1 \neq 0$$

The null hypothesis $H_0$ describes the situation of "no relationship" because it hypothesizes that the true slope $\beta_1$ is 0. The alternative hypothesis posits a relationship: the true slope $\beta_1$ is not 0. That is, there is not no relationship. (Double negatives!)

To gather evidence, we collect data and fit a model. From the model, we can compute a **test statistic**, which tells us how far the observed slope is from the null hypothesis value of 0 (called the **null value**). The test statistic is a *discrepancy measure* where large values indicate higher discrepancy with $H_0$.

The test statistic below is a reasonable proposal:

$$\text{Test statistic} = \frac{\text{estimate} - \text{null value}}{\text{std. error of estimate}}$$

It looks like a z-score. It expresses: how far away is our estimate from the null value in units of standard error? With large values (in magnitude) of the test statistic, our data (our estimate) is discrepant with what the null hypothesis proposes because our estimate is quite far away from the null value in standard error units.

### Logic of hypothesis testing

How large in magnitude must the test statistic be in order to make a decision between $H_0$ and $H_A$? We will use another metric called a **p-value**.

**Assuming $H_0$ is true**, we ask: What is the chance of observing a test statistic which is "as or even more extreme" than the one we just saw? This probability is called a **p-value**.

If our test statistic is large, then our estimate is quite far away from the null value (in standard error units), and then the chance of observing someone this large or larger (assuming $H_0$ is true) would be very small. **A large test statistic leads to a small p-value.**

If our test statistic is small, then our estimate is quite close to the null value (in standard error units), and then the chance of observing someone this large or larger (assuming $H_0$ is true) would be very large. **A small test statistic leads to a large p-value.**

#### Making Decisions

If the p-value is "small", then we reject $H_0$ in favor of $H_A$. Why? A small p-value (by definition) says that if the null hypotheses were indeed true, we are unlikely to have seen such an extreme discrepancy measure (test statistic). We made an assumption that the null is true, and operating under that assumption, we observed something odd and unusual. This makes us reconsider our null hypothesis.

How small is small enough for a p-value? We will set a threshold $\alpha$. P-values less than this threshold will be "small enough". When we talk about error rates of the decisions associated with rejecting or not rejecting the null hypothesis, the meaning of $\alpha$ will become more clear.

### Summary of procedure

1. State hypotheses $H_0$ and $H_A$.
2. Select $\alpha$, a threshold for what is considered to be a small enough p-value.
3. Calculate a test statistic
4. Calculate the corresponding p-value
5. Make a decision:
    - If p-value < $\alpha$, reject $H_0$ and accept $H_A$.
    - Otherwise, we fail to reject $H_0$ for lack of evidence.    
        (Jurors' decisions are "guilty" and "not guilty". Not "guilty" and "innocent".)

### Testing single model coefficients

A big emphasis of our course is regression models. It turns out that many scientific questions of interest can be framed with regression models.

In the `summary()` output, R performs the following hypothesis test by default (for any regression coefficient $\beta$):

$$H_0: \beta = 0 \qquad \text{vs} \qquad H_A: \beta \neq 0$$

$$\text{Test statistic} = \frac{\text{estimate} - \text{null value}}{\text{std. error of estimate}} = \frac{\text{estimate} - 0}{\text{std. error of estimate}} $$

Note that test statistics are random variables! Why? Because they are based on our random sample of data. Thus it will be helpful to understand the distributions of test statistics in terms of probability density functions.

```{block type="reflect", echo=TRUE}
If $H_0$ were true, where would the probability density function of the test statistic be centered?
```

### Distributions of test statistics

What test statistics are we likely to get if $H_0$ is true? The probability density function of the test statistic "under $H_0$" (that is, if $H_0$ is true) is shown below. Note that it is centered at 0. This distribution shows that if indeed the null is true, there is variation in the test statistics we might obtain from random samples, but most test statistics are around zero.

It would be very unlikely for us to get a pretty large (extreme) test statistic if indeed $H_0$ were true. Why? The density drops rapidly at more extreme values.

```{r echo=FALSE, fig.align="center", fig.height=5.3, fig.width=8}
x <- seq(-4,4,0.01)
y1 <- dnorm(x)
y2 <- dt(x, df = 20)
plot(x, y1, type = "l", xlab = "Test statistic", ylab = "Density", main = expression(paste("Sampling distribution of test statistic if ", H[0], " true (\"under ", H[0], "\")")))
lines(x, y2, col = "red", lty = "dashed")
legend("topright", legend = c("Logistic regression\n(Normal distribution)", "", "Linear regression\n(Student's t-distribution)\n df = 20 here"), col = c("black", NA, "red"), lty = c("solid", NA, "dashed"), bty = "n", cex = 0.9)
```

### Graphical description of p-values

Suppose that our observed test statistic is 2.

What test statistics are "as or more extreme"?

- Absolute value of test statistic is at least 2: $|\text{Test statistic}| \geq 2$
- In other words: $\text{Test statistic} \geq 2$ and $\text{Test statistic} \leq -2$

The p-value corresponding to our test statistic is the area under the probability density function in those "as or more extreme" regions.

```{r echo=FALSE, fig.align="center", fig.height=4.5, fig.width=10}
x <- seq(-4,4,0.01)
y <- dnorm(x)
plot(x, y, type = "l", xlab = "Test statistic", ylab = "Density", main = expression(paste("Sampling distribution of test statistic if ", H[0], " true (\"under ", H[0], "\")")))
bool <- x >= 2 & x <= 4
x_shaded <- x[bool]
y_shaded <- y[bool]
abline(v = c(-2,2), col = "red", lty = "dashed", lwd = 2)
polygon(x = c(x_shaded, tail(x_shaded, 1), head(x_shaded, 1)), y = c(y_shaded, 0, 0), col = "darkorchid")
bool <- x >= -4 & x <= -2
x_shaded <- x[bool]
y_shaded <- y[bool]
polygon(x = c(x_shaded, tail(x_shaded, 1), head(x_shaded, 1)), y = c(y_shaded, 0, 0), col = "darkorchid")
area <- pnorm(-2) + pnorm(2, lower.tail = FALSE)
text(x = 3, y = 0.3, paste("Area = p-value =", round(area, 3)))
```

### Example: Linear Regression

Below we fit a linear regression model of house price on living area:

```{r}
homes <- read.delim("http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt")
mod_homes <- lm(Price ~ Living.Area, data = homes)
confint(mod_homes) ## 95% confidence interval by default
summary(mod_homes)$coefficients
```

The `t value` column is the test statistic, and the `Pr(>|t|)` column is the p-value. Note that the "t" comes from the Student t distribution.

- What are $H_0$ and $H_A$?    
    We write the model $\text{Price} = \beta_0 + \beta_1\text{Area}$.    
    $H_0: \beta_1 = 0$ (There is no relationship between price and living area.)    
    $H_A: \beta_1 \neq 0$ (There is a relationship between price and living area.)    
- For a threshold $\alpha = 0.05$, what is the decision regarding $H_0$?    
    Note that when you see `e` in R output, this means "10 to the power". So `9.486240e-268` means $9.49 \times 10^{-268}$. This p-value is less than our threshold, so we reject $H_0$ and say that we have significant evidence for a relationship between price and living area.
- Is this consistent with the confidence interval?    
    This result is consistent with the 95% confidence interval in that the interval does not contain 0.

### Example: Logistic Regression

Below we fit a logistic regression model of whether a movie made a profit (response) on whether it is a history film:

```{r}
movies <- read.csv("https://www.dropbox.com/s/73ad25v1epe0vpd/tmdb_movies.csv?dl=1")
mod_movies <- glm(profit==TRUE ~ History, data = movies, family = "binomial")
confint(mod_movies) ## 95% confidence interval by default
summary(mod_movies)$coefficients
```

The `z value` column is the test statistic, and the `Pr(>|z|)` column is the p-value. Note that the "z" refers to z-score and the Normal distribution.

Try for yourselves!

- What are $H_0$ and $H_A$?
- For a threshold $\alpha = 0.05$, what is the decision regarding $H_0$?
- Is this consistent with the confidence interval?

###Errors 

Just as with model predictions, we may make errors when doing hypothesis tests. 

We may decide to reject $H_0$ when it is actually true. We may decide to not reject $H_0$ when it is actually false. 

We give these two types of errors names. **Type 1 Error** is when you reject $H_0$ when it is actually true. This is a false positive because you are concluding there is a real relationship when there is none. This would happen if one study published that coffee causes cancer in one group of people, but no one else could actually replicate that result since coffee doesn't actually cause cancer.  **Type 2 Error** is when you don't reject $H_0$ when it is actually false. This is a false negative because you would conclude there is no real relationship when there is a real relationship. This happens when our sample size is not large enough to detect the real relationship due to the large amount of noise due to sampling variability.

We care about both of these types of errors. Sometimes we prioritize one over the other. Based on the framework presented, we control the chance of a Type 1 error through the confidence level/p-value threshold we used. In fact, the chance of a Type 1 Error is $\alpha$,

$$P(\text{ Type 1 Error }) = P(\text{ Reject }H_0 ~|~H_0\text{ is true} ) =  \alpha$$

Let $\alpha = 0.05$ for a moment. If the Null Hypothesis ($H_0$) is actually true, then about 5% of the time, we'd get unusual test statistics just by chance. With those samples, we would incorrectly conclude that there was a real relationship.

The chance of a Type 2 Error is often notated as $\beta$ (but this is not the same value as the slope),

$$P(\text{ Type 2 Error }) = P(\text{ Fail to Reject }H_0 ~|~H_0\text{ is false} ) =  \beta$$

In order to calculate this probability, we'd need to know the value (or at least a good idea) of the true effect. 



## Statistical Significance v. Practical Significance

The common underlying question that we ask as Statisticians is "Is there a real relationship in the population?"

We can use confidence intervals or hypothesis testing to help us answer this question.

If we note that the no relationship value is NOT in the confidence interval or the p-value is less then $\alpha$, we can say that there is significant evidence to suggest that there is a real relationship. We can conclude there is a **statistically significant** relationship because the relationship we observed it is unlikely be due only to sampling variabliliy.

But as we discussed in class, there are two ways you can control the width of a confidence interval. If we increase the sample size $n$, the standard error decreases and thus decreasing the width of the interval. If we decrease our confidence level (increase $\alpha$), then we decrease the width of the interval. 

A relationship is **practically significant** if the estimated effect is large enough to impact real life decisions. For example, an Internet company may run a study on website design. Since data on observed clicks is fairly cheap to obtain, their sample size is 1 million people (!). With large data sets, we will conclude almost every relationship is statistically significant because the variability will be incredibly small. That doesn't mean we should always change the website design. How large of an impact did the size of the font make on user behavior? That depends on the business model. On the other hand, in-person human studies are expensive to run and sample sizes tended to be in the 100's. There may be a true relationship but we can't distinguish the "signal" from the "noise" due to the higher levels of sampling variability. While we may not always have statistical significance, the estimated effect is important to consider when designing the next study. 

Hypothesis tests are useful in determining statistical significance (Answering: "Is there a relationship?").

Confidence intervals are more useful in determining practical significance (Answering: "What is the relationship?")

