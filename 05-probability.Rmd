
```{r setup5, echo=FALSE}
library(stringr)
library(infer)
library(nycflights13)
library(broom)
library(dplyr)
library(ggplot2)
library(stringr)
library(rvest)
library(mosaicData) 
library(ggmosaic)
library(NHANES)
library(mosaic)
knitr::opts_chunk$set(echo = FALSE)
```

# Randomness and Probability

RANDOM VARIABILITY TO PROBABILITY
- HOUSING DATA (FIREPLACE EXAMPLE)




This chapter briefly discusses the theory of formal probability so that we have terminology and basic concepts to understand random events. For more details, take MATH 354 (Probability). This framework provides a way of thinking about **uncertainty,** **random variability**, and **average behavior in the long run.**

A **random process** or **random event** is an process/event that is not and cannot be made exact such that the outcome can not be accurately determined. Examples range from the outcome of flipping a coin to the odds ratio that is estimated in a randomly selected sample. 

We've used the term "chances" up until now and we had defined that the chance of an event is between 0 and 1. We are now going to use "probability" as an equivalent word for "chance". Thus, the probability of an event will be between 0 and 1.



##Three Types of Probability

```{block type='reflect'}
What is the probability of getting a 1 from a six-sided die? Consider how do you know that. How can you justify that number?
```

There are three types of probability.

1. **Empirical Probability:** If you could repeat a random process over and over again, we'd get a sense of the possible outcomes and their associated probabilities by calculating their relative frequency in the long run. If you repeatedly tossed a die, then the relative frequency of 1's after tossing the die MANY times would be the empirical probability. If you repeatedly got a sample of 100 people, the relative frequency of estimated odds ratios below 1 would be the empirical probability of getting an odds ratio below 1. 

2. **Theoretical Probability:** If you don't have time to toss a die a million times, you could calculate probabilities based on mathematical theory and assumptions. You would assume that each side is equally likely to land up, thus the chance of getting a 1, is 1/6 for a six-sided die. 

```{block type='reflect'}
What is the probability you'll get an A in this class? What does that number represent? How can you justify that number?
```

3. **Subjective Probability:** If you use a number between 0 and 1 (100%) to reflect your uncertainty in an outcome (rather than based on empirical evidence or mathematical theory), then you are using subjective probability.

In this class, we'll focus on theoretical and empirical probability. In particular, we will use computational tools to estimate empirical probabilities (using simulations) and mathematical tools to estimate theoretical probabilities. 

##Probability Rules

In theoretical probability, we need to define a few terms and set some rules (known as axioms).

The **sample space,**  $S$, is the set of all possible outcomes of a random process.

- Example: If you flip two coins (one side Heads and one side Tails), then the sample space contains four possible outcomes: Heads and Heads (HH), Heads and Tails (HT), Tails and Heads (TH), and Tails and Tails (TT), $S = \{HH,HT,TH,TT\}$.

A subset of outcomes is called an **event**, $A$. 

- Example: If you flip two coins, an event $A$ could be that exactly one of the coins land Heads, $A = \{HT,TH\}$.



For events $A$ and $B$ and sample space $S$, the probability of an event $A$, notated as $P(A)$, follows the rules below:

- Rule 1: $0\leq P(A)\leq 1$ (probability has to be between 0 and 1)
- Rule 2: $P(S) = 1$ (one of the outcomes has to happen)
- Rule 3: $P(\text{not }A) = 1 - P(A)$ (if we know the chance of something happening, we also know that chance it doesn't happen)
- Rule 4: $P(A\text{ or }B) = P(A) + P(B)$ if $A$ and $B$ are disjoint events.
  - $A$ and $B$ are **disjoint/mutually exclusive** if $A$ occuring prevents $B$ from occurring (they both can't happen at the same time).
- Rule 4*: $P(A\text{ or }B) = P(A) + P(B) - P(A\cap B)$
- Rule 5: $ P(A\text{ and }B) = P(A)\times P(B)$ if $A$ and $B$ are independent.
  - $A$ and $B$ are **independent** if $B$ occurring doesn't change the probability of $A$ occurring.
- Rule 5*: $P(A\text{ and }B) =P(A~|~B)P(B) = P(B~|~A)P(A)$.
  - The **conditional probability** of A **given** that event B occurs, $P(A~|~B)$, is equal to the probability of the joint event (A and B) divided by the probability of B.
$$P(A ~| ~B) = \frac{P(A \text{ and } B)}{P(B)} $$
  - Intuition: Given that $B$ happened, we focus on the subset of outcomes in $S$ in which $B$ occurs and then figure out what the chance of $A$ happening within that subset. 

For more details on theoretical probability, please see Appendex A. This material is optional for those of you who want to know the more mathematical reasoning behind the rest of the chapter. 

##Diagnotic Testing and Probability

CONDITIONAL PROBABILITY AND BAYES RULE

Link to article: https://www.washingtonpost.com/news/posteverything/wp/2018/10/05/feature/doctors-are-surprisingly-bad-at-reading-lab-results-its-putting-us-all-at-risk/?utm_term=.73d08eefca3c



Real Example (Down syndrome? or other?)


Relate this to prosecutor's fallacy (false positive, false negative) and switching the condition
 --- leading to hypothesis testing ideas
 


##Random Variable
- Probability Model: BASIC DEF, how to use pmf (assuming they are just given), pdf, to calculate probabilities
- Expected Value and Variance as concepts, not as theory
- Put in context of sampling variability (mean and sd of the sampling distribution) and variability from randomization assignment



##Bernoulli/Binomial

- Tie to Logistic regression
- E(x) = p or E(x) = np or E(x/n) = p
- tie to sampling variability and the disease we talk about previously
- show the pmf graphically (show what it does when n get large)

##Normal
- Many Normal curve (each with its own mean and sd)
- Area under curve as it relates to 68-95-99.7 rule
- relate to z-score
- Big IDEA: +- 2SD


##Sampling Distribution and CLT
- Connecting CLT (NORMAL) and bootstrapping
- SE (estimate of RV SD), z-scores ( value - E(X) / SE)




