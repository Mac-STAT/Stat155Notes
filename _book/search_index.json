[
["index.html", "Math 155 Notes Chapter 1 Introduction", " Math 155 Notes Brianna Heggeseth 2018-08-12 Chapter 1 Introduction For Math 155, we will spend our time in-class working together on real problems, asking each other questions, and expanding our understanding of the material. This online text is a collection of notes that includes definitions, examples, and will support our discussion in class. If you find any typos or have questions, please email bheggese@macalester.edu. "],
["introduction-to-data.html", "Chapter 2 Introduction to Data", " Chapter 2 Introduction to Data We live in a data-driven world. From search engines to satellite images, from cell phones to credit cards, current technology can produce data faster than we can analyze them. Our goal in Statistics is to get information from this data. The main way we do this is by visually exploring the data and building models to try to explain observed variability, while staying grounded in the context of the data. "],
["data-context.html", "2.1 Data Context", " 2.1 Data Context For any data set, you should always ask yourself (or others) a few questions to provide vital context about a data set. Who is in the data set? (What is the observational unit or case? How were they selected?) What is being measured or recorded on each case? (What are the characteristics or variables that were collected?) Where were they collected? (One location? Multiple locations?) When were they collected? (One point in time? Over time?) How were they collected? (Measurements? Questionnaire? By phone? In person?) Why were they collected? (Who paid? Conflicts of interest?) "],
["sampling.html", "2.2 Sampling", " 2.2 Sampling We hope that the cases in the data set are representative of a population of interest. In particular, we say that our data that has been collected is a sample from a population. Population of Interest: A collection of people, creatures, things, cases or others that we interested in knowning more about. Sample: A subset of the population of interest on which we have collected data. Getting information from everyone individual in a population is very difficult (it is called a census) and in many situations, impossible. We want to get a sense of the population by just collected data on a subset. Here is a metaphor to think about. Consider the process of making a soup. We want to make sure there is enough (but not too much) salt or seasonings. We don’t need or want to eat the entire pot of soup to test if there is the right amount. In fact, we just need a spoonful from a well-stirred pot of soup so that it is that taste is representative of the whole pot. What would happen if we put the salt in but didn’t stir. If we taste just the top layer of the soup from the pot? It would be overly salty. But just below the surface, there would be too little. In order for our taste (sample) to be useful to us, we need to ensure that it is representative of the whole. Representative Sample:A group that closely matches the characteristics of its population as a whole. In other words, the sample is a fairly accurate reflection of the population from which the sample is drawn. 2.2.1 Sampling Bias If we do not have a representative sample due to the way individuals were selected to be in the study, then we say that there was bias in the sampling process (sampling bias). Think about putting your spoon only on the edge or only in the middle, while not stirring. Sampling Bias: If the sample is unrepresentative of the population of interest in a systematic way, there is sampling bias. Here are a few flavors of sampling bias: Undercoverage: When some members of the population are inadequately represented in the sample due to the sampling procedure (often from convenience samples). An example would be the Literary Digest 1936 poll that got the presidental election wrong. The survey relied on a convenience sample, drawn from telephone directories and car registration lists. In 1936, people who owned cars and telephones tended to be more affluent. Response bias: When the response does not accurately represent the true value for the individual (due to wording of the question or to increase social desirability). Most people like to present themselves in a favorable light, so they will be reluctant to admit to unsavory attitudes or characteristics or illegal activities in a survey, particularly if survey results are not confidential. Nonresponse bias: When individuals chosen or selected for the sample are unwilling or unable to participate. An example would be an unreturned mail survey that is only sent to a random set of individuals. Voluntary response bias: When the individuals in the sample are self-selected volunteers (they were not chosen or selected by a researcher). An example would be call-in radio shows that solicit audience participation in surveys on controversial topics (abortion, affirmative action, gun control, etc.). 2.2.2 Random Sampling To try and avoid some of these types of sampling bias, we may want to “stir the pot before taking a taste” by taking a random sample so as to make sure the sample is representative of the population. It doesn’t eliminate nonresponse bias, but we can take actions to try to maximize the response rate by repeatedly requesting a response (calling, emailing, going in person, etc). Random Sampling: A procedure for sampling from a population in which (a) the selection of a sample unit is based on chance and (b) every element of the population has a known, non-zero probability of being selected. Random sampling helps produce representative samples by eliminating voluntary response bias and guarding against undercoverage bias. There are many ways to do random sampling. A few flavors are: Simple Random Sampling: *A simple random sample involves having a list of all of the units of a population of interest (called a sampling frame) and then randomly selecting units without replacement. In doing so, every unit has an equal chance of being selected and every sample of size \\(n\\) has an equal chance of being selected.** Stratified Sampling: A stratified sampling allows you to control the characteristics of the sample by first taking the sampling frame and separating the units into homogenous groups based on a chosen set of characteristics (ex. gender, major of study). Then you do a simple random sample within each homogenous groups, controlling the number selected from each group. Cluster Sampling: A cluster sampling is often done for the sake of time and financial constraints and involves taking the sampling frame and separate the units into hetereogeneous groups, typically defined by physical locations, and then a simple random sample is completed on the groups, meaning that a heterogeneous group is randomly selected and every member in that group is in the sample. Systematic Sampling: Systematic sampling involves selecting individuals from the list of units in the population, the sampling frame, by first chooing a random starting point and then selecting all kth individuals in the list. The interval must be fixed ahead of time. "],
["observational-study-vs-experiments.html", "2.3 Observational Study vs. Experiments", " 2.3 Observational Study vs. Experiments Data can be collected in one of two scenerios: Observational Study: Data is collected in such a way such that the researcher does not manipulate or intervene in characteristics of the individuals. Researchers simply observed or record characteristics of the sample through direct measurement or through a questionnaire or survey. Experiment: Data is collected in such a way such that the researcher does manipulate or intervene characteristics of the individuals by randomly assigning individuals to treatment and control groups. Researchers then record characteristics of the individuals in the sample within the treatment and control groups. The main reason for doing an experiment is to try to estimate a relationship between a treatment and a response without the impact of confounding variables. For example, imagine we want to know if taking a daily multivitamin reduces systolic blood pressure. If we did an observational study, we’d select a sample (hopefully randomly) from a population of interest and then ask whether an individual takes a daily multivitamin and measure their blood pressure. Would this data be enough evidence to conclude that vitamin use causes a reduction in blood pressure? No, it wouldn’t. Individuals that take daily multivitamins may also be more health-conscious and thus eat more fruits and vegetables and exercise, which could be related to blood pressure. The diet and exercise would be acting as confounding variables, making it impossible to say for certain if vitamin use has a direct impact on blood pressure. Confounding Variables: Third variables that are related to both a “treatment” (e.g. multivitamin) and a “response” (e.g. blood pressure). Example: Say we note that higher ice creams sales is related to a higher number of pool drownings. What could be a confounding variable in this circumstance? In an experiment, we “manipulate” the characteristics of an individual by randomly assigning them to a treatment. This random assignment is intended to “break” the relationship between any third variable and the “treatment” so as to try to reduce the impact of confounding. It is impossible to entirely remove the possibility of confounding, but the random assignment to a treatment helps. (Note: things can get complicated if individuals don’t comply with the treatment such as take the multivitamin every single day…) Causal Inference: Causal inference is the process of making a conclusion about direct cause and effect between a “treatment” and a “response”. It is very difficult to make causal inferences/statements based on data from an observational study due to the possible presence of confounding variables. There is a whole area of statistics dedicated to methods that attempt to overcome the confounding, but that is beyond the scope of this course. (If you want a “gentle” but mathematical introduction to this area of Statistics, I’d suggest reading “Causal Inference in Statistics: A Primer” by Judea Pearl, Madelyn Glymour, Nicholas P. Jewell) "],
["tidy-data.html", "2.4 Tidy Data", " 2.4 Tidy Data Raw data can come in a variety of formats. In order to analyze the data, we need to get data into a tidy format, in which Rows represent cases (one row per observational unit – this could be an individual or an individual at a particular time) Columns represent variables (one column per characteristic) Variables can be either categorical or quantitative variables. Categorical variable: Characteristic with values that are names of categories; the names of categories could be numbers such as with zipcodes. If the categories have a natural ordering, it could be called an ordinal variable, but we won’t be distinguishing between different types of categorical variables. Quantitative variable: Characteristic with measured numerical values with units. Note: Any quantitative variable can be converted into a categorical variable by creating categories defined by intervals or bins of values. "],
["ethical-considerations.html", "2.5 Ethical Considerations", " 2.5 Ethical Considerations Through this class, we are going to stop and think about the ethical considerations of what we are doing. In particular, we are going to consider the ethics of How the data were collected Random assignment to treatments Data storage Data privacy Data use Choice of sample data used for predictive modelling There will be reading to expose you to issues as well as questions in assessments asking you to consider the ethical issues with a real data set and statistical analysis. "],
["visualizing-data.html", "Chapter 3 Visualizing Data", " Chapter 3 Visualizing Data The first step in any data analysis is to visually explore your data. There is a saying that “a picture is worth a 1000 words.” In making visualizations, our goal is to quickly and easily get a better understanding of the variability and relationships that exist in the data. Here we will cover the standard appropriate graphics for univariate variation and bivariate relationships. The choice of the graphic depends on the type(s) of variable(s): quantitative or categorical. So the first step is to think about the variables you are interested in visualizing and determining whether they are quantitative or categorical. For each type of variable, we will use a real data set to illustrate the visualizations. First, we consider survey data of the electoral registrar in Whickham in the UK (Source: Appleton et al 1996). Survey was conducted in 1972-1974 to study heart disease and thyroid disease (baseline characteristics: age and smoking status) A follow-up on those in the survey was conducted twenty years later (followup characteristics: mortality after 20 years) "],
["one-categorical-variable.html", "3.1 One Categorical Variable", " 3.1 One Categorical Variable Let’s first consider the age distribution. Age, depending on how it is measured, could act as a quantitative variable or categorical variable. In this case, age is a quantitative variable because it is recorded to the nearest year. But, for illustrative purposes, I’m going to create a categorical variable by separating age into intervals. What do I lose when I convert a quantitative variable to a categorical variable? What do I gain? 3.1.1 Bar Plot The height of the bars is the only part that encodes the data (width is meaningless). The height can either represent the frequency (count of cases) or the relative frequency (proportion of cases). data(Whickham) #load data set from package Whickham &lt;- Whickham %&gt;% mutate(ageCat = cut(age, 4)) #Create a new categorical variable with 4 categories based on age (equal length of age intervals) Whickham %&gt;% count(ageCat) %&gt;% mutate(relfreq = n / sum(n)) ## # A tibble: 4 x 3 ## ageCat n relfreq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 (17.9,34.5] 408 0.311 ## 2 (34.5,51] 367 0.279 ## 3 (51,67.5] 347 0.264 ## 4 (67.5,84.1] 192 0.146 Whickham %&gt;% ggplot(aes(x = ageCat)) + geom_bar(fill=&quot;steelblue&quot;) + xlab(&#39;Age Categories in Years&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() What do you notice? What do you wonder? 3.1.2 Pie Chart Pie charts are only useful if you have 2 to 3 possible categories and you want to show relative group sizes. This is the best use for a pie chart: I’m purposefully not showing you how to make a pie chart because a bar chart is a better choice. Here is a good summary of why I hate pie charts: http://www.businessinsider.com/pie-charts-are-the-worst-2013-6 (make sure to read if you like pie charts!) "],
["two-categorical-variables.html", "3.2 Two Categorical Variables", " 3.2 Two Categorical Variables Now, let’s consider two other variables in the same data set. What is the relationship between 20-year mortality outcome and smoking status at the beginning of the study. 3.2.1 Side by Side Bar Plot The height of the bars shows the frequency of the categories within subsets. Whickham %&gt;% count(outcome, smoker) %&gt;% mutate(relfreq = n/sum(n)) #Overall Relative Frequency ## # A tibble: 4 x 4 ## outcome smoker n relfreq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alive No 502 0.382 ## 2 Alive Yes 443 0.337 ## 3 Dead No 230 0.175 ## 4 Dead Yes 139 0.106 Whickham %&gt;% ggplot(aes(x = smoker, fill = outcome)) + geom_bar(position = position_dodge()) + xlab(&quot;Smoker Status&quot;) + ylab(&quot;Counts&quot;) + scale_fill_manual(&quot;20 Year Mortality&quot;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? 3.2.2 Stacked Bar Plot The height of the entire bar shows the marginal distribution (frequency of the X variable, ignoring the other variable). The relative heights show conditional distributions (frequencies within subsets), but it is hard to compare distributions between bars because the overall heights differ. Distribution: the way something is spread out (aka the way in which values vary). Whickham %&gt;% ggplot(aes(x = smoker, fill = outcome)) + geom_bar() + xlab(&quot;Smoker Status&quot;) + ylab(&quot;Counts&quot;) + scale_fill_manual(&quot;20 Year Mortality&quot;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? 3.2.3 Mosaic Plot The relative height of the bars shows the conditional distribution (relative frequency within subsets). The width of the bars shows the marginal distribution (relative frequency of the X variable, ignoring the other variable). Whickham %&gt;% ggplot() + geom_mosaic(aes(x = product(outcome, smoker), fill = outcome)) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;,values=c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? Gut Check: Does our data suggest that smoking is associated with a lower mortality rate? Does our data suggest that smoking reduces mortality? (Note the difference in these two questions - the second implies cause and effect) Whickham %&gt;% ggplot() + geom_mosaic(aes(x = product(outcome, smoker), fill = outcome)) + facet_grid( . ~ ageCat) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;,values=c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? Gut Check: How is it that our conclusions are exactly the opposite if we consider the relationship between smoking and mortality within age subsets? This is called Simpson’s Paradox, which is a situation in which you come to two different conclusions if you look at results overall versus within subsets (e.g. age groups). Let’s look at the marginal distribution of smoking status within each age group. For groups of people that were 68 years of age or younger, it was about 50-50 in terms of smoker vs. non smoker. But, the oldest age group were primarily nonsmokers. Now look at the mortality rates within each age category. The 20-year mortality rate among young people (35 or less) was very low but that increases with increased age. So the oldest age group had the highest mortality rate, due primarily to their age, and also had the highest rate of non-smokers. So when we look at everyone together (not subsetting by age), it looks like smoking is associated with a lower mortality rate, when in fact age was just confounding the relationship between smoking status and mortality. "],
["one-quantitative-variable.html", "3.3 One Quantitative Variable", " 3.3 One Quantitative Variable Next, we will use data from one of the largest ongoing health studies in the USA, named NHANES. In particular, we will focus on data from the NHANES between 2009-2012 (Source: CDC). Since sleep is vitally important to daily functioning, let’s look at the number of hours of sleep respondants reported. 3.3.1 Histogram The height of the bars shows either the frequency within intervals or the density (relative frequency per unit of measure, e.g. proportion of people per hour sleep) The x-axis is a number line and bars do not have to be of equal width (R chooses a default bin width, but you can change how many intervals and how large they are) #For more info about NHANES: https://www.cdc.gov/nchs/nhanes/index.htm NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(fill = &quot;steelblue&quot;) + #Gaps are meaningful xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + #Gaps are meaningful xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;steelblue&quot;) + geom_density(alpha = 0.2, fill = &quot;steelblue&quot;, adjust = 3) + xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). ## Warning: Removed 2245 rows containing non-finite values (stat_density). We focus on three aspects of the histogram/distribution: Shape: Is it symmetric? or is itskewed to the right or left (Distributions are skewed to the long tail - which side has a long tail)? How many modes do you see? Center: Where is a typical value located? Spread (or variation): How spread out are the values? (Concentrated around one or more values or spread out?) Also, we ask: Are there any unusual features such as outliers or gaps? Why? Here is another data set for comparison: CEO salaries for the highest paid CEOs in 2016 (Source: NYTimes). nyturl &lt;- &#39;https://www.nytimes.com/interactive/2017/05/26/business/highest-paid-ceos.html?mcubz=0&#39; dat &lt;- read_html(nyturl) ceo &lt;- dat %&gt;% html_nodes(&quot;.nytg-compensation , .nytg-year&quot;) %&gt;% html_text() %&gt;% str_replace(&#39;\\\\$|-&#39;,&#39;&#39;) #webscraping data ceo &lt;- data.frame(matrix(ceo,ncol = 2,byrow = TRUE)) names(ceo) &lt;- c(&#39;year&#39;,&#39;salary&#39;) ceo$salary &lt;- as.numeric(ceo$salary) ceo &lt;- ceo %&gt;% filter(year == &#39;2016&#39;) #Highest Paid CEO&#39;s in the U.S. in 2016 (Source: NYTimes) ceo %&gt;% ggplot(aes(x = salary)) + geom_histogram(aes(y = ..density..), binwidth = 15, fill = &quot;steelblue&quot;) + geom_density(alpha = 0.2, fill = &quot;steelblue&quot;) + xlab(&#39;Salary ($M)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() 3.3.2 Center For center, we have some choices for numerically summarizing it: Mean: The sum of the values divided by the number of values (sample size), \\(\\bar{y} = \\frac{\\sum^n_{i=1}y_i}{n}\\) Sensitive to outliers, but efficiently uses all the data Median: The “middle” value. The number for which half of the values are below and half are above. Insensitive to outliers, but doesn’t use all the actual values Trimmed means: Drop the lowest and highest k% and take the mean of the rest. A good compromise, but not widely used. summary(NHANES$SleepHrsNight) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 2.000 6.000 7.000 6.928 8.000 12.000 2245 mean(NHANES$SleepHrsNight, na.rm=TRUE) #na.rm = TRUE removes missing values ## [1] 6.927531 median(NHANES$SleepHrsNight, na.rm=TRUE) ## [1] 7 mean(NHANES$SleepHrsNight, trim = 0.05, na.rm=TRUE) #Trim 5% from both tails before taking mean ## [1] 6.948431 summary(ceo$salary) #Note the differences between mean and median ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20.00 36.75 58.00 68.36 96.00 176.00 mean(ceo$salary) #Arthimetic average ## [1] 68.36 median(ceo$salary) #Middle number ## [1] 58 mean(ceo$salary,trim = 0.05) #Trimmed mean falls in between ## [1] 66.17778 3.3.3 Boxplot A simplification of the histogram: Box: 25th Percentile (Q1) to 75th Percentile (Q3) Line in Box: 50th Percentile (Median) Tails: Extend to most extreme observed values within 1.5*(Q3-Q1) from Q1 (left) or Q3 (right) Points: If any points are beyond 1.5*(Q3-Q1) from the box, plot separately NHANES %&gt;% ggplot(aes(y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). #Highest Paid CEO&#39;s in the U.S. in 2016 (Source: NYTimes) ceo %&gt;% ggplot(aes(y = salary)) + geom_boxplot() + ylab(&#39;Salary ($M)&#39;) + theme_minimal() Let’s put the boxplots next to the histograms so we can better compare. Also, let’s add the mean (red dashed), median (blue dotted), and 5% trimmed mean (purple dash-dot). What would the boxplot look like if all of the values were exactly the same? 3.3.4 Spread For spread we have several choices to numerically describe it: Range: the maximum value - the minimum value Sensitive to the outliers since it’s the difference of the extremes IQR: the interquartile range : Q3 - Q1 (75th percentile - 25th percentile). Length of the box in a boxplot Spread of middle 50% of data Like the median. Less sensitive because it doesn’t use all of the data Standard deviation: Root mean squared deviations from mean, \\(s = \\sqrt{\\frac{\\sum^n_{i=1}(y_i-\\bar{y})^2}{n-1}}\\) Roughly the average size of deviation from the mean (\\(n-1\\) instead of \\(n\\)) Uses all the data but very sensitive to outliers and skewed data (large values are first squared). diff(range(NHANES$SleepHrsNight, na.rm = TRUE)) #range gives max and min; take difference ## [1] 10 IQR(NHANES$SleepHrsNight, na.rm = TRUE) #Q3-Q1 ## [1] 2 sd(NHANES$SleepHrsNight, na.rm = TRUE) #standard deviation ## [1] 1.346729 diff(range(ceo$salary)) ## [1] 156 IQR(ceo$salary) ## [1] 59.25 sd(ceo$salary) ## [1] 39.05955 x = rnorm(1500) boxplot(x, horizontal = TRUE, xlab = &quot;Generated Data&quot;) abline(v = range(x), col = &quot;blue&quot;, lty = 3) abline(v = quantile(x, c(0.25, 0.75)), col = &quot;purple&quot;, lty = 1) abline(v = c(-sd(x), sd(x)), col = &quot;green&quot;, lty = 2) What percentage of the data is between the blue dotted lines (length of interval is range)? What percentage of the data is between the purple solid lines (length of interval is IQR)? What percentage of the data is between the green dashed lines (length of interval is 2*SD)? sum(x &gt; -sd(x) &amp; x &lt; sd(x))/length(x) ## [1] 0.6853333 3.3.5 Z-scores How do you decide when an outlier is really unusual (think: athletic victory being very impressive or a data point that may be an error)? If the observation is far from the rest of the measurements in the data, we tend to say that the value is more unusual. We want to quantify this idea of “unusual.” To do this, we often calculate a z-score, a standardized data value. Calculate how far the observation was below (or above) the mean of the sample. Then divide the difference by the standard deviation (measure of spread). \\[ z = \\frac{y - \\bar{y}}{s_y}\\] The z-score just tells you how many standard deviations the observation is above or below the mean. Say that you got a z = 1 on an exam with mean = 80 and sd = 5. That means that you got an 85 on the exam (\\(mean + z*sd = 80 + 1*5\\)). If you got a z = -2 on an exam with mean = 80 and sd = 5, that means you got a 70 on the exam (80 + -2*5). In general, it is quite common to have z-scores between -3 and 3, but very unusual to have them greater than 3 or less than -3. In particular, if you have a unimodal, symmetric distribution z-score values will be between -1 and 1 about 68% of the time, between -2 and 2 about 95% of the time, and between -3 and 3 about 99.7% of the time. For those of you who like mathematical theorems, go check out Chebyshev’s inequality. It gives a mathematical reasoning for why z-scores of magnitude 3 or greater are very unusual. Then take probability to get an even better understanding of it! In summary, for a quantitative variable, Use a histogram to display the distribution of one variable and describe the shape and any unusual features. For “well behaved” distributions (symmetric, unimodal, no outliers), use the mean and standard deviation to describe the center and spread. Then z-scores will roughly follow the 68-95-99.7 rule stated above. For others, use the IQR and median. You can report both mean and median, but it’s usually a good idea to state why. "],
["one-quantitive-variable-and-one-categorical-variable.html", "3.4 One Quantitive Variable and One Categorical Variable", " 3.4 One Quantitive Variable and One Categorical Variable Why do some people sleep more than others? Let’s look at the distribution of hours of sleep at night within subsets of the NHANES data. 3.4.1 Multiple Histograms Does the recorded binary gender explain the variability in the hours of sleep? (Quick ethics consideration: what are the implications of collecting binary gender as a variable even if gender is not considered binary?) NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_grid(~Gender) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). Does the number of child a parents has explain the variability in the hours of sleep? (Quick ethics consideration: who have we excluded from our analysis?) NHANES %&gt;% filter(!is.na(nBabies)) %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_grid(~factor(nBabies)) + theme_minimal() ## Warning: Removed 8 rows containing non-finite values (stat_bin). Does the number of days someone has felt depressed explain the variability in the hours of sleep? NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_grid(~Depressed) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). What do you notice? What do you wonder? 3.4.2 Multiple Boxplots Let’s try that again but with boxplots and see if we can glean any more information. NHANES %&gt;% ggplot(aes(x = Gender, y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Binary Gender&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). NHANES %&gt;% ggplot(aes(x = factor(nBabies), y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Number of Babies&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). NHANES %&gt;% ggplot(aes(x = factor(Depressed), y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Days Depressed&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). What do you notice? What do you wonder? 3.4.2.1 Is this a Real Difference? If we notice differences in the center of these distributions, is it a REAL difference? Is there a difference in the general U.S. population? Remember, we just have a random sample of the population NHANES is supposed to be a representative sample of the U.S. population collected using a random sampling procedure. What if there were no REAL difference? Then the Depressed group labels wouldn’t be related to the hours of sleep. Investigation Plan: Take all of the observed data on sleep and randomly shuffle into new groups (of the same size as before). Calculate the difference in mean hours of sleep between the groups. Record it. Repeat 1 and 2 many times (say 1000 times). Look at the differences based on random shuffles &amp; compare to the observed difference. NHANES &lt;- NHANES %&gt;% mutate(DepressedMost = (Depressed == &#39;Most&#39;)) #TRUE or FALSE (converted to a 2 category variable) obsdiff &lt;- data.frame(d = diff(mean(SleepHrsNight ~ DepressedMost, data = NHANES, na.rm = TRUE))) sim &lt;- do(1000)*diff(mean(SleepHrsNight ~ shuffle(DepressedMost), data = NHANES, na.rm = TRUE)) #Randomly shuffle the DepressedMost labels (assuming no real difference in sleep, depressed feelings shouldn&#39;t impact sleep) sim %&gt;% ggplot(aes(x = TRUE.)) + geom_histogram(fill = &#39;steelblue&#39;) + geom_vline(aes(xintercept = d), obsdiff, color = &#39;red&#39;) + xlab(&#39;Difference in Mean Hours of Sleep&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. What do you notice? What do you wonder? "],
["two-quantitative-variables.html", "3.5 Two Quantitative Variables", " 3.5 Two Quantitative Variables Now, let’s switch to another data set that includes a variety of body measurements. Imagine that you are an entreprenure and you are going to sell dress shirts. Clothing sizing is just weird (don’t get me started…) so let’s use this dataset to try to come up with our own data driven sizes. Two of the key measurements are the neck size and waist size. There are others, but let’s focus on these for the moment. 3.5.1 Scatterplot When you have two quantitative variables, a scatterplot is one appropriate graphical display of the relationship. par(mfrow=c(1,1)) bodyfat &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/bodyfat.txt&quot;) bodyfat %&gt;% ggplot(aes(x = Neck, y = Waist)) + geom_point(color = &#39;steelblue&#39;) + theme_minimal() What do you notice about: Direction of Relationship (positive, negative, neutral) Form of Relationship (linear, curved, none, other) Strength of Relationship (compactness around the relationship) Unusual Features (outliers, differences in variability across x) How could you use this information to determine shirt sizes for your new business venutre? We’ll come back to this. Suppose instead of Waist in inches and Neck size in inches, we switched to metric and plotted Waist and Neck size in cm. Does the strength change? bodyfat %&gt;% ggplot(aes(x = Neck*2.54, y = Waist*2.54)) + geom_point(color = &#39;steelblue&#39;) + theme_minimal() 3.5.2 Correlation Coefficient Since shifting (adding or subtracting) and scaling (multiplying or dividing) make no difference, let’s standardize both variables into z-scores and replot (and add some color): If we were to have a weaker positive relationship, how would this plot change? If we were to have a negative relationship, how would this plot change? We want one number to represent strength and direction of a linear relationship. Points in the \\(1^{st}\\) and \\(3^{rd}\\) quadrants (blue) have the same sign. Points in the \\(2^{nd}\\) and \\(4^{th}\\) (red) have opposite signs. What if we took the product of the \\(z\\)-scores? What would the values look like? The almost average of products of the \\(z\\)-scores is the correlation coefficient, \\[ r = \\frac{\\sum z_x z_y}{n-1} \\] Which points contribute the most to this average? cor(bodyfat$Neck,bodyfat$Waist) ## [1] 0.7284873 Other expressions for r (for the mathematically intrigued) \\[ r = \\frac{\\sum z_x z_y}{n-1} \\] \\[ = \\frac{\\sum{\\frac{(x_i-\\bar{x})}{s_x}\\frac{(y_i-\\bar{y})}{s_y}}}{n-1}\\] \\[= \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{(n-1) s_x s_y}\\] \\[= \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{(n-1)\\sqrt{\\sum{\\frac{(x_i-\\bar{x})^2}{n-1}}}}{\\sqrt{\\sum{\\frac{(y_i-\\bar{y})^2}{n-1}}}}}\\] \\[=\\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{\\sqrt{\\sum{(x_i-\\bar{x})^2}}}{\\sqrt{\\sum{(y_i-\\bar{y})^2}}}}\\] \\[=\\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}}\\] "]
]
