[
["index.html", "Math 155 Notes Preface", " Math 155 Notes Brianna Heggeseth &amp; Leslie Myint Updated: 2018-09-27 Preface This book contains notes for MATH 155 at Macalester College. It contains definitions, data examples, and R code explanations that provide a foundation for the activities we will do in class. This is a living document, which will get updated throughout the semester. When you find typos or have clarifying questions, please email bheggese@macalester.edu or lmyint@macalester.edu. "],
["data-collection-and-quality.html", "Chapter 1 Data Collection and Quality", " Chapter 1 Data Collection and Quality We live in a world where data touch nearly every aspect of our lives: health care, online shopping, transportation, entertainment. From search engines to satellite images, from cell phones to credit cards, current technology can produce data faster than we can analyze them. This course is the beginning of your journey into the field of Statistics, a discipline whose main goal is to extract information and meaning from data. We do this is by visually exploring the data and building models to try to explain observed variability. First, we will take some time to think about the collection of data and what factors might make data more or less reliable. "],
["what-is-data.html", "1.1 What is Data?", " 1.1 What is Data? Data is anything that contains information. We typically think of data being stored in spreadsheets, but it can come in many other formats such as images or collections of text (whether 280 character tweets or fictional novels). For example, we can take the pixels of digital images or text from all State of the Union addresses and transform them into a tidy, rectangular format. Tidy data is a table in which Each row of a rectangular table corresponds to an observation or case (e.g. person, classroom, country, image, speech at a particular time) Each column correspond to a characteristic or feature or variable for those cases (e.g. age, average classroom grade, average county income, intensity of red pixels, number of times the word “Together” is used) Variables can be either categorical or quantitative variables. Categorical variable: A characteristic with values that are names of categories; the names of categories could be numbers such as with zipcodes. If the categories have a natural ordering, it could be called an ordinal variable, but we won’t be distinguishing between different types of categorical variables in this class. Quantitative variable: A characteristic with measured numerical values with units. Note: Any quantitative variable can be converted into a categorical variable by creating categories defined by intervals or bins of values. The following graphic from the book R for Data Science, by Garrett Grolemund and Hadley Wickham illustrates the features of tidy data. The components of a tidy dataset. Chapter 12 of R for Data Science The transformation process from raw data to a tidy data format is often called feature extraction and is not a short or easy task. In this introductory course, we will work with data that are already tidy. Cases are often referred to as the units of analysis. As analysts, it is important for us to consider what to use as the unit of analysis when we have information, say, on both individuals and their classrooms. Do we want to understand matters at the individual or the classroom level? Answers to these questions will depend on the context and our research questions. "],
["data-context.html", "1.2 Data Context", " 1.2 Data Context For any data set, you should always ask yourself a few questions to provide vital context about that data set. Who is in the data set? What is the observational unit or case? How did they end up in the data set? Were they selected randomly or were they in a particular location a particular time? What is being measured or recorded on each case? What are the characteristics, features, or variables that were collected? Where were they collected? In one location? Multiple locations? When was the data collected? One point in time? Over time? If data quality degrades over time (e.g. lab specimens), is this a concern? How were they collected? What instruments and methods used for measurement? What questions were asked and how? Online survey? By phone? In person? Why were they collected? For profit? For academic research? Are there conflicts of interest? Who collected this data? An agency, an individual researcher? Thinking about this data context informs us how we analyze the data, what conclusions we can draw, and whether we can generalize our conclusions to a larger population. Many of these data context questions also hint at general considerations for threats to data quality. Threats to data quality generally arise through sampling, information bias, and study design. "],
["sampling.html", "1.3 Sampling", " 1.3 Sampling When we study a phenomenon, we generally care about making a conclusion that applies to some target population of interest (e.g. all likely voters in the U.S., all eligible voters in the U.S., college students in Minnesota, etc.). However, we cannot feasibly collect data on that entire population (this is called a census) due to financial and time constraints, so we collect a sample of individuals. We want our sample to be representative of the target population in that we want our sample to resemble the target population in the characteristics we are studying. How is representativeness affected by our research question? Can a sample be representative for one goal but not another? When our method of selecting a sample is flawed, sampling bias results, and our sample is unrepresentative of the target population. How does this tend to happen, and how can we avoid it? It is first helpful to define the term sampling frame. A sampling frame is the complete list of individuals/units in the target population. For example, it could be a spreadsheet listing every college student that studies in Minnesota. 1.3.1 Sampling Bias The following are common ways that sampling bias can arise, and they all share the feature that a sampling frame is not used: Convenience Sampling: Individuals that make up a convenience sample are easy to contact or to reach (e.g. stand on a street corner and ask passerbys to answer a few questions). The people sampled will likely be systematically different than the target population. Self-Selection and Volunteer Sampling: Individuals that make up a sample self-select or volunteer to be in a sample (e.g. product reviews on Amazon, individuals that call in to radio shows, blood donors, etc.). They are likely to be systematically different than the target population. One result of using these sampling techniques is that we can get undercoverage in the sample. This happens when some groups of the population are inadequately represented in the sample due to the sampling procedure. A famous example in United States history is the 1936 Literary Digest poll that completely mispredicted the presidential election. The magazine predicted a strong victory for Alfred Landon, but Franklin Delano Roosevelt ended up winning the election by a substantial margin. The survey relied on a convenience sample, drawn from telephone directories and car registration lists. In 1936, people who owned cars and telephones tended to be more affluent and leaned to the right politically. Without a complete sampling frame, we have no control over what units enter the sample because we do not even have a complete list of the units that could be sampled. Imagine that our target population is like a pot of soup, these forms of sampling are similar to scooping only the bits of soup that float to the top (without stirring). 1.3.2 Random Sampling With a sampling frame, we can do better and hopefully avoid sampling bias by using randomization. In our soup analogy, this amounts to mixing the soup thoroughly and dipping our spoon in random locations. These strategies are called probability sampling strategies or, more colloquially, random sampling strategies. In probability sampling, each unit in the sampling frame has a known, nonzero probability of being selected, and the sampling is performed with some chance device (e.g. coin flipping, random number generation). Some probability sampling techniques include: Simple Random Sampling: Each unit in the sampling frame has the same chance of being chosen and individuals are selected without replacement. In doing so, every sample of a given size are equally likely to arise. Stratified Sampling: The units in the sampling frame are first divided into categories/strata (e.g. age categories). Simple random sampling is performed in each category/stratum. Why do this? Just by chance, simple random sampling might oversample young individuals. Stratifying by age first, then performing simple random sampling in these strata ensures a desired age distribution in the sample. Cluster Sampling: Sometimes a sampling frame is more readily available for clusters of units rather than the units themselves. For example, a sampling frame of all hospitals in Minnesota might be more readily available than a sampling frame of all Minnesota hospital patients in a given time frame. In cluster sampling, the initial clusters are sampled with a probability sampling method (like simple random sampling or stratified sampling). All units in the sampled clusters may be chosen, or if sampling frames can be obtained for the sampled clusters, probability sampling is performed within the cluster. This might happen if it is more feasible to obtain patient lists for a small subset, but not all, hospitals. Systematic Sampling: Systematic sampling involves selecting individuals from the list of units in the population, the sampling frame, by first choosing a random starting point and then selecting all kth individuals in the list. The interval must be fixed ahead of time. Before you choose your starting point, everyone has the same chance of being selected. 1.3.3 Nonresponse bias Even with a great random sampling method, our sample can still be unrepresentative if units in our sample do not fully participate after they are selected. For example, if the communication method is via e-mail, individuals who do not read our e-mail are nonresponders. If those individuals who don’t participate are systematically different than those that do, this type of nonresponse bias is called unit nonresponse bias. Let’s say that an individual opens up our e-mail survey. They may answer the first few questions but grow weary and skip the last questions. If those individuals who don’t answer particular questions are systematically different than those that do in their responses, this type of nonresponse bias is called item nonresponse bias. "],
["information-bias.html", "1.4 Information bias", " 1.4 Information bias Lastly, separate from sampling, bias can arise in how we record or measure observations. Response bias/Self-report bias/Social desirability bias: When the recorded response does not accurately represent the true value for the individual due to wording of the question or to increase social desirability. Most people like to present themselves in a favorable light, so they will be reluctant to admit to unsavory attitudes or characteristics (e.g. weight, income) or illegal activities in a survey, particularly if survey results are not confidential. Recall bias: People often unintentionally make mistakes in remembering details about the past. Measurement error: Technologies that measure variables of interest may not always be accurate and human calibration of those instruments may be off as well. "],
["study-design-observational-study-vs-experiments.html", "1.5 Study Design: Observational Study vs. Experiments", " 1.5 Study Design: Observational Study vs. Experiments Once we have a sample selected, data can be collected in one of two general study designs: Observational Study: Data is collected in such a way such that the researcher does not manipulate or intervene in the characteristics of the individuals. Researchers simply observe or record characteristics of the sample through direct measurement or through a questionnaire or survey. Experiment: Data is collected in such a way such that the researcher does manipulate or intervene in characteristics of the individuals by randomly assigning individuals to treatment and control groups. Researchers then record characteristics of the individuals in the sample within the treatment and control groups. The main reason for doing an experiment is to estimate a relationship between a treatment and a response. For example, imagine that we want to know if taking a daily multivitamin reduces systolic blood pressure. If we performed an observational study, we would select a sample (hopefully a representative one) from a population of interest and then ask whether an individual takes a daily multivitamin and measure their blood pressure. Would this data provide enough evidence to conclude that vitamin use causes a reduction in blood pressure? Individuals that take daily multivitamins may also be more health-conscious, eating more fruits and vegetables and exercising more, which may be related to blood pressure. The diet and exercise would be acting as confounding variables, making it impossible to say for certain if vitamin use has a direct impact on blood pressure. Confounding Variables: Third variables that are related to both a “treatment” (e.g. multivitamin) and a “response” (e.g. blood pressure). For example, say we note that on days with higher ice creams sales, there are typically a higher number of pool drownings. What could be a confounding variable in this circumstance? In an experiment, we “manipulate” the characteristics for an individual by randomly assigning them to a treatment group. This random assignment is intended to break the relationship between any third variable and the treatment so as to try to reduce the impact of confounding. It is impossible to entirely remove the possibility of confounding, but the random assignment to a treatment helps. (Note: things can get complicated if individuals don’t comply with the treatment such as take the multivitamin every single day…) Causal Inference: Causal inference is the process of making a conclusion about direct cause and effect between a “treatment” and a “response”. It is very difficult to make causal inferences/statements based on data from an observational study due to the possible presence of confounding variables. There is a whole area of statistics dedicated to methods that attempt to overcome the confounding, but that is beyond the scope of this course. (If you want a “gentle” but mathematical introduction to this area of Statistics, I’d suggest reading “Causal Inference in Statistics: A Primer” by Judea Pearl, Madelyn Glymour, Nicholas P. Jewell) "],
["ethical-considerations.html", "1.6 Ethical Considerations", " 1.6 Ethical Considerations Note that ethics play a very important role in study design, especially when humans and animals are the observational units. In the U.S., the Belmont Report (https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html) is the main federal document that provides the “Ethical Principles and Guidelines for the Protection of Human Subjects of Research”. The three fundamental ethical principles for using any human subjects for research are: Respect for persons: protecting the autonomy of all people and treating them with courtesy and respect and allowing for informed consent. Researchers must be truthful and conduct no deception; Beneficence: The philosophy of “Do no harm” while maximizing benefits for the research project and minimizing risks to the research subjects; and Justice: ensuring reasonable, non-exploitative, and well-considered procedures are administered fairly — the fair distribution of costs and benefits to potential research participants — and equally. For a brief, limited history of ethical regulation in human research, see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593469/. A few key moments in history are Nuremberg Code (1948) in response to medical experiments in Germany performed without consent Declaration of Helskinki (originated in 1964 and frequently revised) established by World Medical Association The Belmont Report written in response to the Tuskegee Syphilis Study (1932 - 1972) Common Rule (1981) is regulatory policy which all U.S. government-funded research and nearly all U.S. academic institutions must abide. Macalester College has an Institutional Review Board (IRB) that oversees research that includes human participants (see: https://www.macalester.edu/irb/). Throughout this class, we are going to stop and think about the ethical considerations of the many parts of statistical practice, ranging from data collection to model prediction. Ethics are the norms or standards for conduct that distinguish between right and wrong. In particular, we are going to consider the ethics of How the data are collected Random assignment to treatments Data storage Data privacy Data use Choice of sample data used for predictive modeling Use of predictive modeling We are going to pay extra attention to negative consequences of the above that may disproportionately impact marginalized groups of people. Throughout the semester, you will be asked to think about answers to the question: “What are the ethical considerations for this data set/analysis?” Like in other disciplines, the choices we make will be biased by our life experiences. Throughout this class, let us be mindful in increasing our awareness of the real consequences caused by choices we make in Statistics. "],
["major-takeaways.html", "1.7 Major Takeaways", " 1.7 Major Takeaways Any observed data is a sample of a larger population or phenomenon. But you need to consider which population. Is it the population of interest? If not, then why? Sampling strategies impact what type of generalizations we can make about a population. Bias occurs when there are systematic differences between observed sample data and the true population of interest due to the sampling process. Study design impacts what type of conclusions we can make. Confounding variables prevent us from making cause and effect conclusions. We need to be aware of the real consequences of our choices when working with data and building models. "],
["visualizing-data.html", "Chapter 2 Visualizing Data", " Chapter 2 Visualizing Data The first step in any data analysis is to visually explore your data. There is a saying that “a picture is worth a 1000 words.” In making visualizations, our goal is to quickly and easily get a better understanding of the variability and relationships that exist in the data. Here we will cover the standard appropriate graphics for univariate variation and bivariate relationships. We will also cover techniques for multivariate relationships (3 or more variables). The choice of the graphic depends on the type(s) of variable(s): quantitative or categorical. So the first step is to think about the variables you are interested in visualizing and determining whether they are quantitative or categorical. For each type of variable, we will use a real data set to illustrate the visualizations. "],
["good-visualization-principles.html", "2.1 Good Visualization Principles", " 2.1 Good Visualization Principles Before we discuss the standard graphics, let’s lay out the basic design principles for good data visualizations. Show the Data This may be self-explanatory, but make sure that the data is the focus and driver of the visualization. Avoid Distorting the Data Avoid 3D charts as the added dimension distorts the comparison. The areas in a graph should equal the magnitude of the data it is representing. Simplify In 1983, Edward Tufte said that “A large share of ink on a graphic should present data-information, the ink changing as the data change. Data-ink is the non-erasable core of a graphic, the non-redundant ink arranged in response to variation in the numbers represented.” Remove any unnecessary ink that does not assist the presentation of the data. Remove distractions. Facilitate Comparisons In order to explain variation, we want the graphics to facilitate comparisons between groups. The design should make it easier to compare between groups rather than harder. Use Contrast Humans have developed to seek out visual contrast. When choosing colors and annotation, strive for more contrast in luminance (white to dark) to make it easier for others to visually perceive. Use Color Appropriately Think about your audience. A small proportion of the population is color-blind; try printing it in grayscale to see if it is still effective. Also, every culture has different associations with colors; ask others for feedback on color choices. Neuroscience research has shown that humans are more sensitive to red and yellow, so those are good colors to use for highlighting key points. Annotate Appropriately Informative text is crucial for providing data context. Make sure to use informative axis labels and titles. It may be worth adding text to explain extreme outliers. For examples of good data visualizations in the news, check out the New York Times column “What’s Going on in This Graph?”. "],
["anatomy-of-a-ggplot-command.html", "2.2 Anatomy of a ggplot command", " 2.2 Anatomy of a ggplot command In this course, we’ll largely construct visualizations using the ggplot function from the ggplot2 R package. NOTE: gg is short for “grammar of graphics”. Plots constructed from the ggplot function are constructed in layers, and the syntax used to create plots is meant to reflect this layered construction. As you read through the rest of this chapter, pay attention to how the syntax generally follows this structure: data %&gt;% ggplot(aes(x = X_AXIS_VARIABLE, y = Y_AXIS_VARIABLE)) + VISUAL_LAYER1 + VISUAL_LAYER2 + VISUAL_LAYER3 + ... The visual layers are features such as points, lines, and panels. The +’s allow us to add layers to build up a plot. "],
["one-categorical-variable.html", "2.3 One Categorical Variable", " 2.3 One Categorical Variable First, we consider survey data of the electoral registrar in Whickham in the UK (Source: Appleton et al 1996). A survey was conducted in 1972-1974 to study heart disease and thyroid disease and a few baseline characteristics were collected: age and smoking status. 20 years later, a follow-up was done to check on mortality status (alive/dead). Let’s first consider the age distribution of this sample. Age, depending on how it is measured, could act as a quantitative variable or categorical variable. In this case, age is recorded as a quantitative variable because it is recorded to the nearest year. But, for illustrative purposes, let’s create a categorical variable by separating age into intervals. Distribution: the way something is spread out (aka the way in which values vary). require(dplyr) data(Whickham) #load data set from Whickham package Whickham &lt;- Whickham %&gt;% mutate(ageCat = cut(age, 4)) #Create a new categorical variable with 4 categories based on age (equal length of age intervals) head(Whickham) ## outcome smoker age ageCat ## 1 Alive Yes 23 (17.9,34.5] ## 2 Alive Yes 18 (17.9,34.5] ## 3 Dead Yes 71 (67.5,84.1] ## 4 Alive No 67 (51,67.5] ## 5 Alive No 64 (51,67.5] ## 6 Alive Yes 38 (34.5,51] What do I lose when I convert a quantitative variable to a categorical variable? What do I gain? 2.3.1 Bar Plot One of the best ways to show the distribution of one categorical variable is with a bar plot. For a bar plot, The height of the bars is the only part that encodes the data (width is meaningless). The height can either represent the frequency (count of cases) or the relative frequency (proportion of cases). ## Numerical summary (frequency and relative frequency) Whickham %&gt;% count(ageCat) %&gt;% mutate(relfreq = n / sum(n)) ## # A tibble: 4 x 3 ## ageCat n relfreq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 (17.9,34.5] 408 0.311 ## 2 (34.5,51] 367 0.279 ## 3 (51,67.5] 347 0.264 ## 4 (67.5,84.1] 192 0.146 ## Graphical summary (bar plot) Whickham %&gt;% ggplot(aes(x = ageCat)) + geom_bar(fill=&quot;steelblue&quot;) + xlab(&#39;Age Categories in Years&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() What do you notice? What do you wonder? 2.3.2 Pie Chart Pie charts are only useful if you have 2 to 3 possible categories and you want to show relative group sizes. This is the best use for a pie chart: We are intentionally not showing you how to make a pie chart because a bar chart is a better choice. Here is a good summary of why many people strongly dislike pie charts: http://www.businessinsider.com/pie-charts-are-the-worst-2013-6. Keep in mind Visualization Principle #4: Facilitate Comparisons. We are much better at comparing heights of bars than areas for slices of a pie chart. "],
["two-categorical-variables.html", "2.4 Two Categorical Variables", " 2.4 Two Categorical Variables Now, let’s consider two other variables in the same Whickham data set. What is the relationship between the 20-year mortality outcome and smoking status at the beginning of the study? 2.4.1 Side by Side Bar Plot There are a few options for visualizing the relationship between two categorical variables. One option is to use a bar plot and add bars for different categories next to each other, called a side-by-side bar plot. For these plots, The height of the bars shows the frequency of the categories within subsets. ## Numerical summary (frequency and overall relative frequency) Whickham %&gt;% count(outcome, smoker) %&gt;% mutate(relfreq = n / sum(n)) ## # A tibble: 4 x 4 ## outcome smoker n relfreq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alive No 502 0.382 ## 2 Alive Yes 443 0.337 ## 3 Dead No 230 0.175 ## 4 Dead Yes 139 0.106 ## Graphical summary (side-by-side bar plot) Whickham %&gt;% ggplot(aes(x = smoker, fill = outcome)) + geom_bar(position = position_dodge()) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? 2.4.2 Stacked Bar Plot Another way to show the same data is by stacking the bars on top of each other with a category. For a stacked bar plot, The height of the entire bar shows the marginal distribution (frequency of the X variable, ignoring the other variable). The relative heights show conditional distributions (frequencies within subsets), but it is hard to compare distributions between bars because the overall heights differ. The widths of the bars have no meaning. ## Numerical summary (conditional distribution - conditioning on outcome) Whickham %&gt;% count(outcome, smoker) %&gt;% group_by(outcome) %&gt;% mutate(relfreq = n / sum(n)) ## # A tibble: 4 x 4 ## # Groups: outcome [2] ## outcome smoker n relfreq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alive No 502 0.531 ## 2 Alive Yes 443 0.469 ## 3 Dead No 230 0.623 ## 4 Dead Yes 139 0.377 ## Numerical summary (conditional distribution - conditioning on smoker) Whickham %&gt;% count(outcome, smoker) %&gt;% group_by(smoker) %&gt;% mutate(relfreq = n / sum(n)) ## # A tibble: 4 x 4 ## # Groups: smoker [2] ## outcome smoker n relfreq ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alive No 502 0.686 ## 2 Alive Yes 443 0.761 ## 3 Dead No 230 0.314 ## 4 Dead Yes 139 0.239 ## Graphical summary (stacked bar plot) Whickham %&gt;% ggplot(aes(x = smoker, fill = outcome)) + geom_bar() + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;,values=c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? 2.4.3 Stacked Bar Plot (Relative Frequencies) We can adjust the stacked bar plot to make the heights the same, so that you can compare conditional distributions. For a stacked bar plot based on proportions (also called a proportional bar plot), The relative heights show conditional distributions (relative frequencies within subsets). The widths of the bars have no meaning. The code below computes the conditional distributions manually first (fractions of outcomes within the two smoking groups). Then these calculations are plotted directly. require(ggplot2) Whickham %&gt;% count(outcome, smoker) %&gt;% group_by(smoker) %&gt;% mutate(relfreq = n / sum(n)) %&gt;% ggplot(aes(x = smoker, y = relfreq, fill = outcome)) + geom_bar(stat = &#39;identity&#39;) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Proportion&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() Another way to make a proportional bar plot is to use the position = &quot;fill&quot; require(ggplot2) Whickham %&gt;% ggplot(aes(x = smoker, fill = outcome)) + geom_bar(position = &quot;fill&quot;) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Proportion&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() 2.4.4 Mosaic Plot The best (Prof. H’s opinion) graphic for two categorical variables is a variation on the stacked bar plot called a mosaic plot. The total heights of the bars are the same so we can compare the conditional distributions. For a mosaic plot, The relative height of the bars shows the conditional distribution (relative frequency within subsets). The width of the bars shows the marginal distribution (relative frequency of the X variable, ignoring the other variable). Making mosaic plots in R requires another package: ggmosaic require(ggmosaic) Whickham %&gt;% ggplot() + geom_mosaic(aes(x = product(outcome, smoker), fill = outcome)) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? With this type of plot, you can see that there are more non-smokers than smokers. Also, you see that there is a higher mortality rate for non-smokers. Does our data suggest that smoking is associated with a lower mortality rate? Does our data suggest that smoking reduces mortality? Note the difference in these two questions - the second implies a cause and effect relationship. Let’s consider a third variable here, age distribution. We can create the same plot, separately for each age group. Whickham %&gt;% ggplot() + geom_mosaic(aes(x = product(outcome, smoker), fill = outcome)) + facet_grid( . ~ ageCat) + xlab(&#39;Smoker Status&#39;) + ylab(&#39;Counts&#39;) + scale_fill_manual(&#39;20 Year Mortality&#39;, values = c(&quot;steelblue&quot;, &quot;lightblue&quot;)) + theme_minimal() What do you notice? What do you wonder? How is it that our conclusions are exactly the opposite if we consider the relationship between smoking and mortality within age subsets? What might be going on? This is called Simpson’s Paradox, which is a situation in which you come to two different conclusions if you look at results overall versus within subsets (e.g. age groups). Let’s look at the marginal distribution of smoking status within each age group. For groups of people that were 68 years of age or younger, it was about 50-50 in terms of smoker vs. non smoker. But, the oldest age group were primarily nonsmokers. Now look at the mortality rates within each age category. The 20-year mortality rate among young people (35 or less) was very low, but mortality increases with increased age. So the oldest age group had the highest mortality rate, due primarily to their age, and also had the highest rate of non-smokers. So when we look at everyone together (not subsetting by age), it looks like smoking is associated with a lower mortality rate, when in fact age was just confounding the relationship between smoking status and mortality. "],
["one-quantitative-variable.html", "2.5 One Quantitative Variable", " 2.5 One Quantitative Variable Next, we use data from one of the largest ongoing health studies in the USA, named NHANES. In particular, we will focus on data from the NHANES between 2009-2012 (Source: CDC). For more info about NHANES: https://www.cdc.gov/nchs/nhanes/index.htm. Since sleep is vitally important to daily functioning, let’s look at the number of hours of sleep respondents reported. 2.5.1 Histogram One main graphical summary we use for quantitative variables is a histogram. It resembles a bar plot, but there are a few key differences: The x-axis is a number line that is divided into intervals called bins. Bins technically do not all have to be of equal width but almost always are. When making histograms in R, R chooses a default bin width, but you have options to change the number and/or width of the bins/intervals. The height of the bars shows either the frequency within intervals (counts of cases that fall into that bin/interval) or the density (fraction of cases that fall into that bin/interval). Gaps between bars are meaningful. They indicate absence of values within an interval. data(NHANES) NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2245 rows containing non-finite values (stat_bin). Note the warning message above: “Removed __ rows containing non-finite values (stat_bin).” Sometimes there is missing information for a variable for some cases in the dataset. We cannot plot these because we don’t know their values! This warning message is just a friendly reminder from R to let you know what it is doing. Also note the message that R gives about bin width to remind us that we can choose this if we wish. If we want to specify the width of the intervals or bins, we can specify binwidth = DESIRED_BIN_WIDTH within geom_histogram. NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). Lastly, notice that the y-axis in the previous two histograms has been the counts (or frequency) within each sleep hour interval. We can adjust this to density, which is relative frequency adjusted for the width of interval so that the sum of the areas of the bars (height x width) equals 1. NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;steelblue&quot;) + geom_density(alpha = 0.2, fill = &quot;steelblue&quot;, adjust = 3) + xlab(&#39;Hours of Sleep (hours)&#39;) + ylab(&#39;Density&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). ## Warning: Removed 2245 rows containing non-finite values (stat_density). The smooth curved line on this plot is called a density plot. It is essentially a smoother version of the histogram. Both the area under a density plot and the total area of all the rectangles in a density histogram equal 1. When describing a distribution, we focus on three aspects of the histogram: Shape: Is it symmetric (can you fold it in half and the sides match up)? or is it skewed to the right or left? (A distribution is left-skewed if there is a long left tail and right-skewed if it has a long right tail.) How many modes (“peaks”/“bumps” in the distribution) do you see? Center: Where is a typical value located? Spread (or variation): How spread out are the values? Concentrated around one or more values or spread out? Unusual features: Are there outliers (points far from the rest)? Are there gaps? Why? Here is another data set for comparison. Here are the annual salaries for the highest paid CEOs in 2016 (Source: NYTimes). To get the data, we are scraping the data from a NYTimes website. For fun, you can look at the code below. nyturl &lt;- &#39;https://www.nytimes.com/interactive/2017/05/26/business/highest-paid-ceos.html?mcubz=0&#39; dat &lt;- read_html(nyturl) ceo &lt;- dat %&gt;% html_nodes(&quot;.nytg-compensation , .nytg-year&quot;) %&gt;% html_text() %&gt;% str_replace(&#39;\\\\$|-&#39;,&#39;&#39;) #webscraping data ceo &lt;- data.frame(matrix(ceo,ncol = 2,byrow = TRUE)) names(ceo) &lt;- c(&#39;year&#39;,&#39;salary&#39;) ceo$salary &lt;- as.numeric(ceo$salary) ceo &lt;- ceo %&gt;% filter(year == &#39;2016&#39;) Let’s create a density histogram of the annual salaries for the highest paid CEO’s in the U.S. in 2016. ceo %&gt;% ggplot(aes(x = salary)) + geom_histogram(aes(y = ..density..), binwidth = 15, fill = &quot;steelblue&quot;) + geom_density(alpha = 0.2, fill = &quot;steelblue&quot;) + xlab(&#39;Salary ($ Millions)&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() We note that some of the highest salaries were close to 200 million U.S. dollars (in 2016), but the majority of the salaries in this sample are closer to 50 million U.D. dollars. Is this distribution of salaries left-skewed or right-skewed? In what populations do you think salaries might be left-skewed? Right-skewed? 2.5.2 Center There are some choices for numerically summarizing the center of a distribution: Mean: The sum of the values divided by the number of values (sample size), \\(\\bar{y} = \\frac{\\sum^n_{i=1}y_i}{n}\\) Sensitive to outliers, but it efficiently uses all the data Median: The “middle” value. The number for which half of the values are below and half are above. Insensitive to outliers, but it doesn’t use all the actual values Trimmed means: Drop the lowest and highest k% and take the mean of the rest. A good compromise, but not widely used. The Greek capital letter sigma, \\(\\sum\\), is used in mathematics to denote a sum. We let \\(y_i\\) represent the value of the \\(i\\)th person for a variable called \\(y\\). So \\(\\sum^n_{i=1}y_i\\) is the sum of all the \\(n\\) values of a variable \\(y\\), all the way from the 1st person to the \\(n\\)th person. We can calculate all of these in R. Hours of sleep per night from the NHANES dataset summary(NHANES$SleepHrsNight) # The $ extracts a certain column of the data set ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 2.000 6.000 7.000 6.928 8.000 12.000 2245 mean(NHANES$SleepHrsNight, na.rm = TRUE) # na.rm = TRUE removes missing values ## [1] 6.927531 median(NHANES$SleepHrsNight, na.rm = TRUE) ## [1] 7 mean(NHANES$SleepHrsNight, trim = 0.05, na.rm = TRUE) # Trimmed mean: trim 5% from both tails before taking mean ## [1] 6.948431 CEO salary information from NYT summary(ceo$salary) # Note the differences between mean and median ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20.00 36.75 58.00 68.36 96.00 176.00 mean(ceo$salary) # Arthimetic average ## [1] 68.36 median(ceo$salary) # Middle number ## [1] 58 mean(ceo$salary,trim = 0.05) # Trimmed mean falls in between ## [1] 66.17778 Note that the mean, median, and trimmed mean are all fairly close for the sleep hours distribution, which looks fairly symmetric. Note also that the mean, median, and trimmed mean are somewhat different for the salary distribution, which looks right skewed. Often with right skewed distributions, the mean tends to be higher than the median because particularly large values are being summed in the calculation. The median and trimmed mean are not as sensitive to these outliers because of the sorting that is involved in their calculation. 2.5.3 Boxplot An alternative graphical summary is a boxplot, which is a simplification of the histogram. The plot consists of A Box: the bottom of the box is at the 25th percentile (\\(Q1\\)) and top of the box is at the 75th percentile (\\(Q3\\)) Line in Box: the line in the middle of the box is at the 50th percentile, the median Tails/Whiskers: The lines extend out from the box to most extreme observed values within \\(1.5 \\times (Q3-Q1)\\) from \\(Q1\\) (bottom) or \\(Q3\\) (top) Points: If any points are beyond \\(1.5 \\times (Q3-Q1)\\) from the box edges, they are considered outliers and are plotted separately A percentile is a measure indicating the value below which a given percentage of observations in a group of observations fall. So the 25th percentile is the value at which 25% of the values are below. The 95th percentile is the point at which 95% of the observations are below. Here is a boxplot of the sleep amount from NHANES. NHANES %&gt;% ggplot(aes(y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). Compare that to the boxplot of the CEO salaries. ceo %&gt;% ggplot(aes(y = salary)) + geom_boxplot() + ylab(&#39;Salary ($M)&#39;) + theme_minimal() Note: In these 2 plots above, the x-axis has number labels, but they don’t mean anything. Let’s put the boxplots next to the histograms so we can better compare the two types of visualizations. Also, let’s add the mean (red dashed), median (blue dotted), and 5% trimmed mean (purple dash-dot) as annotations. For the hours of sleep, the mean, median, and 5% trimmed mean are all pretty much the same. Note also that the distribution looks pretty symmetric based on the histogram. For CEO salaries, the mean and 5% trimmed mean are a bit higher than the median. Note also the mean is always pulled toward the long tail. What would the boxplot look like if all of the values were exactly the same? Sometimes when making multiple boxplots for each of multiple groups, a group may only have one value or a small number of values that all happen to be identical. What will this look like? 2.5.4 Spread There are some choices for numerically summarizing the spread of a distribution: Range: the maximum value - the minimum value Sensitive to the outliers since it’s the difference of the extremes Units (e.g. inches, pounds) are the same as the actual data IQR: the interquartile range : \\(Q3 - Q1\\) (75th percentile - 25th percentile). Length of the box in a boxplot Spread of middle 50% of data Like the median. Less sensitive because it doesn’t use all of the data Units are the same as the actual data Standard deviation (SD): Root mean squared deviations from mean, \\(s_y = \\sqrt{\\frac{\\sum^n_{i=1}(y_i-\\bar{y})^2}{n-1}}\\) Roughly the average size of deviation from the mean (\\(n-1\\) instead of \\(n\\)) Uses all the data but very sensitive to outliers and skewed data (large values are first squared). Units are the same as the actual data Variance: Square of the standard deviation Units are the squared version of the actual data’s units (e.g. squared inches, pounds) Standard deviation is preferred for interpretability of units Variance will come up when we discuss models in the next chapter We can calculate all of these in R. Hours of sleep per night from the NHANES dataset diff(range(NHANES$SleepHrsNight, na.rm = TRUE)) # range gives max and min; take difference ## [1] 10 IQR(NHANES$SleepHrsNight, na.rm = TRUE) # Q3-Q1 ## [1] 2 sd(NHANES$SleepHrsNight, na.rm = TRUE) # standard deviation ## [1] 1.346729 var(NHANES$SleepHrsNight, na.rm = TRUE) # variance ## [1] 1.81368 CEO salary information from NYT diff(range(ceo$salary)) ## [1] 156 IQR(ceo$salary) ## [1] 59.25 sd(ceo$salary) ## [1] 39.05955 var(ceo$salary) ## [1] 1525.649 2.5.5 Some data accounting We’ve looked at different measures of the spread of a distribution. Do some measures of spread encompass a lot of the data? Just a little? Can we be more precise about how much of the data is encompassed by intervals created from different spread measures? x = rnorm(1500) boxplot(x, horizontal = TRUE, xlab = &quot;Generated Data&quot;) abline(v = range(x), col = &quot;blue&quot;, lty = 3) abline(v = quantile(x, c(0.25, 0.75)), col = &quot;purple&quot;, lty = 1) abline(v = c(mean(x) - sd(x), mean(x) + sd(x)), col = &quot;green&quot;, lty = 2) What percentage of the data is between the blue dotted lines (length of interval is range)? What percentage of the data is between the purple solid lines (length of interval is IQR)? What percentage of the data is between the green dashed lines (length of interval is 2*SD)? The code below computes the fraction of data points that fall between the lower bound of 1 SD below the mean and the upper bound of 1 SD above the mean. sum(x &gt; mean(x) - sd(x) &amp; x &lt; mean(x) + sd(x))/length(x) ## [1] 0.6886667 So with this generated data, about 68% of the data values fall within 1 SD of the mean. 2.5.6 Z-scores How do you decide when an outlier is really unusual (think: athletic victory being very impressive or a data point that may be an error)? If the observation is far from the rest of the measurements in the data, we tend to say that the value is unusual. We want to quantify this idea of “unusual”. To do this, we often calculate a z-score, a standardized data value which we will denote with \\(z\\). Calculate how far the observation was below (or above) the mean of the sample. Then divide the difference by the standard deviation (measure of spread). \\[ z = \\frac{y - \\bar{y}}{s_y} \\] The z-score just tells you how many standard deviations the observation is above or below the mean. Say that you got a z-score of 1 on an exam with mean = 80 and SD = 5. That means that you got an 85 on the exam because your exam is one SD above the mean (\\(mean + z \\times SD = 80 + 1 \\times 5\\)). If you got a z = -2 on an exam with mean = 80 and SD = 5, that means you got a 70 on the exam because your exam is two SD below the mean (\\(mean + z \\times SD = 80 + -2 \\times 5\\)). In general, it is quite common to have z-scores between -3 and 3, but fairly unusual to have them greater than 3 or less than -3. Often, if you have a unimodal, symmetric distribution, z-score values will be between -1 and 1 about 68% of the time, between -2 and 2 about 95% of the time, and between -3 and 3 about 99.7% of the time. This will not be true always, but it will be true for a particularly special distribution that we will see later when we cover models. (This distribution is called the normal distribution or Gaussian distribution.) (Optional) Chebyshev’s inequality gives bounds for the percentages no matter the shape of the distribution. It states that for any real number \\(k\\) &gt; 0, the chance of getting a z-score greater in magnitude (ignoring the negative sign) than \\(k\\) is less than or equal to \\(1/k^2\\), \\[P\\left(|Z| \\geq k\\right) \\leq \\frac{1}{k^2}\\] where \\(Z = \\frac{|X - \\mu|}{\\sigma}\\) is a z-score, \\(\\mu\\) is the mean, and \\(\\sigma\\) is the standard deviation. If we plug in values for \\(k\\), we see that the chance of getting a z-score at least 3 in magnitude (&gt; 3 or &lt; -3) is less than \\((1/3^2) = 0.11 = 11\\%\\). at least 4 in magnitude (&gt; 4 or &lt; -4) is less than \\((1/4^2) = 0.06 = 6\\%\\). at least 5 in magnitude (&gt; 5 or &lt; -5) is less than \\((1/5^2) = 0.04 = 4\\%\\). This is true for any shaped distribution (skewed, bimodal, etc.). In summary, for a quantitative variable, Use a histogram to display the distribution of one variable and describe the shape and any unusual features. For “well-behaved” distributions (symmetric, unimodal, no outliers), use the mean and standard deviation to describe the center and spread. Then z-scores will roughly follow the 68-95-99.7 rule stated above. For others, use the IQR and median. You can report both mean and median, but it’s usually a good idea to state why. "],
["one-quantitative-variable-and-one-categorical-variable.html", "2.6 One Quantitative Variable and One Categorical Variable", " 2.6 One Quantitative Variable and One Categorical Variable Let’s return to the NHANES data. We noticed variation in the amount people sleep. Why do some people sleep more than others? Are there any other characteristics that may be able to explain that variation? Let’s look at the distribution of hours of sleep at night within subsets of the NHANES data. 2.6.1 Multiple Histograms Does the recorded binary gender explain the variability in the hours of sleep? What are the ethical implications of collecting binary gender if individuals don’t fit neatly into the categories? NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_grid(~ Gender) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). Do you notice any differences in sleep hour distributions between males and females? Does the number of children a mother has explain the variability in the hours of sleep? Who have we excluded from our analysis by asking this question? NHANES %&gt;% filter(!is.na(nBabies)) %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_wrap(~ factor(nBabies), ncol = 4) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme_minimal() ## Warning: Removed 8 rows containing non-finite values (stat_bin). The 0 to 12 labels at the top of each of these panels correspond to the number of babies a mother had. Do you notice any differences in sleep hour distributions between these groups? Does the number of days someone has felt depressed explain the variability in the hours of sleep? NHANES %&gt;% ggplot(aes(x = SleepHrsNight)) + geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) + xlab(&#39;Hours of Sleep (hours)&#39;) + facet_grid(~ Depressed) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_bin). What’s the rightmost “NA” category? Some individuals in this study did not answer questions about days that they might have felt depressed, but they did report their hours of sleep per night. What selection biases or information biases might be at play here? 2.6.2 Multiple Boxplots Let’s visualize the same information but with boxplots instead of histograms and see if we can glean any other information. NHANES %&gt;% ggplot(aes(x = Gender, y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Binary Gender&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). NHANES %&gt;% ggplot(aes(x = factor(nBabies), y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Number of Babies&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). NHANES %&gt;% ggplot(aes(x = factor(Depressed), y = SleepHrsNight)) + geom_boxplot() + ylab(&#39;Hours of Sleep (hours)&#39;) + xlab(&#39;Days Depressed&#39;) + theme_minimal() ## Warning: Removed 2245 rows containing non-finite values (stat_boxplot). What do you notice? What do you wonder? 2.6.2.1 Is this a Real Difference? If we notice differences in the center of these distributions, is it a REAL difference? That is, is there a difference in the general U.S. population? Remember, we only have a random sample of the population. NHANES is supposed to be a representative sample of the U.S. population collected using a probability sampling procedure. What if there were no REAL difference? Then the Depressed group labels wouldn’t be related to the hours of sleep. Investigation Plan: Take all of the observed data on sleep and randomly shuffle into new groups (of the same size as before). This breaks any associations between Depressed group labels and hours of sleep. Calculate the difference in mean hours of sleep between the groups. Record it. Repeat 1 and 2 many times (say 1000 times). Look at the differences based on random shuffles &amp; compare to the observed difference. NHANES &lt;- NHANES %&gt;% mutate(DepressedMost = (Depressed == &#39;Most&#39;)) #TRUE or FALSE (converted to a 2 category variable) obsdiff &lt;- data.frame(d = diff(mean(SleepHrsNight ~ DepressedMost, data = NHANES, na.rm = TRUE))) sim &lt;- do(1000)*diff(mean(SleepHrsNight ~ shuffle(DepressedMost), data = NHANES, na.rm = TRUE)) #Randomly shuffle the DepressedMost labels (assuming no real difference in sleep, depressed feelings shouldn&#39;t impact sleep) Below, we have a histogram of 1000 values calculated by randomly shuffling individuals in the sample into two groups (assuming no relationship between depression and sleep) and then finding the difference in the mean amount of sleep. The red vertical line showed the observed difference in mean amount of sleep in the data. sim %&gt;% ggplot(aes(x = TRUE.)) + geom_histogram(fill = &#39;steelblue&#39;) + geom_vline(aes(xintercept = d), obsdiff, color = &#39;red&#39;) + xlab(&#39;Difference in Mean Hours of Sleep&#39;) + ylab(&#39;Counts&#39;) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The observed difference in mean hours of sleep (red line) is quite far from the distribution of differences that results when we break the association between depression status and sleep hours (through randomized shuffling of group labels). What do you think this indicates? What evidence for or against a real population difference might this provide? "],
["two-quantitative-variables.html", "2.7 Two Quantitative Variables", " 2.7 Two Quantitative Variables To discuss two quantitative variables, we will switch to new data set and consider a thought experiment. Imagine that you are an entrepreneur selling dress shirts. Clothing sizing can be quite variable across brands, so let’s use this dataset to try to come up with our own data-driven sizes. Two of the key measurements that we will use are the neck size in centimeters and chest size in centimeters. There are others in the data set, but let’s focus on these for the moment. 2.7.1 Scatterplot When you have two quantitative variables, a scatterplot is the main appropriate graphical display of the relationship. bodyfat &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/bodyfat.txt&quot;) bodyfat %&gt;% ggplot(aes(x = Neck, y = Chest)) + geom_point(color = &#39;steelblue&#39;) + xlab(&#39;Neck size (cm)&#39;) + ylab(&#39;Chest size (cm)&#39;) + theme_minimal() What do you notice about: Direction of relationship (positive, negative, neutral) Form of relationship (linear, curved, none, other) Strength of relationship (compactness around the relationship) Unusual features (outliers, differences in variability across \\(x\\)) How might you use this information to determine shirt sizes for your new business venture? Suppose instead of Chest in inches and Neck size in cm, we switched to inches and plotted Chest and Neck size in inches. Does the strength of the relationship change? Look at the plot in inches below. Does this plot look the same as the centimeters plot? bodyfat %&gt;% ggplot(aes(x = Neck/2.54, y = Chest/2.54)) + geom_point(color = &#39;steelblue&#39;) + xlab(&#39;Neck size (in)&#39;) + ylab(&#39;Chest size (in)&#39;) + theme_minimal() 2.7.2 Correlation Coefficient Since shifting (adding or subtracting) and scaling (multiplying or dividing) make no difference in the strength of the relationship, let’s standardize both variables into z-scores (recall z-scores from Section 2.5.5). Then we replot and add some color: The blue points in the upper right and lower left quadrants are either both positive or both negative in their z-score values. This means that those individuals are above average in both Neck Size and Chest Size (upper right), or they are below average in both Neck Size and Chest Size (lower left). If we multiply the x and y values for the blue points, we will get a positive value. The red points in the upper left and lower right quadrants are positive in one and negative in the other. This means that those individuals are either above average in Neck Size but below average in Chest Size or they are below average in Neck Size and above average in Chest Size. If we multiply the x and y values for the red points, we will get a negative value. If we were to have a weaker positive relationship, how would this plot change? If we were to have a negative relationship, how would this plot change? We want one number to represent strength and direction of a linear relationship. Points in Quadrants 1 and 3 (blue) have the Z-scores of the same sign. Points in Quadrants 2 and 4 (red) have Z-scores of the opposite sign. What if we took the product of the \\(z\\)-scores for \\(x\\) and \\(y\\) variables? Situation 1: An individual far above the means in both the \\(x\\) and \\(y\\) variables or far below the means in both the \\(x\\) and \\(y\\) variables will have a very large, positive product of z-scores. Situation 2: An individual far above the mean in \\(x\\) and far below the mean in \\(y\\) will have a very large, negative product of z-scores. (The same goes for low \\(x\\) and high \\(y\\).) The almost average of products of the \\(z\\)-scores is the correlation coefficient, \\[ r_{x,y} = \\frac{\\sum z_x z_y}{n-1} \\] \\(r_{x,y}\\) is the correlation coefficient between variables \\(x\\) and \\(y\\). Some observations: If most of our data points follow Situation 1, the correlation coefficient is an average of mostly large positive values. Thus the correlation coefficient will be high and positive. If most of our data points follow Situation 2, the correlation coefficient is an average of mostly large negative values. Thus the correlation coefficient will be high and negative. If about an equal number of data points follow Situation 1 and Situation 2, we will be averaging a balance of positive and negative numbers, which will be close to zero. Thus the correlation coefficient will be close to zero. Which points contribute the most to this average? Let’s look at the correlation for the entire sample first. Then let’s calculate the correlation for individuals around the mean Neck size. cor(bodyfat$Neck, bodyfat$Chest) # All data points used in calculation ## [1] 0.7688109 subset_bodyfat &lt;- bodyfat %&gt;% filter(Neck &gt; 35 &amp; Neck &lt; 40) # Keep individuals with Neck size between 35cm and 40cm cor(subset_bodyfat$Neck, subset_bodyfat$Chest) # Only middle subset of data points used in calculation ## [1] 0.5658835 2.7.3 Properties \\(-1 \\leq r \\leq 1\\) Sign of \\(r\\) goes with the direction of the relationship. \\(r_{x,y} = r_{y,x}\\), it doesn’t matter which is \\(x\\) and which is \\(y\\). \\(r_{ax+b, cy+d} = r_{x,y}\\), linear change of scale doesn’t affect \\(r\\). Why? \\(r\\) measures strength of linear relationship (not a curved relationship) One outlier can completely change \\(r\\). Let’s look at a few scatterplot examples and the corresponding correlation. (Optional) Other expressions for r for the mathematically intrigued: \\[ r = \\frac{\\sum z_x z_y}{n-1} \\] \\[ = \\frac{\\sum{\\frac{(x_i-\\bar{x})}{s_x}\\frac{(y_i-\\bar{y})}{s_y}}}{n-1}\\] \\[= \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{(n-1) s_x s_y}\\] \\[= \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{(n-1)\\sqrt{\\sum{\\frac{(x_i-\\bar{x})^2}{n-1}}}}{\\sqrt{\\sum{\\frac{(y_i-\\bar{y})^2}{n-1}}}}}\\] \\[=\\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{\\sqrt{\\sum{(x_i-\\bar{x})^2}}}{\\sqrt{\\sum{(y_i-\\bar{y})^2}}}}\\] \\[=\\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}}\\] 2.7.4 Is correlation always the right way to judge strength? The plot below shows the relationship between brownie quality and oven temperature at which the brownie is baked. The correlation is near 0, but it doesn’t mean that there’s no relationship. We can clearly see a quadratic relationship, but there’s just no linear relationship. "],
["three-or-more-variables.html", "2.8 Three or more variables", " 2.8 Three or more variables In complex data sets that measure many variables, it is necessary to get a fuller understanding of the relationships in the data than we can see with plots that look at only one or two variables. The following visual features can help us turn bivariate plots into multivariate plots: Color of points and lines Shape of points Size of points Panels (facets) Let’s look at another data set, the 1985 Current Population Survey. This is a smaller scale survey administered by the United States government between the decennial census. 2.8.1 A bivariate plot Our primary interest is the wage variable which gives the hourly wage for each individual in the data set in US dollars. What is the relationship between years of education and hourly wage? data(CPS85) # Load the data CPS85 %&gt;% ggplot(aes(x = educ, y = wage)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + theme_minimal() We can see that years of education and hourly wage are positively correlated but what about the impact of other variables? 2.8.2 Enriching with color We can enrich this bivariate plot by showing additional information via color. data(CPS85) ## Load the data CPS85 %&gt;% ggplot(aes(x = educ, y = wage, color = age)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + theme_minimal() Adding color for a quantitative variable, age, does not reveal any obvious patterns; that is, we don’t see obvious clustering by color. Perhaps this is because there are too many colors (remember Visualization Principle #6: Use Color Appropriately). Are any patterns revealed if we use 4 age categories instead? CPS85 %&gt;% mutate(age_cat = cut(age, 4)) %&gt;% ggplot(aes(x = educ, y = wage, color = age_cat)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + guides(color = guide_legend(title = &quot;Age category&quot;)) + # Legend title theme_minimal() With 4 age categories, no age patterns are evident, but this does help us see that the least educated people in this data set are mostly in the youngest and oldest age categories. 2.8.3 Enriching with shape We can also encode information via point shape. Here we let shape encode marital status. CPS85 %&gt;% ggplot(aes(x = educ, y = wage, shape = married)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + guides(shape = guide_legend(title = &quot;Marital status&quot;)) + # Legend title theme_minimal() Often encoding information with color is preferable to encoding it with shapes because differences in shapes are not as easily discernible. Remember that statistical visualizations are meant to help you better understand your data. If you are having trouble easily picking out patterns when using a certain visual feature (e.g. shape, color), try another one to see if the clarity of the plot increases for you. 2.8.4 Enriching with size The size of a point is useful for conveying the magnitude of a quantitative variable. For example, we may wish to see non-categorized age information with point size. CPS85 %&gt;% ggplot(aes(x = educ, y = wage, size = age)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + guides(size = guide_legend(title = &quot;Age&quot;)) + # Legend title theme_minimal() 2.8.5 Enriching with panels Panels (or facets) are a great way to see how relationships differ between levels of a single categorical variable or between combinations of two categorical variables. Let’s look at the relationship between hourly wage and years of education across job sectors. The following creates a row of plots of this relationship over job sectors. CPS85 %&gt;% ggplot(aes(x = educ, y = wage)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + facet_grid(. ~ sector) + theme_minimal() With a small change in notation (sector ~ . versus . ~ sector), we can create a column of plots. CPS85 %&gt;% ggplot(aes(x = educ, y = wage)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + facet_grid(sector ~ .) + theme_minimal() We can also create panels according to two categorical variables. How do the relationships additionally differ by union status? CPS85 %&gt;% ggplot(aes(x = educ, y = wage)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + facet_grid(sector ~ union) + theme_minimal() 2.8.6 Putting everything together The combination of these different visual features can result in powerful visual understanding. Let’s combine paneling with color information to explore if there are marital status patterns in these union-job sector subgroups. CPS85 %&gt;% ggplot(aes(x = educ, y = wage, color = married)) + geom_point() + xlab(&quot;Years of education&quot;) + ylab(&quot;Hourly wage (US dollars)&quot;) + facet_grid(sector ~ union) + guides(color = guide_legend(title = &quot;Marital status&quot;)) + # Legend title theme_minimal() Creating effective multivariate visualizations takes a lot of trial and error. Some visual elements will better highlight patterns than others, and often times, you’ll have to try several iterations before you feel that you are learning something insightful from the graphic. Be tenacious, and keep in mind the good visualization principles outlined at the beginning of this chapter! "],
["major-takeaways-1.html", "2.9 Major Takeaways", " 2.9 Major Takeaways STOP: Think about whether a variable is categorical or quantitative. This informs the type of graphic and summaries that is appropriate. The shape of a histogram tells you about the relationships between mean and median. If you are describing a histogram, make sure to comment on the shape, center, spread, and outliers. If you are describing a scatterplot, make sure to comment about the direction, form, strength, and unusual features. Visualizations are just a starting place. Stop to notice what you learn and what questions you have about the data. Let that inform the next visualization you make. "],
["regression-models.html", "Chapter 3 Regression Models", " Chapter 3 Regression Models In our visualization of data, we’ve seen trends, patterns, and variation. Let’s now endeavor to describe those trends, patterns, and variation more precisely and quantitatively by building statistical models. "],
["goals-for-models.html", "3.1 Goals for models", " 3.1 Goals for models Broadly, a model is a simplified representation of the world. When we build models, we may have different goals. One goal when building models is prediction. Given data on a response or outcome variable \\(y\\) and one or more predictor or explanatory variables \\(x\\), the goal is to find a function, \\(f\\) that will model and predict \\(y\\) from \\(x\\). This \\(x\\) may be a single variable, but is most often a set of variables. We’ll be building up to multivariate modeling over the course of this chapter. What are the qualities of a good function \\(f\\)? We want to find an \\(f(x)\\) such that \\(\\hat{y} = f(x)\\) is a good predictor of \\(y\\). In other words, we want the model prediction \\(\\hat{y}\\) to be close to the observed response. We want \\(y-\\hat{y}\\) to be small. This difference \\(y-\\hat{y}\\) is called a residual. We’ll discuss residuals more later. Can you think of some concrete examples in which we’d want a model to do prediction? Another goal when building models is description. We want a model, \\(f(x)\\), to “explain” the relationship between the \\(x\\) and \\(y\\) variables. Note that an overly complicated model may not be that useful here because it can’t help us understand the relationship. A more complex model may, however, produce better predictions. George Box is often quoted “All models are wrong but some are useful.” Depending on our goal, one model may be more useful than another. Can you think of some concrete examples in which we’d want a model to do explain a phenomenon? To begin, we will consider a simple, but powerful model in which we limit this function \\(f\\) to be a straight line with a y-intercept, \\(b_0\\), and slope, \\(b_1\\). \\[\\hat{y} = f(x) = b_0 + b_1x\\] This is a simple linear regression model. It is the foundation of many statistical models used in modern statistics and is more flexible than you may think. In the past, you may have seen the equation of a line as \\[y = mx + b\\] where \\(m\\) is the slope and \\(b\\) is the y-intercept. We will be using different notation so that it can generalize to multiple linear regression. The y-intercept is the value when \\(x=0\\) and the slope is change in y for each addition value of x (rise/run). "],
["lines.html", "3.2 Lines", " 3.2 Lines Let’s return to the thought experiment in which you were a manufacturer of dress shirts. bodyfat &lt;- read.delim(&quot;http://sites.williams.edu/rdeveaux/files/2014/09/bodyfat.txt&quot;) bodyfat %&gt;% ggplot(aes(x = Neck, y = Chest)) + geom_point(color = &#39;steelblue&#39;) + xlab(&#39;Neck size (cm)&#39;) + ylab(&#39;Chest size (cm)&#39;) + theme_minimal() If you were to add one or multiple lines to the plot above to help you make business decisions, where would you want it (or them)? Let’s say you were only going to make one size of shirt. You might want to add a horizontal line at the mean Chest size and a vertical line at the mean Neck size. bodyfat %&gt;% ggplot(aes(x = Neck, y = Chest)) + geom_point(color = &#39;steelblue&#39;) + geom_hline(yintercept = mean(bodyfat$Chest)) + geom_vline(xintercept = mean(bodyfat$Neck)) + xlab(&#39;Neck size (cm)&#39;) + ylab(&#39;Chest size (cm)&#39;) + theme_minimal() We can see that a shirt made to these specifications would fit the “average person.” However, this might not serve your market very well. For many people, the shirt would be too tight because their chest and/or neck sizes would be larger than average. For many people, the shirt would be too large because they chest and/or neck sizes would be smaller than average. Let’s try something else. Let’s allow ourselves 5 different sizes (XS, S, M, L, XL). Then, we can cut the Neck sizes variable into 5 groups of equal length and estimate the mean Chest sizes within each of these groups. What do these lines tell us for our business venture? What if we wanted to be able to make more sizes? Could we get a pretty good sense of what the chest sizes should be for a given neck size? Could we find one line to describe the relationship between Neck size and Chest size? What does line tell us for our business venture? If the scatterplot between two quantitative variables resembles a straight line, a straight line could roughly describe the mean of y for each value of x. a straight line could describe how much we’d expect y to change based on a 1 unit change in x. a straight line could help us predict the y based on a new value of x. "],
["choosing-the-best-fitting-line.html", "3.3 Choosing the best fitting line", " 3.3 Choosing the best fitting line To choose the best fitting line, we need to choose the right intercept (\\(b_0\\)) and slope (\\(b_1\\)), \\[ \\hat{y} = f(x) = b_0 + b_1x \\] We have \\(n\\) points on a scatterplot, \\((x_i,y_i)\\) where \\(i=1,...,n\\). 3.3.1 First idea Minimize the sum of the residuals, \\(e_i = y_i - \\hat{y}_i = y_i - ( b_0 + b_1x_i)\\). The residual is the prediction error, the difference between what you observe and what you predict based on the line. Problem: We will have positive and negative residuals; they will cancel each other out if we add them together. This won’t give us what we want. 3.3.2 Second idea Minimize the sum of the absolute value of the residuals, \\(\\sum_{i=1}^n |y_i - \\hat{y}_i| = \\sum_{i=1}^n |e_i|\\). Problem: This is referred to as Least Absolute Deviations, but there isn’t always one unique line that satisfies this. So, this won’t give us what we want. 3.3.3 Third idea Minimize the sum of squared residuals, \\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2= \\sum_{i=1}^n e_i^2\\). This is referred to as Least Squares and has a unique solution. Yeah!!! "],
["least-squares.html", "3.4 Least Squares", " 3.4 Least Squares Let’s try to find the minimum of the Sum of Squared Residuals by searching over a grid of values for (intercept, slope). Below is a visual of the sum of squared residuals for a variety of values on a grid. The surface height is sum of squared residuals for each combination of slope and intercept. We can see there is valley where the minimum must be. Let’s visualize this in a slightly different way. We’ll encode the surface height as color (white is lowest). The large values of the sum of squared residuals are dominating this image, so let’s change the color scheme to see more variation in smaller values (white is lowest). We can limit our search to \\(b_0 \\in (-10,10)\\) and \\(b_1 \\in (2,3)\\). b0 = seq(-10,10,by=.05) b1 = seq(2,3,by=.05) b &lt;- expand.grid(b0,b1) ss &lt;- apply(b,1,f) b[ss == min(ss),] ## Var1 Var2 ## 6142 -3.7 2.75 We have the minimum point. Over the grid of pairs of values, the minimum sum of squared residuals happens when the intercept is -3.7 and the slope is 2.75. (Optional) Alternative ways to find the minimum sum of squared residuals We could try a numerical optimization algorithm such as steepest descent. We could use multivariable calculus (find partial derivatives, set equal to 0, and solve). To get started on the calculus, solve the following two equations for the two unknowns (\\(b_0\\) and \\(b_1\\)): \\[\\frac{\\partial }{\\partial b_0}\\sum_{i=1}^n (y_i - (b_0 + b_1x_i))^2 = 0\\] \\[\\frac{\\partial }{\\partial b_1}\\sum_{i=1}^n (y_i - (b_0 + b_1x_i))^2 = 0\\] If you are a math/physics/cs major, you should try this by hand and see if you can get the solutions below. If you find the minimum via calculus, you’ll find that we can write the Least Squares solution in an equation format as functions of summary statistics (!), the estimated slope is \\[ b_1 = r\\frac{s_y}{s_x}\\] and the estimated intercept is \\[ b_0 = \\bar{y} - b_1\\bar{x} \\] where \\(\\bar{x}\\) is the mean of the variable on the x-axis, \\(\\bar{y}\\) is the mean of the variable on the y-axis, \\(s_x\\) is the standard deviation of the variable on the x-axis, \\(s_y\\) is the standard deviation of the variable on the y-axis, and \\(r\\) is the correlation coefficient between the two variables. Let’s do that calculation “by hand” first in R. sy = sd(bodyfat$Chest) sx = sd(bodyfat$Neck) r = cor(bodyfat$Chest,bodyfat$Neck) ybar = mean(bodyfat$Chest) xbar = mean(bodyfat$Neck) (b1 = r*sy/sx) ## [1] 2.736883 (b0 = ybar - b1*xbar) ## [1] -3.188483 From now on, we’ll use the lm() function which stands for linear model. This gives us the Least Squares solution to the “best fitting line”. lm(Chest ~ Neck, data = bodyfat) # When you see ~, think &#39;as a function of&#39; ## ## Call: ## lm(formula = Chest ~ Neck, data = bodyfat) ## ## Coefficients: ## (Intercept) Neck ## -3.188 2.737 "],
["properties-of-least-squares-line.html", "3.5 Properties of Least Squares Line", " 3.5 Properties of Least Squares Line \\((\\bar{x},\\bar{y})\\) is on the line. Least squares residuals sum to 0. The standard deviation of the residuals gives us a sense of how bad our predictions (based on the line) could be. \\[s_e = \\sqrt{\\frac{\\sum^n_{i=1} (y_i-\\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum^n_{i=1} e_i^2}{n-2}} \\] In R: \\(s_e\\) is called the “residual standard error”. lm.fit = lm(Chest ~ Neck, data = bodyfat) summary(lm.fit) ## ## Call: ## lm(formula = Chest ~ Neck, data = bodyfat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.1447 -3.3328 -0.5342 3.0738 16.5974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.1885 5.4951 -0.58 0.562 ## Neck 2.7369 0.1446 18.93 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.225 on 248 degrees of freedom ## Multiple R-squared: 0.5911, Adjusted R-squared: 0.5894 ## F-statistic: 358.5 on 1 and 248 DF, p-value: &lt; 2.2e-16 tidy(lm.fit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -3.19 5.50 -0.580 5.62e- 1 ## 2 Neck 2.74 0.145 18.9 4.59e-50 This simple linear regression line is \\[\\widehat{Chest} = -3.18 + 2.73*Neck\\] Given your neck size, we can predict your chest size within 5 to 10 inches since \\(s_e = 5.22\\). If you were a shirt manufacturer, what would you do with this information? 3.5.1 Real companies Let’s see how some real companies create shirts. In the plots below, the red boxes represent the advertised range for Neck and Chest sizes for each brand. For Calvin Klein, we see that the red boxes are below the least squares line (black line). So for a given neck size, Calvin Klein makes shirts that are a little bit too small at the chest. For Express, we see that the red boxes are generally on the least squares line, except for the smallest size. This means that Express shirts are generally a good fit at the chest and neck for the 4 largest sizes, but the smallest shirt size is a bit too small at the chest for the neck size. For Brooks Brothers, the red boxes are a bit below the least squares line for the 3 smallest sizes and a little above the line for the largest size. This means that the 3 smallest sizes are a bit too small at the chest for those neck size and that the largest shirt is a bit big at the chest for that neck size. We haven’t told you how the sample data we’ve been using was collected. As you compared the brands to this sample data, what assumptions were you making about the population that the sample was drawn from? What questions do you have about the sample data? "],
["interpretation.html", "3.6 Interpretation", " 3.6 Interpretation Let’s look at the summary of the lm() function in R again. summary(lm.fit) ## ## Call: ## lm(formula = Chest ~ Neck, data = bodyfat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.1447 -3.3328 -0.5342 3.0738 16.5974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.1885 5.4951 -0.58 0.562 ## Neck 2.7369 0.1446 18.93 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.225 on 248 degrees of freedom ## Multiple R-squared: 0.5911, Adjusted R-squared: 0.5894 ## F-statistic: 358.5 on 1 and 248 DF, p-value: &lt; 2.2e-16 Least Squares/Regression Line (\\(\\hat{y} = b_0 + b_1x\\)) The line gives the estimated average of \\(y\\) for each value of \\(x\\) (within observed range of x) Example: The regression line of (Predicted Chest = -3.18 + 2.73*Neck) gives the estimated average Chest size for a given Neck size, based on our sample of data. The “within observed range of \\(x\\)” is very important. We can’t predict values of \\(y\\) for values of \\(x\\) that are very different from what we have in our data. Trying to do so is a big no-no called extrapolation. We’ll discuss this more in a bit. Intercept (\\(b_0\\)) The intercept gives the average value of \\(y\\) when \\(x\\) is zero (think about context) Example: If Neck size = 0, then the person doesn’t exist. In this context, the intercept doesn’t make much sense to interpret. For this reason, sometimes the predictor \\(x\\) is centered. That is, the mean of \\(x\\) for all individuals in the sample is computed, and this mean is subtracted from each person’s \\(x\\) value. In this case, the intercept is interpreted as the average value of \\(y\\) when \\(x\\) is at its (sample) mean value. Slope (\\(b_1\\)) The slope gives the change in average \\(y\\) per 1 unit increase in \\(x\\) (not for individuals and not causal) Example: If we consider all individuals with a neck size 1 cm larger, then we’d expect the average chest size to be by about 2.7cm larger. 3.6.1 Correlation or Association vs. Causation What is causation? What is a causal effect? Are there criteria for defining a cause? These are deep questions that have been debated by scientists of all domains for a long time. We are at a point now where there is some consensus on the definition of a causal effect. The causal effect of a variable \\(x\\) on another variable \\(y\\) is the amount that we expect \\(y\\) to change if we intervene on/manipulate \\(x\\) by changing it by one unit. When can we interpret an estimated slope \\(b_1\\) from a simple linear regression as a causal effect of \\(x\\) on \\(y\\)? Well, in the simple linear regression case, almost never. To interpret the slope causally, there would have to be no confounding variables (no variables that impact both \\(x\\) and \\(y\\).) In no real world situation will this be the case. Then what are we to do? We will get to adjusting/controlling for confounders in the section on multiple linear regression. Multiple linear regression offers us a way to obtain causal effects of variables if we use it carefully. It will be tempting to say: control for everything we can! But we will see that this is not the correct thing to do, as there are very real dangers of over-controlling for variables. For now, let it suffice to say that multiple linear regression is as useful as the corn kerneler below. Immensely useful in the right circumstances - but only those circumstances. Source: Gizmodo "],
["evaluation.html", "3.7 Evaluation", " 3.7 Evaluation In this section, we consider model evaluation. We seek to develop tools that allow us to answer: is this model “good”? 3.7.1 Prediction Let’s consider another data example. Can we predict your college GPA based on your high school GPA? (Disclaimer: this is not Macalester data.) sat &lt;- read.csv(&quot;Data/sat.csv&quot;) sat %&gt;% ggplot(aes(x = high_GPA, y = univ_GPA)) + geom_point(color = &#39;steelblue&#39;) + geom_smooth(method = &#39;lm&#39;, se = FALSE) + xlab(&#39;High School GPA&#39;) + ylab(&#39;College GPA&#39;) + theme_minimal() First things first. Describe the scatterplot. Direction: Positive relationship (higher high school GPA is associated with higher college GPA) Form: generally linear Strength: There is a weak relationship when high school GPA &lt; 3.0 (r = .32) and a fairly strong relationship when high school GPA &gt; 3.0 (r = .68). Unusual: As seen with the strength, there is increased variability in college GPA among individuals with lower high school GPA. That variability decreases with increased high school GPA. We call this pattern of unequal variation as “thickening.” The code below computes the correlation coefficients separately for students with high school GPAs above 3 and for students with high school GPAs less than or equal to 3. We see that the correlation is higher for the high GPA group. sat %&gt;% mutate(HighHSGPA = high_GPA &gt; 3) %&gt;% group_by(HighHSGPA) %&gt;% summarize(Cor = cor(high_GPA,univ_GPA)) ## # A tibble: 2 x 2 ## HighHSGPA Cor ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 0.316 ## 2 TRUE 0.687 Let’s build a model to predict college GPA based on high school GPA based on this sample data. Since we noted that there was a linear relationship, let’s find the least squares regression line. lm.gpa &lt;- lm(univ_GPA ~ high_GPA, data = sat) summary(lm.gpa) ## ## Call: ## lm(formula = univ_GPA ~ high_GPA, data = sat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69040 -0.11922 0.03274 0.17397 0.91278 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.09682 0.16663 6.583 1.98e-09 *** ## high_GPA 0.67483 0.05342 12.632 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2814 on 103 degrees of freedom ## Multiple R-squared: 0.6077, Adjusted R-squared: 0.6039 ## F-statistic: 159.6 on 1 and 103 DF, p-value: &lt; 2.2e-16 The best fitting line is \\[ \\hbox{Predicted College GPA} = 1.09 + 0.675 \\times \\hbox{High School GPA} \\] Let’s plug in a few values. If High School GPA = 2: \\[ \\hbox{Predicted College GPA} = 1.09 + 0.675 \\times 2 = 2.44 \\] ## Calcualtion by hand 1.09 + 0.675*2 ## [1] 2.44 ## Calcuation using R&#39;s predict() function predict(lm.gpa, newdata = data.frame(high_GPA = 2)) ## 1 ## 2.446483 If High School GPA = 3.5: \\[ \\hbox{Predicted College GPA} = 1.09 + 0.675 \\times 3.5 = 3.45 \\] 1.09 + 0.675*3.5 ## [1] 3.4525 predict(lm.gpa, newdata = data.frame(high_GPA = 3.5)) ## 1 ## 3.458728 If High School GPA = 4.5: \\[ \\hbox{Predicted College GPA} = 1.09 + 0.675 \\times 4.5 = 4.13 \\] 1.09 + 0.675*4.5 ## [1] 4.1275 predict(lm.gpa, newdata = data.frame(high_GPA = 4.5)) ## 1 ## 4.133558 Does it make sense to use this model for high school GPA’s &gt; 4? Some high schools have a max GPA of 5.0 due to weighting of advanced courses. What is the maximum high school GPA in this data set? What if your college doesn’t allow for GPA’s above 4.0? sat %&gt;% summarize(max(high_GPA)) # Max high school GPA in this data set ## max(high_GPA) ## 1 4 Making predictions beyond the observed range of values is called extrapolation and is generally a risky thing to do. If you extrapolate beyond the minimum or maximum of the observed values, then you are assuming that the relationship you observe can be extended into the new prediction range. This is the main issue of forecasting, making predictions in the future. You have to assume that the trend that you observe now will continue in the future and that the current state of affairs will stay the same. For an infamous case of extrapolation, check out this article that appeared in the journal Nature. 3.7.2 Prediction Errors Recall that a residual \\(e_i\\) for data point \\(i\\) is the difference between the actual and predicted values: \\(e_i = y_i - \\hat{y}_i\\). If the residuals were approximately unimodal and symmetric, we expect about 95% of the residuals to be within 2 standard deviations of 0 (the mean residual). (Recall Section 2.5.5.) hist(residuals(lm.gpa), main = &quot;Distribution of residuals in the GPA model&quot;) Below we calculate \\(SSE\\), the sum of squared residuals, and the standard deviation of the residuals, \\(s_e\\). SSE = sum(residuals(lm.gpa)^2) n = length(residuals(lm.gpa)) # Sample size s = sqrt(SSE/(n-2)) # Residual standard error s ## [1] 0.2814443 2*s ## [1] 0.5628886 Using this model (that is, using your high school GPA), we can predict your college GPA within about \\(0.56\\) GPA points. Is this useful? Is a margin of \\(0.56\\) GPA points good enough? Let’s compare this margin of error with the margin of error about the mean (the standard deviation): sd(sat$univ_GPA) ## [1] 0.4471936 2*sd(sat$univ_GPA) ## [1] 0.8943873 Without knowing your high school GPA, we could have just guessed your college GPA as the mean college GPA in the sample, and this guess would be within \\(\\pm 0.89\\) of your actual college GPA. This is a higher margin of error than the approximate \\(0.56\\) if we did use your high school GPA (in the simple linear regression model). We are able to predict your college GPA with a smaller margin of error than if we just guessed your college GPA with the mean. Our model has explained some (but not all) of the variation in college GPA. The standard deviation of college GPA is based on the sum of squared total variation, SSTO (variation around the mean), \\[ SSTO = \\sum{(y_i -\\bar{y})^2} \\] \\(SSTO\\) is the numerator of the standard deviation of \\(y\\) (without knowing anything about \\(x\\)). (SSTO = sum((sat$univ_GPA - mean(sat$univ_GPA))^2)) ## [1] 20.79814 We define \\(SSTO\\) here because it will help to compare \\(SSTO\\) to \\(SSE\\) (sum of squared residuals from the model) to obtain a measure of how well our models are fitting the data - how well they are predicting the outcome. 3.7.3 \\(R^2\\) Let’s study how models reduce unexplained variation. Unexplained variation is given by \\(SSTO\\). It is just the overall variability in the outcome. Think back to interpreting standard deviation and variance as measures of spread. We used these to describe broadly how variable the outcome was. Variation in the outcome that remains after trying to explain the outcome is given by \\(SSE\\), the sum of squared residuals. So to study how models reduce unexplained variation, we will compare residuals from a linear regression model (which uses the predictor \\(x\\)) with the original deviations from the mean (which do not use the predictor \\(x\\)). We started with the sum of the deviations from the mean \\(SSTO = \\sum{(y_i - \\bar{y})^2}\\) before we had info about high school GPA (\\(x\\)). Now, with our knowledge of \\(x\\), we have \\(SSE = \\sum{(y_i - \\hat{y_i})^2}\\) \\(SSE\\) should be smaller than \\(SSTO\\) (!) Two extreme cases: If the error \\(SSE\\) goes to zero, we’d have a “perfect fit”. If \\(SSE = SSTO\\), \\(x\\) has told us nothing about \\(y\\). So we define a measure called R-squared, which is the fraction or percent of the total variation in \\(y\\) “accounted for” by the model in \\(x\\). \\[ R^2 = 1 - \\frac{SSE}{SSTO} = 1 - \\frac{ \\sum{(y_i - \\hat{y_i})^2}}{ \\sum{(y_i - \\bar{y})^2}}\\] In R, lm() will calculate R-Squared for us, but we can also see that it equals the value from the formula above. 1 - SSE/SSTO ## [1] 0.6077187 glance(lm.gpa) #r.squared = R^2, sigma = s_e (ignore the rest) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.608 0.604 0.281 160. 1.18e-22 2 -14.9 35.7 43.7 ## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Is there a “good” value of \\(R^2\\)? Same answer as correlation – no. \\(R^2\\) doesn’t tell you the direction or the form of the relationship. Note: \\(R^2 = r^2\\) for simple linear models with one x variable (where \\(r\\) is the correlation coefficient). "],
["diagnostics.html", "3.8 Diagnostics", " 3.8 Diagnostics Residuals are what’s left over from a linear fit. We can actually learn a lot by studying what is left over, what is left unexplained by the model. INSERT SCATOLOGY JOKE. What do we need for a simple linear model to make sense? Variables are both Quantitative Relationship is Straight Enough There are no extreme Outliers Spread is roughly same throughout – No Thickening To check these, we look at the original scatterplot and a plot of residuals vs. fitted or predicted values, augment(lm.gpa, data = sat) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + labs(x = &quot;Fitted values (predicted values)&quot;, y = &quot;Residuals&quot;) + theme_minimal() What do you think? Is there any pattern? (Is the original scatterplot straight enough?) Is there equal spread across prediction values? We want to avoid extreme outliers because points that are both far from the mean of x and do not fit the overall relationship have leverage to change the line. See the example below. See how the relationship changes with the addition of one point, one extreme outlier. Check out this interactive visualization to get a feel for leverage: http://omaymas.github.io/InfluenceAnalysis/ 3.8.1 Solutions to Issues If the observed data don’t satisfy the conditions above, what can we do? Should we give up using a statistical model? No! Problem: Both variables are NOT Quantitative If your x-variable is categorical, we’ll turn it into a quantitative variable using indicator variables (coming up) If you have a binary variable (exactly 2 categories) that you want to predict as your y variable, we’ll use logistic regression (coming up) Problem: Relationship is NOT Straight Enough If the plot does not thicken, we can add higher degree (e.g. \\(x^2, x^3, x^4\\), etc.) terms to the model (multiple linear regression - coming up). If the plot does thicken, see solutions below. Problem: Spread is NOT the same throughout You may be able to transform the y-variable using mathematical functions (\\(log(y)\\), \\(y^2\\), etc.) to make the spread more consistent (one approach is to use Box-Cox Transformation – take more statistics classes to learn more) Be careful in interpreting the standard deviation of the residuals. (For example, if you take a log transformation of the outcome, the units of the standard deviation of the residuals will be on the log scale.) Problem: You have extreme outliers Look into the outliers. Determine if they could be due to human error. Think carefully about them, dig deep. Do a sensitivity analysis: Fit a model with and without the outlier and see if your conclusions drastically change (see if those points had leverage to change the model). "],
["multiple-linear-regression.html", "3.9 Multiple Linear Regression", " 3.9 Multiple Linear Regression We can generalize the idea of a simple linear model by including many explanatory variables (X’s). A multiple linear regression model can be written as: \\[\\hat{y}_i = b_0 + b_1x_{i1} + \\cdots + b_kx_{ik}\\] Each coefficient \\(b_j\\) can be interpreted as the increase in the predicted/average y associated with a 1 unit increase in \\(x_j\\), keeping all other variables constant. (*There are some exceptions - we’ll get there.) These X variables can be: Quantitative variables (or transformations of them) Indicator variables for categorical variables (only need \\(k-1\\) indicators for a variable with \\(k\\) categories) Interaction terms (product of two variables, which allows for effect modification) Let’s talk about a new data example: home prices. We want to build a model to predict the price of a home based on its many characteristics. Here we have a data set of homes recently sold in New England with many variables such as the age of the home, the land value, whether or not it has central air conditioning, the number of fireplaces, the sale price, and more… homes &lt;- read.delim(&#39;http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt&#39;) head(homes) ## Price Lot.Size Waterfront Age Land.Value New.Construct Central.Air ## 1 132500 0.09 0 42 50000 0 0 ## 2 181115 0.92 0 0 22300 0 0 ## 3 109000 0.19 0 133 7300 0 0 ## 4 155000 0.41 0 13 18700 0 0 ## 5 86060 0.11 0 0 15000 1 1 ## 6 120000 0.68 0 31 14000 0 0 ## Fuel.Type Heat.Type Sewer.Type Living.Area Pct.College Bedrooms ## 1 3 4 2 906 35 2 ## 2 2 3 2 1953 51 3 ## 3 2 3 3 1944 51 4 ## 4 2 2 2 1944 51 3 ## 5 2 2 3 840 51 2 ## 6 2 2 2 1152 22 4 ## Fireplaces Bathrooms Rooms ## 1 1 1.0 5 ## 2 0 2.5 6 ## 3 1 1.0 8 ## 4 1 1.5 5 ## 5 0 1.0 3 ## 6 1 1.0 8 *The exception to the interpretation comment above is if our variables are strongly correlated. In this case, we cannot keep all other variables constant because if you increase the value of one, then a variable with high correlation will also likely change in value. 3.9.1 Indicator Variables In New England, fireplaces are often used as a way to heat the house. Let’s study the impact of a fireplace has on the sale price of a home. In particular, we only care if the home has 1 or more fireplaces or no fireplaces. So we make a new variable that is TRUE if there are more than 0 fireplaces in a home and FALSE otherwise. homes &lt;- homes %&gt;% mutate(AnyFireplace = Fireplaces &gt; 0) In order to include this information in our linear regression model, we need to turn that categorical variable (AnyFireplace, TRUE or FALSE) into an indicator variable, which has a numeric value of 0 or 1: \\[ 1_{AnyFireplaceTRUE} = \\begin{cases}1 \\quad \\text{ if a home has at least one fireplace}\\\\ 0\\quad \\text{ if a home does not have a fireplace} \\end{cases}\\] In fact, R creates this indicator for you when you put a categorical variable as an X variable in the model. lm.home &lt;- lm(Price ~ AnyFireplace, data = homes) tidy(lm.home) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 174653. 3419. 51.1 0. ## 2 AnyFireplaceTRUE 65261. 4522. 14.4 1.17e-44 Our “best fitting line” is \\[ \\hbox{Predicted Price} = 174653.35 + 65260.61 \\times 1_{AnyFireplaceTRUE} \\] What does this mean? Let’s think about two types of homes: a home with one or more fireplaces and a home without a fireplace. Let’s write out the equations for those two types of homes. Home with fireplace: \\[ \\hbox{Predicted Price} = 174653.35 + 65260.61 \\times 1 = \\$ 239,914 \\] 174653.35 + 65260.61*1 ## [1] 239914 Home without fireplace: \\[ \\hbox{Predicted Price} = 174653.35 + 65260.61 \\times 0 = \\$ 174,653.35 \\] The difference between these predicted prices is $65,260.61. So is this how much a fireplace is worth? If I installed a fireplace in my house, should the value of my house go up $65,260? No, because we are not making causal statements based on observational data. What could be confounding this relationship? What third variable may be related to both the price and whether or not a house has a fireplace? Let’s look at the size of the house. Is price related to the area of living space (square footage)? homes %&gt;% ggplot(aes(x = Living.Area, y = Price)) + geom_point(color = &#39;steelblue&#39;) + theme_minimal() Is the presence of a fireplace related to area of living space? homes %&gt;% ggplot(aes(x = AnyFireplace, y = Living.Area)) + geom_boxplot() + theme_minimal() We see that the amount of living area differs between homes with fireplaces and homes without fireplaces. Thus, Living Area could confound the relationship between AnyFireplace and Price because it is related to both variables. Let’s put Living Area in the model along with AnyFireplace to account for it (to control/adjust for it). lm.home2 &lt;- lm(Price ~ AnyFireplace + Living.Area, data = homes) tidy(lm.home2) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13599. 4992. 2.72 6.51e- 3 ## 2 AnyFireplaceTRUE 5567. 3717. 1.50 1.34e- 1 ## 3 Living.Area 111. 2.97 37.5 2.20e-225 Our “best fitting line” is \\[ \\hbox{Predicted Price} = 13599.16 + 5567.37 \\times 1_{AnyFireplaceTRUE} + 111.21 \\times \\hbox{Living.Area} \\] What does this mean? Let’s think about two types of homes: a home with one or more fireplaces and a home without a fireplace. Home with fireplace: \\[ \\begin{align*} \\hbox{Predicted Price} &amp;= 13599.16 + 5567.37 \\times 1 + 111.21 \\times \\hbox{Living.Area} \\\\ &amp;= \\$19,166.53 + \\$111.21 \\times \\hbox{Living.Area} \\end{align*} \\] 13599.16 + 5567.37*1 ## [1] 19166.53 Home without fireplace: \\[ \\begin{align*} \\hbox{Predicted Price} &amp;= 13599.16 + 5567.37 \\times 0 + 111.21 \\times \\hbox{Living.Area} \\\\ &amp;= \\$13,599.16 + \\$111.21 \\times \\hbox{Living.Area} \\end{align*} \\] If we keep Living.Area constant by considering two equally sized homes, then we’d expect the home with the fireplace to be worth $5567.37 more than a home without a fireplace. We see this by taking the difference between the two equations: \\[ \\hbox{Predicted Price (with Fireplace)} - \\hbox{Predicted Price (without Fireplace)} \\] \\[ \\begin{align*} \\,&amp;= (\\$19,166.53 + \\$111.21 \\times \\hbox{Living.Area}) - ( \\$13599.16 + \\$111.21 \\times \\hbox{Living.Area}) \\,&amp;= \\$19,166.53 - \\$13,599.16 = \\$5567.37 \\end{align*} \\] The difference between the intercepts is 5567.37. Note this was the estimated coefficient for AnyFireplaceTRUE. So the $5567.37 is the increase in the predicted or average Price associated with a 1 unit change in AnyFireplace (TRUE or FALSE), keeping all other variables (Living.Area) constant. Similarly, we could reason that $111.21 is the increase in the predicted or average Price associated with a 1 square footage increase in Living.Area, keeping all other variables (AnyFireplace) constant. Let’s look back at the relationship between Living.Area and Price and color the scatterplot by AnyFireplace. So we are now looking at three variables at a time. The above model with AnyFireplace and Living.Area results in two lines, with different intercepts but the same slope (parallel lines). homes %&gt;% ggplot(aes(x = Living.Area, y = Price, color = AnyFireplace)) + geom_point() + theme_minimal() Let’s try and fit two separate lines to these two groups of homes, home with any fireplaces and home with no fireplaces. Do these lines have the same intercepts? Same slopes? homes %&gt;% ggplot(aes(x = Living.Area, y = Price, color = AnyFireplace)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) + theme_minimal() In this case, it look as though having a fireplace in your house slightly changes the relationship between Living.Area and Price. In fact, having a fireplace in your house, the increase in your price for every 1 square foot is greater than that for homes without fireplaces (slopes are different). 3.9.2 Interaction Variables We can actually allow for different slopes within one regression model, rather than fitting two separate models. If we add a variable in the model as is, it changes the intercept. We can achieve different slopes by allowing a variable \\(x_1\\) to affect the slope for another variable \\(x_2\\). That is, \\(x_1\\) impacts the effect of \\(x_2\\) on the outcome \\(y\\). (Fireplace presence impacts the effect of living area on house price.) \\[b_2 = a + bx_1\\] This is called effect modification (when one variable can modify the effect of another variable on the outcome). A model with effect modification looks like: \\[\\hat{y} = b_0 + b_1x_{1} + b_2x_{2}= b_0 + b_1x_{1} + (a+bx_1)x_{2}= b_0 + b_1x_{1} +ax_2+bx_1x_{2}\\] The model above has an interaction term, which is the product of two variables. Here we have \\(x_1*x_2\\). Let’s build an effect modification model for our housing data. Let’s include an interaction term between AnyFireplace and Living.Area to allow for different slopes. lm.home3 &lt;- lm(Price ~ AnyFireplace*Living.Area, data = homes) tidy(lm.home3) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 40901. 8235. 4.97 7.47e- 7 ## 2 AnyFireplaceTRUE -37610. 11025. -3.41 6.61e- 4 ## 3 Living.Area 92.4 5.41 17.1 1.84e-60 ## 4 AnyFireplaceTRUE:Living.Area 26.9 6.46 4.16 3.38e- 5 What does this mean? Let’s think about two types of homes: a home with one or more fireplaces and a home without a fireplace. Home with fireplace: \\[ {\\small \\begin{align*} \\hbox{Predicted Price} &amp;= 40901.29 + -37610.41 \\times 1 + 92.36391 \\times \\hbox{Living.Area} + 26.85 \\times \\hbox{Living.Area} \\times 1 \\\\ &amp; = \\$3,290.88 + \\$119.21 \\times \\hbox{Living.Area} \\end{align*} } \\] 40901.29 + -37610.41*1 ## [1] 3290.88 92.36391 + 26.85*1 ## [1] 119.2139 Home without fireplace: \\[ {\\small \\begin{align*} \\hbox{Predicted Price} &amp;= 40901.29 + -37610.41 \\times 0 + 92.36391 \\times \\hbox{Living.Area} + 26.85 \\times \\hbox{Living.Area} \\times 0 \\\\ &amp;= \\$40,901.29 + \\$92.36 \\times \\hbox{Living.Area} \\end{align*} } \\] We see a different slope and different intercepts for these two groups. 3.9.3 Causation We alluded earlier that multiple linear regression could provide estimates of causal effects in the right circumstances. What are those circumstances? When we include all confounding variables. This entails being specific about what a confounding variable is. A confounder is a common cause of both the causal variable of interest and the outcome. (e.g. Living area could be a confounder of fireplace presence and house price.) We also alluded earlier that we should not just throw every variable we have into a multiple regression model. Why? Imagine a scenario for understanding how smoking affects lung cancer development. It is very important to consider whether a variable is a mediator of the relationship between the cause and the outcome. A mediator in this example could be tar. Suppose that smoking only affects lung cancer risk by creating tar on the lungs. If we adjust for tar (by holding it constant), then we also effectively hold smoking constant too! If smoking is held constant, then we cannot estimate its effect on cancer risk because it is not varying! Wait - we could never possibly know of or measure all confounding variables, could we!? This is true, but that doesn’t mean that our endeavor to understand causation is fruitless. As long as we can describe the relationship between known confounders as precisely as possible, we have a starting ground for moving forward. We collect data, analyze how well our model predicts that data, and collect more data based on that, perhaps measuring more potential confounders as our scientific knowledge grows. We can also conduct sensitivity analyses by asking: how strongly must a confounder affect the variable of causal interest and the outcome to completely negate or reverse the association we see? Such endeavors and more are the subject of the field of causal inference. Reminder: If you want a “gentle” but mathematical introduction to Causal Inference, I’d suggest reading “Causal Inference in Statistics: A Primer” by Judea Pearl, Madelyn Glymour, Nicholas P. Jewell). Fun Fact: Nicholas Jewell was Prof. Heggeseth’s PhD advisor! 3.9.4 Conditions for Multiple Linear Regression In order for a multiple linear regression model to make sense, Relationships between each quantitative \\(X\\)’s and \\(Y\\) are straight enough (check scatterplots and residual plot) About equal spread of residuals across fitted values (check residual plot) No extreme outliers (points far away in X’s can have leverage to change the line) 3.9.5 Is the Difference Real? We could ask: is there really a difference in the slopes for Living Area and Price between homes with and without a fireplace? lm.home4 &lt;- lm(Price ~ Living.Area*AnyFireplace, data = homes) summary(lm.home4) ## ## Call: ## lm(formula = Price ~ Living.Area * AnyFireplace, data = homes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -241710 -39588 -7821 28480 542055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 40901.294 8234.665 4.967 7.47e-07 *** ## Living.Area 92.364 5.412 17.066 &lt; 2e-16 *** ## AnyFireplaceTRUE -37610.413 11024.853 -3.411 0.000661 *** ## Living.Area:AnyFireplaceTRUE 26.852 6.459 4.157 3.38e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 68760 on 1724 degrees of freedom ## Multiple R-squared: 0.513, Adjusted R-squared: 0.5122 ## F-statistic: 605.4 on 3 and 1724 DF, p-value: &lt; 2.2e-16 If we ask ourselves this question, we are assuming a few things: We would like to make a general statement about a target population of interest. We don’t have data for everyone in our population (we don’t have a census). Depending on who ends up in our sample, the relationship/difference/estimate may change a bit. We want to know how much the relationship/difference/estimate may change based on sampling variation. Let’s treat our sample (of size \\(n\\)) as our ‘fake’ population (since we don’t have the full population). Randomly sample from our sample (with replacement) a new sample of size \\(n\\) Calculate the least squares regression line. Repeat. set.seed(333) ## Setting the seed ensures that our results are reproducible ## Repeat the sampling and regression modeling 1000 times boot &lt;- do(1000)*lm(Price ~ Living.Area*AnyFireplace, data = resample(homes)) ## Plot the distribution of the 1000 slope differences boot %&gt;% ggplot(aes(x = Living.Area.AnyFireplaceTRUE)) + geom_histogram() + xlab(&#39;Bootstrap Difference in Slopes&#39;) This is called Bootstrapping and it is used to: Measure the variability in the estimate (the estimate is the difference in slopes in this case) between random samples and Provide an interval of plausible values for the estimate (the estimate is the difference in slopes in this case). Let’s first look at the variability of the difference in slopes across the bootstrap samples. The standard deviation of the slopes will be similar to the std.error from the linear model output. boot %&gt;% summarize(sd(Living.Area.AnyFireplaceTRUE))#this is going to be of similar magnitude to the Std Error in output ## sd(Living.Area.AnyFireplaceTRUE) ## 1 9.340525 tidy(lm.home4) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 40901. 8235. 4.97 7.47e- 7 ## 2 Living.Area 92.4 5.41 17.1 1.84e-60 ## 3 AnyFireplaceTRUE -37610. 11025. -3.41 6.61e- 4 ## 4 Living.Area:AnyFireplaceTRUE 26.9 6.46 4.16 3.38e- 5 This standard deviation is somewhat close to the \\(6.459\\) in the Std. Error column of the summary(lm.home4) output above. To get an interval of plausible values, we look at the histogram and take the middle 95%. The lower end will be the 2.5th percentile and the upper end will be the 97.5th percentile. boot %&gt;% summarize(lower = quantile(Living.Area.AnyFireplaceTRUE, 0.025), upper = quantile(Living.Area.AnyFireplaceTRUE, 0.975)) ## lower upper ## 1 8.896268 44.89519 Based on this evidence, do you think it is possible that the slopes are the same for the two types of homes (with and without fireplaces)? 3.9.6 Dealing with Non-Linear Relationships If we notice a curved relationship between two quantitative variables, it doesn’t make sense to use a straight line to approximate the relationship. What can we do? 3.9.6.1 Transform Variables One solution to non-linear relationships is to transform the explanatory (X) variables or transform the outcome variable (Y). Hot Tip #1: If there is unequal spread around the curved relationship, focus first on transforming Y. If the spread is roughly the same around the curved relationship, focus on transforming X. When we say transform a variable, we are referring to taking the values of a variable and plugging them into a mathematical function such as \\(\\sqrt{x}\\), \\(\\log(x)\\) (which represents natural log, not log base 10), \\(x^2\\), \\(1/x\\), etc. We will focus on power functions and organize them in a Ladder of Powers of y (or x): \\[ \\vdots\\\\ y^3\\\\ y^2\\\\ \\mathbf{y = y^1}\\\\ \\sqrt{y}\\\\ y^{1/3}\\\\ y^{0} ~~~ (we~use~\\log(y)~here )\\\\ y^{-1/3}\\\\ 1/\\sqrt{y}\\\\ 1/y\\\\ 1/y^2\\\\ \\vdots \\] We start at \\(y\\) (power = 1) and think about going up or down the ladder. But which way? Our friend J.W. Tukey (the same guy who invented the boxplot) came up with an approach to help us decide. You must ask yourself: Which part of the circle does the scatterplot most resemble (concavity and direction)? Which quadrant? The sign of x and y in the quadrant tells you the direction to move on the ladder (positive = up, negative = down). Practice: Which quadrant does this relationship resemble? require(gapminder) gapminder %&gt;% filter(year &gt; 2005) %&gt;% ggplot(aes(y = lifeExp, x = gdpPercap)) + geom_point() Based on this plot, we see that the spread is roughly equal around the curved relationship and that it is concave down and positive (quadrant 2: top left). This suggests that we focus on going down the ladder with x. Try these transformations until you find a relationship that is roughly straight. If you go too far, the relationship will become more curved. Let’s try going down the ladder. gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = sqrt(gdpPercap)) %&gt;% ggplot(aes(y = lifeExp, x = TgdpPercap)) + geom_point() Not quite straight. Let’s keep going. gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = gdpPercap^(1/3)) %&gt;% ggplot(aes(y = lifeExp, x = TgdpPercap)) + geom_point() Not quite straight. Let’s keep going. gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = log(gdpPercap)) %&gt;% ggplot(aes(y = lifeExp, x = TgdpPercap)) + geom_point() Getting better. Let’s try to keep going. gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = -1/gdpPercap) %&gt;% ggplot(aes(y = lifeExp, x = TgdpPercap)) + geom_point() TOO FAR! Back up. Now we see some unequal spread so let’s also try transforming Y. gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = log(gdpPercap)) %&gt;% mutate(TlifeExp = lifeExp^2) %&gt;% ggplot(aes(y = TlifeExp, x = TgdpPercap)) + geom_point() That doesn’t change it much. Maybe this is as good as we are going to get. Transformations can’t make relationships look exactly linear with equal spread, but sometimes we can make it closer to that ideal. Let’s try and fit a model with just these two variables. lm.gap &lt;- gapminder %&gt;% filter(year &gt; 2005) %&gt;% mutate(TgdpPercap = log(gdpPercap)) %&gt;% lm(lifeExp ~ TgdpPercap, data = .) summary(lm.gap) ## ## Call: ## lm(formula = lifeExp ~ TgdpPercap, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.947 -2.661 1.215 4.469 13.115 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9496 3.8577 1.283 0.202 ## TgdpPercap 7.2028 0.4423 16.283 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.122 on 140 degrees of freedom ## Multiple R-squared: 0.6544, Adjusted R-squared: 0.652 ## F-statistic: 265.2 on 1 and 140 DF, p-value: &lt; 2.2e-16 What does \\(b_1\\) mean in this context? \\[\\widehat{LifeExp} = b_0 + b_1 log(Income)\\] The slope is the the additive increase in \\(\\widehat{LifeExp}\\) when \\(log(Income)\\) increases to \\(log(Income) + 1\\). Let’s think about \\(log(Income) + 1\\). Using some rules of logarithms: \\[log(Income) + 1 = log(Income) + log(e^1) = log(e*Income) = log(2.71*Income)\\] So adding 1 to \\(log(Income)\\) is equivalent to multiplying Income by 2.71. In our model, we note that if GDP is increased by 271% (multiplying by 2.71) the predicted average life expectancy of a country increases by about 7.2 years. For the sake of illustration, imagine we fit a model where we had transformed life expectancy. What does \\(b_1\\) mean in this context? \\[\\widehat{log(LifeExp)} = b_0 + b_1 Income\\] The slope is the the additive increase in \\(\\widehat{log(LifeExp)}\\) when \\(Income\\) increases to \\(Income + 1\\). Let’s think about \\(\\widehat{log(LifeExp)} + b_1\\). Using rules of logarithms: \\[\\widehat{log(LifeExp)} + b_1 = \\widehat{log(LifeExp)} + log(e^{b_1}) = log(\\widehat{LifeExp} * e^{b_1}) \\] The additive increase in \\(log(LifeExp)\\) is a multiplicative increase of \\(LifeExp\\) by a factor of \\(e^{b_1}\\). 3.9.7 Alternative Solutions We could also model non-linear relationships by including higher degree terms in a linear model like the example below. By using poly(), we now include \\(x\\) and \\(x^2\\) as variables in the model. x &lt;- rnorm(100, 5, 1) y &lt;- 200 + 20*x - 5*x^2 + rnorm(100,sd = 10) dat &lt;- data.frame(x,y) dat %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth() lm(y ~ poly(x, degree = 2, raw = TRUE), data = dat) ## ## Call: ## lm(formula = y ~ poly(x, degree = 2, raw = TRUE), data = dat) ## ## Coefficients: ## (Intercept) poly(x, degree = 2, raw = TRUE)1 ## 190.65 25.91 ## poly(x, degree = 2, raw = TRUE)2 ## -5.75 A more advanced solution (which is not going to be covered in class) is a generalized additive model, which allows you to specify which variables have non-linear relationships with y and estimates that relationship for you using spline functions (super cool stuff!). We won’t talk about how this model is fit or how to interpret the output, but there are other cool solutions out there! require(gam) plot(gam(y ~ s(x), data = dat)) "],
["logistic-regression.html", "3.10 Logistic Regression", " 3.10 Logistic Regression If you are interested in predicting a binary categorical variable (only 2 possible outcomes), the standard linear regression models don’t apply. If you let the two outcomes be 0 and 1, you’ll never get a straight line relationship with an x variable. Throughout this section, we will refer to one outcome as ‘success’ (denoted 1) and ‘failure’ (denoted 0). Depending on the context of the data, the success could be a negative thing such as ‘heart attack’ or ‘20 year mortality’. We will let \\(\\pi\\) be the chance of success and \\(1-\\pi\\) be the chance of failure. We want to build a model to explain why the chance of success may be higher for one group of people in comparison to another. 3.10.1 Logistic and Logit The logistic function is an S shaped curve (sigmoid curve). For our purposes, the function will take the form \\[f(x) = \\frac{1}{1 + e^{b_0 +b_1x}}\\] For any real value x, f(x) will be a value between 0 and 1. This is perfect for us since probabilities/chances should also be between 0 and 1. In fact, we’ll let the chance of failure, \\(1-\\pi\\), be modeled by this function. \\[1-\\pi = \\frac{1}{1 + e^{b_0 +b_1x}}\\] With a bit of algebra and rearranging terms, we can write this in terms of \\(\\pi\\), the chance of success. \\[\\pi = \\frac{e^{b_0 +b_1x}}{1 + e^{b_0 +b_1x}}\\] Let’s define one more term. The odds of success is the ratio of the probability of success to the probability of failure, \\(\\pi/(1-\\pi)\\). With a bit more algebra and rearranging terms, we can write the above model in terms of a regression model, \\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = b_0 +b_1x\\] On the left hand side, we have the natural log of the odds, called the logit function. On the right hand side, we have an equation for a line. This is a simple logistic regression model. Just like a linear regression model, we can extend this model to a multiple logistic regression model by adding additional x variables, \\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = b_0 +b_1x_1+b_2x_2+b_3x_3+\\cdots +b_kx_k\\] 3.10.2 Fitting the Model Based on observed data that includes responses (1 for success, 0 for failure) and predictor variables, we need to find the slope coefficients, \\(b_0\\),…,\\(b_k\\) that best fits the data. The way we do this is through a technique called maximum likelihood estimation. We will not discuss the details in this class; we’ll save this for an upper level stats class. In R, we do this with a general linear model function, glm(). For a data set, let’s go back in history to January 28, 1986. On this day, the Challenger U.S. space shuttle took off and exploded about minute after the launch. After the fact, scientists ruled that the disaster was due to o-ring seal failure. Let’s look at experimental data on the o-rings prior to the fateful day. require(vcd) data(SpaceShuttle) SpaceShuttle %&gt;% filter(!is.na(Fail)) %&gt;% ggplot(aes(x = factor(Fail), y = Temperature) )+ geom_boxplot() + theme_minimal() SpaceShuttle %&gt;% filter(!is.na(Fail)) %&gt;% group_by(Temperature) %&gt;% summarise(PercentFail = mean(Fail == &#39;yes&#39;)) %&gt;% ggplot(aes(x = Temperature, y = PercentFail)) + geom_point() + theme_minimal() What is the plot above telling us about the relationship between chance of o-ring failure and temperature? Let’s fit a simple logistic regression model to predict the chance of o-ring failure based on the temperature using the experimental data. model.glm &lt;- glm(Fail ~ Temperature, data = SpaceShuttle, family = binomial) summary(model.glm) ## ## Call: ## glm(formula = Fail ~ Temperature, family = binomial, data = SpaceShuttle) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0611 -0.7613 -0.3783 0.4524 2.2175 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.0429 7.3786 2.039 0.0415 * ## Temperature -0.2322 0.1082 -2.145 0.0320 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 28.267 on 22 degrees of freedom ## Residual deviance: 20.315 on 21 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 24.315 ## ## Number of Fisher Scoring iterations: 5 3.10.3 Interpretation Let’s take a look at these estimates from the model. What do they mean? tidy(model.glm) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 15.0 7.38 2.04 0.0415 ## 2 Temperature -0.232 0.108 -2.14 0.0320 If you want to get a sense of what the number -0.232 means, we need to do a bit of algebra. Our estimated model is \\[\\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = b_0 +b_1x\\] where \\(b_0 = 15\\) and \\(b_1 = -0.232\\). If we imagine increasing x by 1, then we get a different set of predicted probabilities of success, \\(\\hat{\\pi}^*\\), \\[\\log\\left(\\frac{\\hat{\\pi}^*}{1-\\hat{\\pi}^*}\\right) = b_0 +b_1(x+1)\\] Let’s find the difference between these two equations, \\[\\log\\left(\\frac{\\hat{\\pi}^*}{1-\\hat{\\pi}^*}\\right) - \\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = b_0 +b_1(x+1) - (b_0 +b_1x)\\] and simplify the right hand side, \\[\\log\\left(\\frac{\\hat{\\pi}^*}{1-\\hat{\\pi}^*}\\right) - \\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = b_1\\] and then simplify the left hand side, \\[\\log\\left( \\frac{\\hat{\\pi}^*/(1-\\hat{\\pi}^*)}{\\hat{\\pi}/(1-\\hat{\\pi})}\\right) = b_1\\] Let’s exponentiate both sides, \\[\\left( \\frac{\\hat{\\pi}^*/(1-\\hat{\\pi}^*)}{\\hat{\\pi}/(1-\\hat{\\pi})}\\right) = e^{b_1}\\] We find that \\(e^{b_1} = e^{-0.232} = 0.793\\) is the odds ratio based on increasing x by 1 unit (it is a ratio of odds). If a ratio is greater than 1, that means that the denominator is less than the numerator or numerator is greater than denominator. If a ratio is less than 1, that means that the denominator is greater than the numerator or numerator is less than denominator. If the ratio is equal to one, the numerator equals the denominator (odds are equal). In this case, we have a ratio &lt; 1 which means that the estimated odds of o-ring failure is lower for increased temperatures (in particular by increasing by 1 degree). This makes sense since we saw that the chance of o-ring failure decrease with warmer temperatures. 3.10.4 Prediction On January 28, 1986, the temperature was 26 F degrees. Let’s predict the chance of “success,” which is a failure of o-rings in our data context, at that temperature. predict(model.glm, newdata = data.frame(Temperature = 26), type = &#39;response&#39;) ## 1 ## 0.9998774 They didn’t have any experimental data testing o-rings at this low of temperatures, but even based on the data collected, they predict the chance of failure to be nearly 100% (near certainty). "],
["major-takeaways-2.html", "3.11 Major Takeaways", " 3.11 Major Takeaways All models are wrong, but some are useful and fair. We want a model with small residuals (prediction errors). To determine if a model is useful and fair, we study what is left over (the residuals). We use models to describe phenomena by interpreting slope estimates. Make sure you are talking about the average or predicted outcome! If you have multiple variables, you keep all others fixed (if possible). We also use models to prediction values, but be careful about predicting outside the observed range of our explanatory (X) variables. That is called extrapolation. "],
["random-variability.html", "Chapter 4 Random Variability", " Chapter 4 Random Variability Up until this point, we have thought about Data collection process (sampling and study design) and Data Quality (issues of bias) Data visualization (the first step of any data analysis) Modeling (to explain observed variation) Throughout the past three chapters, we have also sprinkled in the idea that the sample we observe is one random representation of the true population or phenomenon. Therefore, any numerical summary of the sample, any statistic, would be an estimate the true numerical summary of the population, the corresponding unknown parameter. If we could repeat the random sampling process, each sample we would get would be slightly different. The individual composition would differ every time. Sometimes we would randomly over represent one group people and another time we would randomly over represent another group of people, just by chance. Our statistic that we calculate would change slightly for every sample. If we could repeat the randomization process in an experiment, each treatment would be slightly different. The individual composition would differ every time. Sometimes we would randomly over represent one group people and another time we would randomly over represent another group of people, just by chance. Our statistic that we calculate and compare between groups would change slightly for every reshuffling of individuals. This is the idea of random variability due to random sampling or randomization, typically. The sample composition varies and therefore, the statistics varies. Let’s explore this concept a bit more before we formally talk about probability and chances. "],
["random-sampling-from-a-population.html", "4.1 Random Sampling from a Population", " 4.1 Random Sampling from a Population The data set we will work with contains ALL flights leaving New York City in 2013. This data represents a full census of the target population of flights leaving NYC in a particular year. We’ll start by creating two new variables, season defined as winter (Oct - March) or summer (April - Sept) and day_hour defined as morning (midnight to noon) or afternoon (noon to midnight). Since we have the full population, we could just describe the flights that happened. Instead, we are going to use this population to help us think about sampling variability. Let’s take one random sample of 100 flights from the data set, using a simple random sample strategy. Let’s look at the arrival delay (in minutes) with a histogram and calculate the median and mean arrival delay. ## # A tibble: 1 x 2 ## medians means ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.5 4.54 Now, let’s take another random sample of 100 flights from the full population of flights. ## # A tibble: 1 x 2 ## medians means ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -9.5 -0.02 We could keep the process going–take a sample of 100 flights, look at the histogram and calculate the median and mean. Repeat many, many times. We can add a little bit of code to help us simulate this sampling 1000 times and calculate the median and mean for each sample of 100 flights. Now we have 1000 medians and 1000 means, each corresponding to a sample median and sample mean of 100 random flights from the population. Let’s summarize and visualize this simulation. These histograms estimate the sampling distribution of sample median arrival delay and the sampling distribution of sample mean arrival delay, both of which describe the variability in the sample statistic across all possible random samples from the population. 4.1.1 IRL: Bootstrapping In real life (IRL), we don’t have a population to repeated draw from. We only have a sample that was already drawn from the larger target population. To get a sense of the sampling variability, we could try to mimic this process using our best stand-in for the population, our sample. We will call the sample our “fake population” for the moment. 4.1.1.1 Generate To generate the different random samples of the same size (100 flights) from our “fake population”, we have to draw sample of 100 flights WITH REPLACEMENT, meaning that we have to put a flight back into the pool after drawing them out. 4.1.2 Calculate In our simulation above, we calculated the median and mean arrival delay. In theory, we could calculate any numerical summary of data, be it the mean, median, sd, 25th percentile, etc. 4.1.3 Summarize Let’s summarize these 1000 medians and 1000 means generated from resampling (with replacement) from our sample (our “fake population”). ## # A tibble: 1 x 4 ## mean_medians mean_means sd_medians sd_means ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.774 4.82 2.98 2.95 Let’s compare this to the summaries from the simulation from the population. ## # A tibble: 1 x 4 ## mean_medians mean_means sd_medians sd_means ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -4.76 6.91 2.60 4.43 They won’t be exactly the same, but they should be of roughly similar magnitude. 4.1.4 Visualize Let’s visualize these 1000 medians and 1000 means generated from resampling (with replacement) from our sample (our “fake population”). Let’s compare this to the visuals from the simulation from the population. The process of resampling from our sample is called bootstrapping and it becoming the one of main computational tools for estimating sampling variability in Statistics. This is a really important concept in Statistics! We’ll return to the ideas of sampling variability and bootstrapping throughout the rest of the course. "],
["randomization-into-groups.html", "4.2 Randomization into Groups", " 4.2 Randomization into Groups Now, we have been thinking about arrival delays in general. If you were planning a trip, you may be able to choose between two flights that leave at different times of day. Do morning flights have shorter arrival delays on average than afternoon flights? If so, book the early flight! Let’s look at the data! We use a random sample of 500 flights from the population to investigate this question. Let’s summarize and visualize the relationship between hour of the day (morning or afternoon) and the arrival delay. ## # A tibble: 2 x 3 ## day_hour median mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afternoon 2 14.8 ## 2 morning -6 5.21 We don’t know the exact reason why some flights were scheduled in the morning or the afternoon and why one flight might be delayed (it is probably due to complex combination of things). If there were no difference between morning and afternoon flights, then the it wouldn’t matter whether the flight was in the morning or afternoon. The day_hour variable would be a irrelevant to the arrival delay. If that were true, then we could reshuffle the values of day_hour and it wouldn’t change our conclusions. Wouldn’t it be great if we could see how the mean arrival delays might change if we shuffled the flights around from the “morning” group to “afternoon” group, randomly. In fact, wouldn’t it be great if we could look at every permutation of flights between two groups. 4.2.1 IRL: Randomization Tests In real life, we don’t often consider every possible permutation (reshuffling of group members) due to the magnitude of permutations. However, we can randomly reshuffle flights about 1000 times to try to approximate many permutations. 4.2.1.1 Hypothesis Our null hypothesis (a hypothesis that is convservative) is that there is no relationship between time of day and the arrival delay. 4.2.1.2 Generate We can generate 1000 new data sets based on randomly reshuffling the labels of day_hour. For each of these data sets, we calculate the difference in mean arrival delay between morning and afternoon groups. 4.2.1.3 Visualize The histogram below shows the histogram of differences in means if the null hypothesis were true. The vertical line shows the observed difference in means. "]
]
