```{r setup3, include=FALSE}
library(broom)
library(dplyr)
library(ggplot2)
library(stringr)
library(rvest)
library(mosaicData) 
library(ggmosaic)
library(NHANES)
library(mosaic)
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
```

# Regression Models

In our visualization of data, we've seen trends, patterns, and variation. Let's now endeavor to describe those trends, patterns, and variation more *precisely* and *quantitatively* by building statistical *models*.

## Modeling Goals

Broadly, a model is a simplified representation of the world. When we build models, we may have different goals.

One goal when building models is **prediction**. Given data on a **response or outcome variable**, $y$, and one or more **predictor or explanatory variables**, $x$, the goal is to find a mathematical function, $f$, of $x$ that gives good predictions of $y$.  For example, we might want to be able to predict a customer's chest size knowing their neck size. This $x$ may be a single variable, but it is most often a set of variables. We'll be building up to multivariate modeling over the course of this chapter. 

```{block, type="reflect"}
Can you think of some other concrete examples in which we'd want a model to do prediction? Consider what predictions might be made about you every day...
```

What are the qualities of a good model and function, $f$? We want to find an $f(x)$ such that $\hat{y} = f(x)$ is a good predictor of $y$. In other words, we want the model prediction $\hat{y}$ to be close to the observed response. We want $y-\hat{y}$ to be small. This difference between the observed value and the prediction, $y-\hat{y}$, is called a **residual**. We'll discuss residuals more later.

Another goal when building models is **description**. We want a model, $f(x)$, to "explain" the relationship between the $x$ and $y$ variables. Note that an overly complicated model may not be that useful here because it can't help us *understand* the relationship. A more complex model may, however, produce better predictions. [George Box](https://en.wikipedia.org/wiki/George_E._P._Box) is often quoted "All models are wrong but some are useful." Depending on our goal, one model may be more useful than another.

```{block, type="reflect"}
Can you think of some concrete examples in which we'd want a model to do explain a phenomenon? Consider how policy decisions get made...
```

To begin, we will consider a simple, but powerful model in which we limit this function, $f(x)$, to be a straight line with a y-intercept, $b_0$, and slope, $b_1$.

$$\hat{y} = f(x) = b_0 + b_1x$$

This is a **simple linear regression model**. It is the foundation of many statistical models used in modern statistics and is more flexible than you may think. 

```{block, type='math'}
In the past, you may have seen the equation of a line as 

$$y = mx + b$$

where $m$ is the slope and $b$ is the y-intercept. We will be using different notation so that it can generalize to multiple linear regression. 

The y-intercept is the value when $x=0$ and the slope is change in $y$ for each 1 unit increase of $x$ ("rise over run"). 
```


## Lines

Let's return to the thought experiment in which you were a manufacturer of button-down dress shirts.

```{r echo=TRUE}
body <- read.delim("http://sites.williams.edu/rdeveaux/files/2014/09/bodyfat.txt")

body %>%
    ggplot(aes(x = Neck, y = Chest)) +
    geom_point(color = 'steelblue') + 
    xlab('Neck size (cm)') + 
    ylab('Chest size (cm)') +
    theme_minimal()
```

```{block type='reflect'}
If you were to add one or multiple lines to the plot above to help you make business decisions, where would you want it (or them)?
```

Let's say you were only going to make one size of shirt. You might want to add a horizontal line at the mean Chest size and a vertical line at the mean Neck size. 

```{r echo=TRUE}
body %>%
    ggplot(aes(x = Neck, y = Chest)) +
    geom_point(color = 'steelblue') + 
    geom_hline(yintercept = mean(body$Chest)) +
    geom_vline(xintercept = mean(body$Neck)) +
    xlab('Neck size (cm)') + 
    ylab('Chest size (cm)') +
    theme_minimal()
```

We can see that a shirt made to these specifications would fit the "average person." However, this might not serve your market very well. For many people, the shirt would be too tight because their chest and/or neck sizes would be larger than average. For many people, the shirt would be too large because they chest and/or neck sizes would be smaller than average. 

Let's try something else. Let's allow ourselves 5 different sizes (XS, S, M, L, XL). Then, we can cut the Neck sizes variable into 5 groups of equal length and estimate the mean Chest sizes within each of these groups. 

```{r echo=FALSE}
bfmeans <- body %>%
  mutate(NeckGroups = cut(Neck, 5)) %>%
  group_by(NeckGroups) %>%
  summarize(Means = mean(Chest))

labs <- levels(bfmeans$NeckGroups)
bfmeans <- cbind(bfmeans, lower = as.numeric( sub("\\((.+),.*", "\\1", labs) ),
      upper = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) ))

body %>%
  ggplot(aes(x = Neck, y = Chest)) +
  geom_point(color = 'steelblue') + 
  geom_segment(aes(x = lower, xend = upper, y = Means, yend = Means),bfmeans)+
  geom_vline(aes(xintercept = upper), data = bfmeans) +
  geom_vline(aes(xintercept = lower), data = bfmeans) +
  xlab('Neck size (cm)') + 
  ylab('Chest size (cm)') +
  theme_minimal()
```

```{block, type="reflect"}
What do these lines tell us for our business venture?
```

What if we wanted to be able to make more sizes? Could we get a pretty good sense of what the chest sizes should be for a given neck size? Let's try allowing for 8 different sizes. 

```{r echo=FALSE}
bfmeans <- body %>%
  mutate(NeckGroups = cut(Neck, 8)) %>%
  group_by(NeckGroups) %>%
  summarize(Means = mean(Chest))

labs <- levels(bfmeans$NeckGroups)
bfmeans <- cbind(bfmeans, lower = as.numeric( sub("\\((.+),.*", "\\1", labs) ),
      upper = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) ))

body %>%
  ggplot(aes(x = Neck, y = Chest)) +
  geom_point(color = 'steelblue') + 
  geom_segment(aes(x = lower, xend = upper, y = Means, yend = Means),bfmeans)+
  geom_vline(aes(xintercept = upper), data = bfmeans) +
  geom_vline(aes(xintercept = lower), data = bfmeans) +
  xlab('Neck size (cm)') + 
  ylab('Chest size (cm)') +
  theme_minimal()
```

```{block, type="reflect"}
What are the pros and cons of having a larger number of sizes?
```

```{block, type="reflect"}
Stop and think about the data collection process. If you were measuring your own neck size, how precise do you think you could get? What factors might impact that precision?
```


We notice that there is a generally linear relationship between neck and chest size. Perhaps we find one line to describe the relationship between Neck size and Chest size and use that to decide on sizes later?

```{r echo=FALSE}
body %>%
  ggplot(aes(x = Neck, y = Chest)) +
  geom_point(color = 'steelblue') + 
  geom_smooth(method = 'lm', se = FALSE) +
  geom_segment(aes(x = lower, xend = upper, y = Means, yend = Means),bfmeans)+
  geom_vline(aes(xintercept = upper), data = bfmeans) +
  geom_vline(aes(xintercept = lower), data = bfmeans) +
  xlab('Neck size (cm)') + 
  ylab('Chest size (cm)') +
  theme_minimal()
```

```{block, type='reflect'}
What does line tell us for our business venture?
```

If the scatterplot between two quantitative variables **resembles a straight line**,

- a straight line could roughly **describe** the mean of $y$ for each value of $x$.
- a straight line could **describe** how much we'd *expect* $y$ to change based on a 1 unit change in $x$.
- a straight line could help us **predict** the $y$ based on a new value of $x$.


## "Best" fitting line

To choose the "best" fitting line, we need to choose the intercept ($b_0$) and slope ($b_1$),

$$ \hat{y} = f(x) = b_0 + b_1x $$

that gives us the "best" fit to the $n$ points on a scatterplot, $(x_i,y_i)$ where $i=1,...,n$. 

```{block, type='reflect'}
What do we mean by "best"? In general, we'd like good predictions and a model that describes the average relationship. But we need to be more precise about what we mean by "best". 
```

### First idea

One idea of "best" is that we want the line that minimizes the sum of the **residuals**, $e_i = y_i - \hat{y}_i = y_i - ( b_0 + b_1x_i)$. The residual is the error in our prediction, the difference between what you observe and what you predict based on the line.

- **Problem**: We will have positive and negative residuals; they will cancel each other out if we add them together. While a good idea, this definition of "best" won't give us what we want. We'll want an idea that deals with the negative signs. 


### Second idea

Another idea of "best" is that we want the line that minimizes the sum of the absolute value of the residuals, $\sum_{i=1}^n |y_i - \hat{y}_i| = \sum_{i=1}^n |e_i|$.

- **Problem**: This definition of "best" results in a procedure referred to as **Least Absolute Deviations**, but there isn't always one unique line that satisfies this. So, while this is a valid definition of "best," this isn't stable as there isn't always one "best" line. 

### Third idea

Lastly, another idea of "best" is that we want the line that minimizes the sum of squared residuals, $\sum_{i=1}^n (y_i - \hat{y}_i)^2= \sum_{i=1}^n e_i^2$.

- This is referred to as **Least Squares** and has a unique solution. We'll will focus on this definition of "best" in this class. It also has some really nice mathematical properties and [connections to linear algebra](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b). 

## Least Squares

Let's try to find the line that **minimizes the Sum of Squared Residuals** by searching over a grid of values for (intercept, slope).

Below is a visual of the sum of squared residuals for a variety of values of the intercept and slope. The surface height is sum of squared residuals for each combination of slope and intercept.

```{r echo=FALSE}
f <- function(b){
  sum((body$Chest - (b[1] + b[2] * body$Neck))^2)
} 
b0 = seq(-20,20,by=.5)
b1 = seq(-10,20,by=1)
b <- expand.grid(b0,b1)
ss <- apply(b,1,f)

persp(b0,b1,matrix(ss,length(b0),length(b1)),theta=65,zlab='Sum of Squares')
```

We can see there is valley where the minimum must be. Let's visualize this in a slightly different way. We'll encode the surface height as color (white is lowest).

```{r echo=FALSE}
image(b0,b1,matrix(ss,length(b0),length(b1)),col=c('white',topo.colors(100)[100:1]))
```

The large values of the sum of squared residuals are dominating this image, so let's change the color scheme to see more variation in smaller values (white is lowest).

```{r echo=FALSE}
b0 = seq(-10,10,by=.1)
b1 = seq(0,5,by=.1)
b <- expand.grid(b0,b1)
ss <- apply(b,1,f)
image(b0,b1,matrix(log(ss),length(b0),length(b1)),col=c('white',topo.colors(100)[100:1])) #Log Sum of Squares to see variation in lower end
```

We can limit our search to $b_0 \in (-10,10)$ and $b_1 \in (2,3)$.

```{r echo=TRUE}
b0 = seq(-10,10,by=.05)
b1 = seq(2,3,by=.05)
b <- expand.grid(b0,b1)
ss <- apply(b,1,f)

b[ss == min(ss),]
```

We have the minimum point. Over the grid of pairs of values, the minimum sum of squared residuals happens when the intercept is -3.7 and the slope is 2.75.

```{block, type="math"}
(Optional) Alternative ways (faster than exhaustive search) to find the minimum sum of squared residuals

- We could try a numerical optimization algorithm such as steepest descent.
- We could use multivariable calculus (find partial derivatives, set equal to 0, and solve).

To get started on the calculus, solve the following two equations for the two unknowns ($b_0$ and $b_1$):

$$\frac{\partial }{\partial b_0}\sum_{i=1}^n (y_i - (b_0 + b_1x_i))^2 = 0$$
$$\frac{\partial }{\partial b_1}\sum_{i=1}^n (y_i - (b_0 + b_1x_i))^2 = 0$$

If you are a math/stat/physics/cs major, you should try this by hand and see if you can get the solutions below. 
```

If you find the minimum using calculus (super useful class!), you'll find that we can write the Least Squares solution in an equation format as functions of summary statistics (!), the estimated slope is

$$ b_1  = r\frac{s_y}{s_x}$$

and the estimated intercept is

$$ b_0 = \bar{y} - b_1\bar{x} $$

where $\bar{x}$ is the mean of the variable on the x-axis, $\bar{y}$ is the mean of the variable on the y-axis, $s_x$ is the standard deviation of the variable on the x-axis, $s_y$ is the standard deviation of the variable on the y-axis, and $r$ is the correlation coefficient between the two variables.

Let's do that calculation "by hand" first in R.

```{r echo=TRUE}
body %>%
  summarize(sy = sd(Chest), sx = sd(Neck), r = cor(Chest,Neck), ybar = mean(Chest), xbar = mean(Neck)) %>%
  mutate(b1 = r*sy/sx, b0 = ybar - b1*xbar) %>%
  select(b0, b1)
```

Wow. That was quite a bit of coding. From now on, we'll take the shortcut and use the `lm()` function which stands for **l**inear **m**odel. This function gives us the Least Squares solution to the "best" fitting line, as defined by minimizing the sum of squared residuals.

```{r echo=TRUE}
lm(Chest ~ Neck, data = body) # When you see ~, think 'as a function of'
```


## Properties of Least Squares Line

- $(\bar{x},\bar{y})$ is ALWAYS on the least squares line. 

- The residuals from the least squares line ALWAYS sum to 0.

- The mean of the residuals from the least squares line is ALWAYS 0.

- The **standard deviation of the residuals** gives us a sense of how bad our predictions (based on the line) could be. 

$$s_e = \sqrt{\frac{\sum^n_{i=1} (y_i-\hat{y}_i)^2}{n-2}}  = \sqrt{\frac{\sum^n_{i=1} (e_i-0)^2}{n-2}} $$

In R: the standard deviation of the residuals, $s_e$, is referred to as the "residual standard error". Don't confuse this with "Std. Error," that is something else that we'll get to. 

```{r echo=TRUE}
body %>%
  lm(Chest ~ Neck, data = .) %>% #If you don't want the pipe to pass to the first argument, you can specify the argument with .
  summary()
```

This simple linear regression line is

$$\widehat{Chest} = -3.18 + 2.73*Neck$$

If you have a neck size of 38 cm, then we predict the chest size of an ideal shirt is about 100.5 cm (~ -3.18 + 2.73*38) 

```{r}
-3.18 + 2.73*38 #the intercept and slope are rounded here first (not great)

body %>%
  lm(Chest ~ Neck, data = .) %>% 
  predict(newdata = data.frame(Neck = 38)) #the intercept and slope are not rounded before prediction (better!)
```


Given your neck size, we can probably predict your chest size within 5 to 10 cm since $s_e = 5.22$ (1 to 2 SD's -- recall Section \@ref(intro-zscore)). 

```{block, type="reflect"}
If you were a shirt manufacturer, is this a good enough prediction? What is the impact on the customer? Think of if the prediction were an overestimate (loose) or an underestimate (too tight).
```

### Real companies

Let's see how some real companies create shirts. In the plots below, the red boxes represent the advertised range (in cm) for Neck and Chest sizes for each brand.

![](Photos/stylish-dress-shirt-size-size-chart-.jpg)


```{r echo=FALSE}
with(body,plot(Chest~Neck,main='Calvin Klein'))
abline(with(body,lm(Chest~Neck)))
rect(14*2.54,36*2.54,14.5*2.54,37*2.54,col='red')
rect(15*2.54,38*2.54,15.5*2.54,39*2.54,col='red')
rect(16*2.54,40*2.54,16.5*2.54,41*2.54,col='red')
rect(17*2.54,42*2.54,17.5*2.54,43*2.54,col='red')
```

For Calvin Klein, we see that the red boxes are below the least squares line (black line). So for a given neck size, Calvin Klein makes shirts that are a little bit too small at the chest.


```{r echo=FALSE}
with(body,plot(Chest~Neck,main = 'Express'))
abline(with(body,lm(Chest~Neck)))
rect(13*2.54,33*2.54,13.5*2.54,33.5*2.54,col='red')
rect(14*2.54,36.5*2.54,14.5*2.54,38.5*2.54,col='red')
rect(15*2.54,39*2.54,15.5*2.54,41.5*2.54,col='red')
rect(16*2.54,42*2.54,16.5*2.54,44.5*2.54,col='red')
rect(17*2.54,45*2.54,17.5*2.54,47.5*2.54,col='red')
rect(18*2.54,48*2.54,18.5*2.54,50.5*2.54,col='red')
```

For Express, we see that the red boxes are generally on the least squares line, except for the smallest size. This means that Express shirts are generally a good fit at the chest and neck for the 4 largest sizes, but the smallest shirt size is a bit too small at the chest for the neck size.


```{r echo=FALSE}
with(body,plot(Chest~Neck,main='Brooks Brothers'))
abline(with(body,lm(Chest~Neck)))
rect(13*2.54,32*2.54,13.5*2.54,34*2.54,col='red')
rect(14*2.54,34*2.54,14.5*2.54,36*2.54,col='red')
rect(15*2.54,38*2.54,15.5*2.54,40*2.54,col='red')
rect(16*2.54,42*2.54,16.5*2.54,44*2.54,col='red')
rect(17*2.54,46*2.54,17.5*2.54,48*2.54,col='red')
```

For Brooks Brothers, the red boxes are a bit below the least squares line for the 3 smallest sizes and a little above the line for the largest size. This means that the 3 smallest sizes are a bit too small in the chest for our customers (in our data set) with those neck sizes and that the largest shirt is a bit big at the chest for that neck size.

```{block, type="reflect"}
We haven't told you how the customer data we've been using was collected. As you compared the brands to this sample data, what assumptions were you making about the population that the sample was drawn from? 

What questions do you have about the sampling procedure?
```

## Interpretation

Let's look at the summary print out of the `lm()` function in R again. We'll highlight some of the most important pieces of the print out and discuss how you interpret them.

```{r echo=TRUE}
body %>%
  lm(Chest ~ Neck, data = .) %>% 
  summary()
```

**Intercept** ($b_0$)

- **Where to find:** In the table called "Coefficients", look for the number under "Estimate" for "(Intercept)". It is -3.1885 for this model. 
- **Definition:** The intercept gives the average value of $y$ when $x$ is zero (**think about context**)
- **Interpretation:** If Neck size = 0, then the person doesn't exist. In this context, the intercept doesn't make much sense to interpret. 
- **Discussion:** If the intercept doesn't make sense in the context, we might refit the model with the $x$ variable once it is **centered**. That is, the mean of $x$ for all individuals in the sample is computed, and this mean is subtracted from each person's $x$ value. In this case, the intercept is interpreted as the average value of $y$ when $x$ is at its (sample) mean value. See the example below -- we get an intercept of 100.66, which is the average Chest size for customers with average Neck size.

```{r echo=TRUE}
body %>%
  mutate(CNeck = Neck - mean(Neck)) %>%
  lm(Chest ~ CNeck, data = .) %>% 
  summary()
```



**Slope** ($b_1$)

- **Where to find:** In the table called "Coefficients", look for the number under "Estimate" for the namve of the variable, "Neck" or "CNeck". It is 2.7369 for this model. 
- **Definition:** The slope gives the change in average $y$ per 1 unit increase in $x$ (**not for individuals and not causal statement**)
- **Interpretation:** If we compare two groups of individuals with a 1 cm difference in the neck size (e.g. 38 cm and 39 cm OR 40 cm and 41 cm), then we'd expect the average chest sizes to be different by about 2.7cm.
- **Another Interpretation:** We'd expect the average chest size to increase by about 2.7 cm for each centimeter increase in neck size.
- **Discussion:** Note that both interpretations are not written about an individual because we can't easily change our neck size by 1 cm. The slope describes the change in the average in our sample data set, not the change for one person. 

**Least Squares/Regression Line** ($\hat{y} = b_0 + b_1x$)

- **Where to find:** In the table called "Coefficients", all of the values you need to write down the line are under "Estimate".

- **Definition:** The line gives the estimated average of $y$ for each value of $x$ (**within the observed range of x**)
- **Interpretation:** The regression line of (Predicted Chest = -3.18 + 2.73*Neck) gives the estimated average Chest size for a given Neck size, based on our sample of data.
- **Discussion:** The "within the observed range of $x$" phrase is very important. We can't predict values of $y$ for values of $x$ that are very different from what we have in our data. Trying to do so is a big no-no called **extrapolation**. We'll discuss this more in a bit.

### Correlation or Association vs. Causation

<center>
![[xkcd: Correlation](https://xkcd.com/552/)](https://imgs.xkcd.com/comics/correlation.png)
</center>

What is causation? What is a causal effect? Are there criteria for defining a cause?

These are deep questions that have been debated by scientists of all domains for a long time.We are at a point now where there is some consensus on the definition of a causal effect. The causal effect of a variable $x$ on another variable $y$ is the amount that we expect $y$ to change if we **intervene on/manipulate** $x$ by changing it by one unit.

When can we interpret an estimated slope $b_1$ from a simple linear regression as a causal effect of $x$ on $y$? Well, in the simple linear regression case, almost never. To interpret the slope causally, there would have to be **no confounding variables** (no variables that impact both $x$ and $y$). If we performed an experiment, this might be possible, but in most cases, it will not be the possible.

Then what are we to do? 

1. Think about the possible causal pathways and try and create a DAG (from the first chapter).
2. Try to **adjust/control for** possible confounders using multiple linear regression (coming up soon). 
3. After fitting a model, we should step back and consider other criteria such as [Hill's Criteria](https://en.wikipedia.org/wiki/Bradford_Hill_criteria) before concluding there is a cause and effect relationship. Think about whether there is a large effect, whether it can be reproduced in another sample, whether the the effect happens before the cause in time, etc. 

**Multiple linear regression** offers us a way to estimate causal effects of variables *if we use it carefully*. It will be tempting to say: control for everything we can! But we will see that this is not the correct thing to do, as there are very real dangers of over-controlling for variables (known as throwing everything in the model). For now, let it suffice to say that multiple linear regression is as useful as the corn kerneler below. Immensely useful in the right circumstances - but only those circumstances.

![Source: [Gizmodo](https://gizmodo.com/5988717/12-highly-specific-kitchen-gadgets)](https://i.kinja-img.com/gawker-media/image/upload/s--ko0l0gLh--/c_scale,f_auto,fl_progressive,q_80,w_800/18gkhag74ei0njpg.jpg)


## Evaluation

In this section, we consider model evaluation. We seek to develop tools that allow us to answer: is this a "good" model?

### Prediction


Let's consider another data example. Can we predict your college GPA based on your high school GPA? (Disclaimer: this is not Macalester data)


```{r echo=TRUE}
sat <- read.csv("Data/sat.csv")
```

```{r echo=TRUE}
sat %>%
    ggplot(aes(x = high_GPA, y = univ_GPA)) +
    geom_point(color = 'steelblue') +
    geom_smooth(method = 'lm', se = FALSE) +
    xlab('High School GPA') +
    ylab('College GPA') + 
    theme_minimal()
```

First things first. Describe the scatterplot. 

- **Direction**: Positive relationship (higher high school GPA is associated with higher college GPA)
- **Form**: generally linear
- **Strength**: There is a weak relationship when high school GPA < 3.0 ($r = 0.32$) and a fairly strong relationship when high school GPA > 3.0 ($r = 0.68$).
- **Unusual**: As seen with the strength, there is greater variability in college GPA among individuals with lower high school GPA. That variability decreases with increased high school GPA. We call this pattern of unequal variation as **"thickening"** or **heteroscedasticity** (this terms is used quite a bit in econometrics). 

The code below computes the correlation coefficients separately for students with high school GPAs above 3 and for students with high school GPAs less than or equal to 3. We see that the correlation is higher for the high GPA group.

```{r echo=TRUE}
sat %>%
    mutate(HighHSGPA = high_GPA > 3) %>%
    group_by(HighHSGPA) %>%
    summarize(Cor = cor(high_GPA,univ_GPA))
```

Let's build a model to predict college GPA based on high school GPA based on this sample data. Since we noted that there was a linear relationship, let's find the least squares regression line.

```{r echo=TRUE}
sat %>%
  lm(univ_GPA ~ high_GPA, data = .) %>%
  summary()
```


The best fitting line is

$$ \hbox{Predicted College GPA} = 1.09 + 0.675 \times \hbox{High School GPA} $$


Let's plug in a few values.

- If High School GPA = 2:    
$$ \hbox{Predicted College GPA} = 1.09 + 0.675 \times 2 = 2.44 $$

```{r echo=TRUE}
## Calculation by hand; rounded before prediction
1.09 + 0.675*2
## Calcuation using R's predict() function
sat %>%
  lm(univ_GPA ~ high_GPA, data = .) %>%
  predict(newdata = data.frame(high_GPA = 2))
```

- If High School GPA = 3.5:    
$$ \hbox{Predicted College GPA} = 1.09 + 0.675 \times 3.5 = 3.45 $$

```{r echo=TRUE}
1.09 + 0.675*3.5 #rounded before prediction
sat %>%
  lm(univ_GPA ~ high_GPA, data = .) %>%
  predict(newdata = data.frame(high_GPA = 3.5)) #rounded after prediction
```

- If High School GPA = 4.5:    
$$ \hbox{Predicted College GPA} = 1.09 + 0.675 \times 4.5 = 4.13 $$

```{r echo=TRUE}
1.09 + 0.675*4.5 #rounded before prediction
sat %>%
  lm(univ_GPA ~ high_GPA, data = .) %>%
  predict(newdata = data.frame(high_GPA = 4.5)) #rounded after prediction 
```

```{block, type='reflect'}
Does it make sense to use this model for high school GPA's > 4?  Some high schools have a max GPA of 5.0 due to the weighting of advanced courses. 
```

- What is the maximum high school GPA in this data set? 
- What if your college doesn't allow for GPA's above 4.0? 


```{r echo=TRUE}
sat %>%
    summarize(max(high_GPA)) # Max high school GPA in this data set
```

Making predictions beyond the observed range of values is called **extrapolation** and is generally a risky thing to do. If you make prediction for values of $x$ beyond the minimum or maximum of the observed values, then you are assuming that the relationship you observe can be extended into the new prediction range. This is the main issue of **forecasting**, making predictions in the future. You have to assume that the trend that you observe now will continue in the future and that the current state of affairs will stay the same. For an infamous case of extrapolation, check out [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3173856/) that appeared in the journal Nature.

### Prediction Errors

Recall that a residual, $e_i$, for the $i$th data point is the difference between the actual and predicted values: $e_i = y_i - \hat{y}_i$.

If the residuals were approximately unimodal and symmetric, we expect about 95% of the residuals to be within 2 standard deviations of 0 (the mean residual). (Recall Section \@ref(intro-zscore).)

```{r echo=TRUE}
sat %>%
  lm(univ_GPA ~ high_GPA, data = .) %>%
  residuals() %>%
  hist(main = "Distribution of residuals in the GPA model")
```

Below we calculate $SSE$ (the sum of squared errors/residuals), and the standard deviation of the residuals ($s_e$).

```{r echo=TRUE}
lm.gpa <- sat %>%
  lm(univ_GPA ~ high_GPA, data = .)
SSE <- sum(residuals(lm.gpa)^2)
n <- length(residuals(lm.gpa)) # Sample size
s <- sqrt(SSE/(n-2)) #sd of residuals
s
2*s
```

Using this model (that is, using your high school GPA), we can predict your college GPA within about $0.56$ GPA points. Is this useful? Is predicting within a margin of $0.56$ GPA points good enough? Let's compare this margin of error with the margin of error about the mean (the standard deviation):

```{r echo=TRUE}
sd(sat$univ_GPA)
2*sd(sat$univ_GPA)
```

*Without* knowing your high school GPA, we could have just guessed your college GPA as the overall mean college GPA in the sample, and this guess would probably be within $\pm 0.89$ of your actual college GPA. This is a higher margin of error than the approximate $0.56$ if we did use your high school GPA (in the simple linear regression model). We are able to predict your college GPA with a smaller margin of error than if we just guessed your college GPA with the mean. Our model has *explained* some (but not all) of the variation in college GPA.

The standard deviation of college GPA is based on the sum of squared total variation, SSTO (variation around the mean),

$$ SSTO = \sum{(y_i -\bar{y})^2} $$

$SSTO$ is the numerator of the standard deviation of $y$ (without knowing anything about $x$).

```{r echo=TRUE}
(SSTO = sum((sat$univ_GPA - mean(sat$univ_GPA))^2))
```

We define $SSTO$ here because it will help to compare $SSTO$ to $SSE$ (sum of squared residuals from the model) to obtain a measure of how well our models are **fitting** the data - how well they are predicting the outcome.

### $R^2$

Let's study how *models reduce unexplained variation*.

- Before a model is fit, the unexplained variation is given by $SSTO$. It is the overall variability in the outcome. Think back to interpreting standard deviation and variance as measures of spread. We used these to describe broadly how much the outcome varys
- Unexplained variation in the outcome that remains after modeling is given by $SSE$, the sum of squared residuals.

So to study how models reduce unexplained variation, we compare the magnitude of the residuals from a linear regression model (which uses the predictor $x$) with the original deviations from the mean (which do not use the predictor $x$).

```{r echo=FALSE}
gpavar <- data.frame(resids = c(residuals(lm.gpa), sat$univ_GPA-mean(sat$univ_GPA)), type = rep(c('Linear Model Residuals','Original Deviations from Mean'), each = nrow(sat)))

gpavar %>%
  ggplot(aes(x = type, y = resids)) +
  geom_boxplot() +
  ylab('Residuals') +
  xlab('Type of Residuals')
```


We started with the sum of the deviations from the mean $SSTO = \sum{(y_i - \bar{y})^2}$ before we had info about high school GPA ($x$).

- Now, with our knowledge of $x$, we have $SSE = \sum{(y_i - \hat{y_i})^2}$

- $SSE$ should be smaller than $SSTO$ (!)

Two extreme cases: 

- If the error $SSE$ goes to zero, we'd have a "perfect fit". 
- If $SSE = SSTO$, $x$ has told us nothing about $y$.

- So we define a measure called **R-squared**, which is the *fraction* or *proportion* of the total variation in $y$ "accounted for" or "explained" by the model in $x$. 



```{block type='math'}
$$ R^2 = 1 - \frac{SSE}{SSTO} = 1 - \frac{ \sum{(y_i - \hat{y_i})^2}}{ \sum{(y_i - \bar{y})^2}}$$
```

In R, `lm()` will calculate R-Squared ($R^2$) for us, but we can also see that it equals the value from the formula above. 


```{r echo=TRUE}
1 - SSE/SSTO
glance(lm.gpa) #r.squared = R^2, sigma = s_e (ignore the rest)
```

- Is there a "good" value of $R^2$? Same answer as correlation -- no.

- $R^2$ doesn't tell you the direction or the form of the relationship.

- Note: $R^2 = r^2$ for simple linear models with one x variable (where $r$ is the correlation coefficient).


## Diagnostics

Residuals are what's left over from a linear fit. We can actually learn a lot by studying what is left over, what is left unexplained by the model. INSERT SCATOLOGY JOKE. 

What do we need for a simple linear model to make sense?

  * Variables are both **Quantitative**
  
  * Relationship is **Straight Enough**
  
  * There are no extreme **Outliers** 
  
  * Spread is roughly same throughout -- **No Thickening**
  
To check these, we look at the original scatterplot and a plot of *residuals* vs. *fitted or predicted values*,

```{r echo=TRUE}
augment(lm.gpa, data = sat) %>%
    ggplot(aes(x = .fitted, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    labs(x = "Fitted values (predicted values)", y = "Residuals") +
    theme_minimal()
```

What do you think?

  * Is there any pattern? (Is the original scatterplot straight enough?)
  * Is there equal spread across prediction values?

We want to avoid extreme outliers because points that are both far from the mean of x and do not fit the overall relationship have **leverage** to change the line.

See the example below. See how the relationship changes with the addition of one point, one extreme outlier.  

```{r echo=FALSE}
x = runif(50,0,10)
y = rnorm(50)
dat = data.frame(x,y)
dat %>%
  ggplot(aes(x,y)) + 
  geom_point() + 
  geom_smooth(method='lm') + 
  xlim(0,20) + 
  ylim(-5,10) + 
  theme_minimal() 

x = c(x,20)
y = c(y,10)
dat = data.frame(x,y)

dat %>%
  ggplot(aes(x,y)) + 
  geom_point() + 
  geom_smooth(method='lm') + 
  xlim(0,20) + 
  ylim(-5,10) + 
  theme_minimal() 
```

Check out this interactive visualization to get a feel for leverage: http://omaymas.github.io/InfluenceAnalysis/


### Solutions to Issues

If the observed data don't satisfy the conditions above, what can we do? Should we give up using a statistical model? No!

* Problem: Both variables are NOT **Quantitative**
    - If your x-variable is categorical, we'll turn it into a quantitative variable using **indicator variables** (coming up) 
    - If you have a binary variable (exactly 2 categories) that you want to predict as your y variable, we'll use **logistic regression** (coming up)

* Problem: Relationship is NOT **Straight Enough**

    - If the plot does not thicken, we can add higher degree (e.g. $x^2, x^3, x^4$, etc.) terms to the model (**multiple linear regression** - coming up).
    - If the plot does thicken, see solutions below.

* Problem: Spread is NOT the same throughout
    - You may be able to transform the y-variable using mathematical functions ($log(y)$, $y^2$, etc.) to make the spread more consistent (one approach is to use **Box-Cox Transformation** -- take more statistics classes to learn more)
    - Be careful in interpreting the standard deviation of the residuals. (For example, if you take a log transformation of the outcome, the units of the standard deviation of the residuals will be on the log scale.)

* Problem: You have **extreme outliers**
    - Look into the outliers. Determine if they could be due to human error. Think carefully about them, dig deep.
    - Do a **sensitivity analysis**: Fit a model with and without the outlier and see if your conclusions drastically change (see if those points had leverage to change the model).


## Multiple Linear Regression

We can generalize the idea of a simple linear model by including many explanatory variables (X's). A **multiple linear regression model** can be written as:

$$\hat{y}_i = b_0 + b_1x_{i1} + \cdots + b_kx_{ik}$$

- Each coefficient $b_j$ can be interpreted as the increase in the predicted/average y associated with a 1 unit increase in $x_j$, **keeping all other variables constant**. (\*There are some exceptions - we'll get there.)

- These X variables can be:
    - Quantitative variables (or transformations of them)
    - Indicator variables for categorical variables (only need $k-1$ indicators for a variable with $k$ categories)
    - Interaction terms (product of two variables, which allows for *effect modification*)
    
Let's talk about a new data example: home prices. We want to build a model to predict the price of a home based on its many characteristics. Here we have a data set of homes recently sold in New England with many variables such as the age of the home, the land value, whether or not it has central air conditioning, the number of fireplaces, the sale price, and more...

```{r echo=TRUE}
homes <- read.delim('http://sites.williams.edu/rdeveaux/files/2014/09/Saratoga.txt')
head(homes)
```

\*The exception to the interpretation comment above is if our variables are strongly correlated. In this case, we cannot keep all other variables constant because if you increase the value of one, then a variable with high correlation will also likely change in value. 


### Indicator Variables

In New England, fireplaces are often used as a way to heat the house. Let's study the impact of a fireplace has on the sale price of a home. In particular, we only care if the home has 1 or more fireplaces or no fireplaces. So we make a new variable that is TRUE if there are more than 0 fireplaces in a home and FALSE otherwise. 

```{r echo=TRUE}
homes <- homes %>%
  mutate(AnyFireplace = Fireplaces > 0)
```

In order to include this information in our linear regression model, we need to turn that categorical variable (`AnyFireplace`, TRUE or FALSE) into an **indicator variable**, which has a numeric value of 0 or 1:

$$ 1_{AnyFireplaceTRUE} = \begin{cases}1 \quad \text{ if a home has at least one fireplace}\\ 0\quad \text{ if a home does not have a fireplace} \end{cases}$$

In fact, R creates this indicator for you when you put a categorical variable as an X variable in the model.

```{r echo=TRUE}
lm.home <- lm(Price ~ AnyFireplace, data = homes)
tidy(lm.home)
```

Our "best fitting line" is

$$ \hbox{Predicted Price} = 174653.35 + 65260.61 \times 1_{AnyFireplaceTRUE} $$

**What does this mean?**

Let's think about two types of homes: a home with one or more fireplaces and a home without a fireplace. Let's write out the equations for those two types of homes. 

- Home with fireplace:    
$$ \hbox{Predicted Price} = 174653.35 + 65260.61 \times 1 = \$ 239,914 $$

```{r echo=TRUE}
174653.35 + 65260.61*1
```

- Home without fireplace:    
$$ \hbox{Predicted Price} = 174653.35 + 65260.61 \times 0 = \$ 174,653.35 $$


The difference between these predicted prices is \$65,260.61. So is this how much a fireplace is worth? If I installed a fireplace in my house, should the value of my house go up \$65,260?

- No, because we are not making causal statements based on observational data. What could be confounding this relationship? What third variable may be related to both the price and whether or not a house has a fireplace?

Let's look at the size of the house. Is price related to the area of living space (square footage)?

```{r echo=TRUE}
homes %>%
    ggplot(aes(x = Living.Area, y = Price)) + 
    geom_point(color = 'steelblue') +
    theme_minimal()
```

Is the presence of a fireplace related to area of living space?

```{r echo=TRUE}
homes %>%
    ggplot(aes(x = AnyFireplace, y = Living.Area)) + 
    geom_boxplot() +
    theme_minimal()
```

We see that the amount of living area differs between homes with fireplaces and homes without fireplaces. Thus, Living Area could confound the relationship between AnyFireplace and Price because it is related to both variables.

Let's put Living Area in the model along with AnyFireplace to account for it (to control/adjust for it).

```{r echo=TRUE}
lm.home2 <- lm(Price ~ AnyFireplace + Living.Area, data = homes)
tidy(lm.home2)
```

Our "best fitting line" is

$$ \hbox{Predicted Price} = 13599.16 + 5567.37 \times 1_{AnyFireplaceTRUE} + 111.21 \times \hbox{Living.Area} $$

**What does this mean?** 

Let's think about two types of homes: a home with one or more fireplaces and a home without a fireplace.

- Home with fireplace:    
$$
\begin{align*}
\hbox{Predicted Price} &= 13599.16 + 5567.37 \times 1 + 111.21 \times \hbox{Living.Area} \\
&= \$19,166.53 + \$111.21 \times \hbox{Living.Area}
\end{align*}
$$

```{r echo=TRUE}
13599.16 + 5567.37*1
```

- Home without fireplace:    
$$
\begin{align*}
\hbox{Predicted Price} &= 13599.16 + 5567.37 \times 0 + 111.21 \times \hbox{Living.Area} \\
&= \$13,599.16 + \$111.21 \times \hbox{Living.Area}
\end{align*}
$$

If we keep Living.Area constant by considering two equally sized homes, then we'd expect the home with the fireplace to be worth \$5567.37 more than a home without a fireplace. We see this by taking the difference between the two equations:

$$ \hbox{Predicted Price (with Fireplace)} - \hbox{Predicted Price (without Fireplace)} $$
$$
\begin{align*}
\,&= (\$19,166.53 + \$111.21 \times \hbox{Living.Area}) - ( \$13599.16 + \$111.21 \times \hbox{Living.Area})
\,&= \$19,166.53 - \$13,599.16 = \$5567.37
\end{align*}
$$

The difference between the intercepts is 5567.37.

- Note this was the estimated coefficient for AnyFireplaceTRUE. 

- So the \$5567.37 is the increase in the predicted or average Price associated with a 1 unit change in AnyFireplace (TRUE or FALSE), **keeping all other variables (Living.Area) constant**.

- Similarly, we could reason that \$111.21 is the increase in the predicted or average Price associated with a 1 square footage increase in Living.Area, **keeping all other variables (AnyFireplace) constant**.

Let's look back at the relationship between Living.Area and Price and color the scatterplot by AnyFireplace. So we are now looking at three variables at a time. The above model with AnyFireplace and Living.Area results in two lines, with different intercepts but the same slope (parallel lines).

```{r echo=TRUE}
homes %>%
    ggplot(aes(x = Living.Area, y = Price, color = AnyFireplace)) + 
    geom_point() +
    theme_minimal()
```

Let's try and fit two separate lines to these two groups of homes, home with any fireplaces and home with no fireplaces. Do these lines have the same intercepts? Same slopes?

```{r echo=TRUE}
homes %>%
    ggplot(aes(x = Living.Area, y = Price, color = AnyFireplace)) + 
    geom_point() +
    geom_smooth(method = 'lm', se = FALSE) +
    theme_minimal()
```

In this case, it look as though having a fireplace in your house slightly changes the relationship between Living.Area and Price. In fact, having a fireplace in your house, the increase in your price for every 1 square foot is greater than that for homes without fireplaces (slopes are different).


### Interaction Variables

We can actually allow for different slopes within one regression model, rather than fitting two separate models. 

- If we add a variable in the model as is, it changes the intercept.

- We can achieve different slopes by allowing a variable $x_1$ to affect the slope for another variable $x_2$. That is, $x_1$ impacts the effect of $x_2$ on the outcome $y$. (Fireplace presence impacts the effect of living area on house price.)
$$b_2 = a + bx_1$$
This is called **effect modification** (when one variable can modify the effect of another variable on the outcome).
- A model with effect modification looks like:
$$\hat{y} = b_0 + b_1x_{1} + b_2x_{2}= b_0 + b_1x_{1} + (a+bx_1)x_{2}= b_0 + b_1x_{1} +ax_2+bx_1x_{2}$$
The model above has an **interaction term,** which is the product of two variables. Here we have $x_1*x_2$.

Let's build an effect modification model for our housing data. Let's include an interaction term between AnyFireplace and Living.Area to allow for different slopes.

```{r echo=TRUE}
lm.home3 <- lm(Price ~ AnyFireplace*Living.Area, data = homes)
tidy(lm.home3)
```

**What does this mean?** 

Let's think about two types of homes: a home with one or more fireplaces and a home without a fireplace.

- Home with fireplace:    
$$
{\small
\begin{align*}
\hbox{Predicted Price} &= 40901.29 + -37610.41 \times 1 + 92.36391 \times \hbox{Living.Area} + 26.85 \times \hbox{Living.Area} \times 1 \\
& = \$3,290.88 + \$119.21 \times \hbox{Living.Area}
\end{align*}
}
$$

```{r echo=TRUE}
40901.29 + -37610.41*1
92.36391 + 26.85*1
```

- Home without fireplace:    
$$
{\small
\begin{align*}
\hbox{Predicted Price} &= 40901.29 + -37610.41 \times 0 + 92.36391 \times \hbox{Living.Area} + 26.85 \times \hbox{Living.Area} \times 0 \\
&= \$40,901.29 + \$92.36 \times \hbox{Living.Area}
\end{align*}
}
$$

We see a different slope and different intercepts for these two groups.

### Causation

We alluded earlier that multiple linear regression could provide estimates of causal effects in the right circumstances. What are those circumstances? When we include **all** confounding variables. This entails being specific about what a confounding variable is. A confounder is a **common cause** of both the causal variable of interest and the outcome. (e.g. Living area could be a confounder of fireplace presence and house price.)

We also alluded earlier that we should not just throw every variable we have into a multiple regression model. Why? Imagine a scenario for understanding how smoking affects lung cancer development. It is very important to consider whether a variable is a **mediator** of the relationship between the cause and the outcome. A mediator in this example could be tar. Suppose that smoking only affects lung cancer risk by creating tar on the lungs. If we adjust for tar (by holding it constant), then we also effectively hold smoking constant too! If smoking is held constant, then we cannot estimate its effect on cancer risk because it is not varying!

Wait - we could never possibly know of or measure all confounding variables, could we!? This is true, but that doesn't mean that our endeavor to understand causation is fruitless. As long as we can describe the relationship between known confounders as precisely as possible, we have a starting ground for moving forward. We collect data, analyze how well our model predicts that data, and collect more data based on that, perhaps measuring more potential confounders as our scientific knowledge grows. We can also conduct sensitivity analyses by asking: how strongly must a confounder affect the variable of causal interest and the outcome to completely negate or reverse the association we see? Such endeavors and more are the subject of the field of *causal inference*.

*Reminder: If you want a "gentle" but mathematical introduction to Causal Inference, I'd suggest reading "Causal Inference in Statistics: A Primer" by Judea Pearl, Madelyn Glymour, Nicholas P. Jewell). Fun Fact: Nicholas Jewell was Prof. Heggeseth's PhD advisor!*

### Conditions for Multiple Linear Regression

In order for a multiple linear regression model to make sense, 

- Relationships between each quantitative $X$'s and $Y$ are **straight enough** (check scatterplots and residual plot)
- About **equal spread** of residuals across fitted values (check residual plot)
- No extreme outliers (points far away in X's can have **leverage** to change the line)


### Is the Difference Real?

We could ask: is there *really* a difference in the slopes for Living Area and Price between homes with and without a fireplace?

```{r echo=TRUE}
lm.home4 <- lm(Price ~ Living.Area*AnyFireplace, data = homes) 
summary(lm.home4)
```

If we ask ourselves this question, we are assuming a few things:

1. We would like to make a general statement about a **target population of interest**.

2. We don't have data for everyone in our population (we don't have a **census**).

3. Depending on who ends up in our **sample**, the relationship/difference/estimate may change a bit.

4. We want to know how much the relationship/difference/estimate may change based on **sampling variation.**  

- Let's treat our sample (of size $n$) as our 'fake' population (since we don't have the full population).
    - Randomly sample from our sample (with replacement) a new sample of size $n$
- Calculate the least squares regression line.
- Repeat.

```{r echo=TRUE}
set.seed(333) ## Setting the seed ensures that our results are reproducible
## Repeat the sampling and regression modeling 1000 times
boot <- do(1000)*lm(Price ~ Living.Area*AnyFireplace, data = resample(homes))

## Plot the distribution of the 1000 slope differences
boot %>%
    ggplot(aes(x = Living.Area.AnyFireplaceTRUE)) +
    geom_histogram() +
    xlab('Bootstrap Difference in Slopes')
```

This is called **Bootstrapping** and it is used to:

1. Measure the variability in the estimate (the estimate is the difference in slopes in this case) between random samples and
2. Provide an interval of plausible values for the estimate (the estimate is the difference in slopes in this case).

Let's first look at the variability of the difference in slopes across the bootstrap samples. The standard deviation of the slopes will be similar to the std.error from the linear model output.

```{r echo=TRUE}
boot %>%
  summarize(sd(Living.Area.AnyFireplaceTRUE))#this is going to be of similar magnitude to the Std Error in output
tidy(lm.home4)
```

This standard deviation is somewhat close to the $6.459$ in the Std. Error column of the `summary(lm.home4)` output above.

To get an interval of plausible values, we look at the histogram and take the middle 95%. The lower end will be the 2.5th percentile and the upper end will be the 97.5th percentile.

```{r echo=TRUE}
boot %>%
  summarize(lower = quantile(Living.Area.AnyFireplaceTRUE, 0.025), upper = quantile(Living.Area.AnyFireplaceTRUE, 0.975))
```

```{block, type="reflect"}
Based on this evidence, do you think it is possible that the slopes are the same for the two types of homes (with and without fireplaces)?
```

### Dealing with Non-Linear Relationships

If we notice a curved relationship between two quantitative variables, it doesn't make sense to use a straight line to approximate the relationship. 

What can we do?

#### Transform Variables

One solution to non-linear relationships is to transform the explanatory  (X) variables or transform the outcome variable (Y).

**Guideline #1:** If there is unequal spread around the curved relationship, focus first on transforming Y. If the spread is roughly the same around the curved relationship, focus on transforming X. 

When we say transform a variable, we are referring to taking the values of a variable and plugging them into a mathematical function such as $\sqrt{x}$, $\log(x)$ (which represents natural log, not log base 10), $x^2$, $1/x$, etc. 

**Guideline #2:** We will focus on power functions and organize them in a **Ladder of Powers** of y (or x):


$$
\begin{align}
\vdots\\
y^3\\
y^2\\
\mathbf{y = y^1}\\
\sqrt{y}\\
y^{1/3}\\
y^{0} ~~~  (we~use~\log(y)~here )\\
y^{-1/3}\\
1/\sqrt{y}\\
1/y\\
1/y^2\\
\vdots
\end{align}
$$

We start at $y$ (power  = 1) and think about going up or down the ladder. 

But which way?

Our friend J.W. Tukey (the same guy who invented the boxplot) came up with an approach to help us decide. You must ask yourself: Which part of the circle does the scatterplot most resemble (concavity and direction)? Which quadrant?

**Guideline #3:** The sign of x and y in the quadrant tells you the direction to move on the ladder (positive = up, negative = down).

<img src="Photos/tukey.png" width="400" height="500" />

Practice: Which quadrant does this relationship below resemble? 

```{r}
require(gapminder)

gapminder %>% 
  filter(year > 2005) %>%
  ggplot(aes(y = lifeExp, x =  gdpPercap)) + 
  geom_point()
```

Based on this plot, we see that the spread is roughly equal around the curved relationship and that it is concave down and positive (quadrant 2: top left). This suggests that we focus on going down the ladder with x. 

Try these transformations until you find a relationship that is roughly straight. If you go too far, the relationship will become more curved.

Let's try going down the ladder.

```{r}
gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = sqrt(gdpPercap)) %>%
  ggplot(aes(y = lifeExp, x =  TgdpPercap)) + 
  geom_point()
```

Not quite straight. Let's keep going.


```{r}
gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = gdpPercap^(1/3)) %>%
  ggplot(aes(y = lifeExp, x =  TgdpPercap)) + 
  geom_point()
```

Not quite straight. Let's keep going.


```{r}
gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = log(gdpPercap)) %>% 
  ggplot(aes(y = lifeExp, x =  TgdpPercap)) + 
  geom_point()
```

Getting better. Let's try to keep going.


```{r}
gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = -1/gdpPercap) %>% 
  ggplot(aes(y = lifeExp, x =  TgdpPercap)) + 
  geom_point()
```

TOO FAR! Back up. Let's stick with log(gdpPercap).

Now we see some unequal spread so let's also try transforming Y.

```{r}
gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = log(gdpPercap)) %>% 
  mutate(TlifeExp = lifeExp^2) %>% 
  ggplot(aes(y = TlifeExp, x =  TgdpPercap)) + 
  geom_point()
```

That doesn't change it much. Maybe this is as good as we are going to get. 

Transformations can't make relationships look exactly linear with equal spread, but sometimes we can make it closer to that ideal.

Let's try and fit a model with just these two variables.

```{r}
lm.gap <- gapminder %>% 
  filter(year > 2005) %>%
  mutate(TgdpPercap = log(gdpPercap)) %>% 
  lm(lifeExp ~ TgdpPercap, data = .)

summary(lm.gap)
```

**Interpretations**

What does $b_1$ mean in this context?

$$\widehat{LifeExp} = b_0 + b_1 log(Income)$$


- The slope is the the additive increase in $\widehat{LifeExp}$ when $log(Income)$ increases to $log(Income) + 1$.

- Let's think about $log(Income) + 1$. Using some rules of logarithms:

$$log(Income) + 1 = log(Income) + log(e^1) = log(e*Income) = log(2.71*Income)$$
So adding 1 to $log(Income)$ is equivalent to multiplying Income by 2.71.


In our model, we note that if GDP is increased by 271% (multiplying by 2.71) the predicted average life expectancy of a country increases by about 7.2 years. 


For the sake of illustration, imagine we fit a model where we had transformed life expectancy. What does $b_1$ mean in this context?

$$\widehat{log(LifeExp)} = b_0 + b_1 Income$$


- The slope is the the additive increase in $\widehat{log(LifeExp)}$ when $Income$ increases to $Income + 1$.


- Let's think about $\widehat{log(LifeExp)} + b_1$. Using rules of logarithms: 
$$\widehat{log(LifeExp)} + b_1 = \widehat{log(LifeExp)} + log(e^{b_1}) = log(\widehat{LifeExp} * e^{b_1}) $$
The additive increase in $log(LifeExp)$ is a multiplicative increase of $LifeExp$ by a factor of $e^{b_1}$.

### Alternative Solutions

We could also model non-linear relationships by including higher degree terms in a linear model like the example below. By using poly(), we now include $x$ and $x^2$ as variables in the model.

```{r}
x <- rnorm(100, 5, 1)
y <- 200 + 20*x - 5*x^2 + rnorm(100,sd = 10)
dat <- data.frame(x,y)
dat %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth()
lm(y ~ poly(x, degree = 2, raw = TRUE), data = dat)
```

A more advanced solution (which is not going to be covered in class) is a generalized additive model, which allows you to specify which variables have non-linear relationships with y and estimates that relationship for you using spline functions (super cool stuff!). We won't talk about how this model is fit or how to interpret the output, but there are other cool solutions out there!

```{r}
require(gam)
plot(gam(y ~ s(x), data = dat))
```


## Logistic Regression

If you are interested in predicting a binary categorical variable (only 2 possible outcomes), the standard linear regression models don't apply. If you let the two outcomes be 0 and 1, you'll never get a straight line relationship with an x variable. 

Throughout this section, we will refer to one outcome as 'success' (denoted 1) and 'failure' (denoted 0). Depending on the context of the data, the success could be a negative thing such as 'heart attack' or '20 year mortality'. 

We will let $p$ be the chance of success and $1- p$ be the chance of failure. We want to build a model to explain why the chance of success may be higher for one group of people in comparison to another. 

### Logistic and Logit

The **logistic function** is an S shaped curve (sigmoid curve). For our purposes, the function will take the form

$$f(x) = \frac{1}{1 + e^{b_0 +b_1x}}$$

```{r echo=FALSE}
x <- seq(-10,10,by=.1)
b0 <- 2
b1 <- -0.5
plot(x, 1/(1 + exp(b0 + b1*x)), type = 'l', ylim = c(0,1), ylab = 'f(x)')
```

For any real value x, f(x) will be a value between 0 and 1. This is perfect for us since probabilities/chances should also be between 0 and 1. 

In fact, we'll let the chance of failure, $1-p$, be modeled by this function.

$$1-p = \frac{1}{1 + e^{b_0 +b_1x}}$$
```{block, type='math'}
With a bit of algebra and rearranging terms, we can write this in terms of $p$, the chance of success. 

$$p = \frac{e^{b_0 +b_1x}}{1 + e^{b_0 +b_1x}}$$

Let's define one more term. The **odds** of success is the ratio of the probability of success to the probability of failure, $p/(1-p)$.

With a bit more algebra and rearranging terms, we can write the above model in terms of a regression model,

$$\log\left(\frac{p}{1-p}\right) = b_0 +b_1x$$
On the left hand side, we have the natural log of the odds, called the **logit** function. On the right hand side, we have an equation for a line. This is a **simple logistic regression model**.
```

Just like a linear regression model, we can extend this model to a **multiple logistic regression model** by adding additional x variables,

$$\log\left(\frac{p}{1-p}\right) = b_0 +b_1x_1+b_2x_2+b_3x_3+\cdots +b_kx_k$$

### Fitting the Model

Based on observed data that includes responses (1 for success, 0 for failure) and predictor variables, we need to find the slope coefficients, $b_0$,...,$b_k$ that best fits the data. The way we do this is through a technique called **maximum likelihood estimation**. We will not discuss the details in this class; we'll save this for an upper level stats class.

In R, we do this with a general linear model function, `glm()`.

For a data set, let's go back in history to January 28, 1986. On this day, the Challenger U.S. space shuttle took off and exploded about minute after the launch. After the fact, scientists ruled that the disaster was due to o-ring seal failure. Let's look at experimental data on the o-rings prior to the fateful day.


```{r echo=TRUE}
require(vcd)
data(SpaceShuttle)

SpaceShuttle %>%
  filter(!is.na(Fail)) %>%
  ggplot(aes(x = factor(Fail), y = Temperature) )+ 
  geom_boxplot() +
  theme_minimal()

SpaceShuttle %>%
  filter(!is.na(Fail)) %>%
  group_by(Temperature) %>%
  summarise(PercentFail = mean(Fail == 'yes')) %>%
  ggplot(aes(x = Temperature, y = PercentFail)) + 
  geom_point() +
  theme_minimal()
```

```{block, type="reflect"}
What is the plot above telling us about the relationship between chance of o-ring failure and temperature?
```


Let's fit a simple logistic regression model to predict the chance of o-ring failure based on the temperature using the experimental data. 

```{r echo=TRUE}
model.glm <- glm(Fail ~ Temperature, data = SpaceShuttle, family = binomial)
summary(model.glm)
```


### Interpretation

Let's take a look at these estimates from the model. What do they mean?

```{r echo=TRUE}
tidy(model.glm)
```

If you want to get a sense of what the number -0.232 means, we need to do a bit of algebra. 

```{block, type='math'}
Our estimated model is

$$\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = b_0 +b_1x$$
where $b_0 = 15$ and $b_1 = -0.232$.

If we imagine increasing x by 1, then we get a different set of predicted probabilities of success, $\hat{p}^*$,

$$\log\left(\frac{\hat{p}^*}{1-\hat{p}^*}\right) = b_0 +b_1(x+1)$$

Let's find the difference between these two equations,

$$\log\left(\frac{\hat{p}^*}{1-\hat{p}^*}\right) - \log\left(\frac{\hat{p}}{1-\hat{p}}\right) = b_0 +b_1(x+1) - (b_0 +b_1x)$$
and simplify the right hand side,

$$\log\left(\frac{\hat{p}^*}{1-\hat{p}^*}\right) - \log\left(\frac{\hat{p}}{1-\hat{p}}\right) = b_1$$
and then simplify the left hand side,

$$\log\left( \frac{\hat{p}^*/(1-\hat{p}^*)}{\hat{p}/(1-\hat{p})}\right) = b_1$$

Let's exponentiate both sides,

$$\left( \frac{\hat{p}^*/(1-\hat{p}^*)}{\hat{p}/(1-\hat{p})}\right) = e^{b_1}$$
```

We find that $e^{b_1} = e^{-0.232} = 0.793$ is the **odds ratio** based on increasing x by 1 unit (it is a ratio of odds). 

If a ratio is greater than 1, that means that the denominator is less than the numerator or numerator is greater than denominator. If a ratio is less than 1, that means that the denominator is greater than the numerator or numerator is less than denominator. If the ratio is equal to one, the numerator equals the denominator (odds are equal).


In this case, we have a ratio < 1 which means that the estimated odds of o-ring failure is lower for increased temperatures (in particular by increasing by 1 degree). This makes sense since we saw that the chance of o-ring failure decrease with warmer temperatures. 

### Prediction

On January 28, 1986, the temperature was 26 F degrees. Let's predict the chance of "success," which is a failure of o-rings in our data context, at that temperature.  

```{r echo=TRUE}
predict(model.glm, newdata = data.frame(Temperature = 26), type = 'response')
```

They didn't have any experimental data testing o-rings at this low of temperatures, but even based on the data collected, they predict the chance of failure to be nearly 100% (near certainty). 

#### Hard Predictions/Classifications

These predicted chances of "success" are useful to give us a sense of uncertainty in our prediction, but how high should the predicted chance be to do "hard" predictions of "success"?

It depends. 

If we used a threshold of 0.8, then we'd say that for any experiment with a predicted chance of o-ring failure 0.8 or greater, we'll predict that there will be o-ring failure. As with any predictions, we may make an error. With this threshold, what is our **accuracy** (# of correctly predicted/# of data points)? 

In the table below, we see that there were three data points in which we correctly predicted o-ring failure. There were 4 data points in which we erroneously predicted that it wouldn't fail when it actually did and correctly predicted no failure for 16 data points. So in total, our accuracy is (16+3)/(16+4+3) =  0.82. 

The only errors we made were **false negatives**; we didn't predict failure but failure did actual happen in the experiment. In this data context, false negatives have real consequences on human lives because a shuttle would launch and potentially explode because we had predicted there would be no o-ring failure. 

The **false negative rate** is the number of false negatives divided by the false negatives + true positives (denominator should be total number of experiments with o-ring failures). With a threshold of 0.80, our false negative rate is 4/(3+4) = 0.57.

A **false positive** (predicting failure when it doesn't happen) would delay launch but have minimal impact on human lives. 


The **false positive rate** is the number of false positives divided by the false positives + true negatives (denominator should be total number of experiments with no o-ring failures). With a threshold of 0.80, our false positive rate is 0/16 = 0.

```{r}
augment(model.glm, type.predict ='response') %>%
  mutate(predictOFail = .fitted >= 0.8) %>%
  count(Fail,predictOFail)
```

What if we used a lower threshold to reduce the number of false negatives? Let's lower it to 0.25 so that we predict o-ring failure more easily. Let's find our accuracy: (10+4)/(10+3+6+4) = 0.61. Worse than before, but let's check false negative rate: 3/(3+4) = 0.43. That's lower. But now we have a fairly high false positive rate: 6/(6+10) = 0.375. So of the experiments with o-ring failure, we predicted wrong 37.5% of the time. 

```{r}
augment(model.glm, type.predict ='response') %>%
  mutate(predictOFail = .fitted >= 0.25) %>%
  count( Fail, predictOFail)
```

### Model Evaluation

In deciding whether a logistic regression model is a good and useful model, we need to consider the accuracy, the false positive rate and the false negative rate. Depending on the context, we may focus on maximizing the overall accuracy or we may focus on minimizing the false negative rate or minimizing the false positive rate. 


### Alternative Classification Models

Logistic regression is a very useful model to predict a binary outcome that is an extension of linear regression. However, it has its limitations. We are assuming a linear relationship between explanatory variables and the log odds. This is hard to check because we don't have a variable for odds that we could then quickly plot. 

Other methods out there are more flexible but also more complex. Sometimes for a task, complex is not necessary: see https://www.huffingtonpost.com/2014/02/10/klemens-torggler-evolution-door_n_4762261.html. 

- Classification Trees can predict a binary outcome and choose the variables that are most important by recursively partitioning the data into groups that are more similar in terms of the outcome as well as in chosen predictor variables.

- Random Forests are a collection of classification tress that are more stable than one classification tree.

- Boosted Trees are classification trees that are sequentially created to target the errors from the last tree.

- Neural Networks are a type of classification algorithm that creates new features based on the original data that are the best predictors of the outcome. 

Take Machine Learning to learn more about these methods. 


##Major Takeaways

1. All models are wrong, but some are useful and fair. 

2. We want a model with small residuals (prediction errors).

3. To determine if a model is useful and fair, we study what is left over (the residuals). 

4. We use models to describe phenomena by interpreting slope estimates. Make sure you are talking about the average or predicted outcome! If you have multiple variables, you keep all others fixed (if possible).

5. We also use models to prediction values, but be careful about predicting outside the observed range of our explanatory (X) variables. That is called extrapolation.


